{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90435024-0a71-4c5d-b805-d04e5a92f68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-time Deployment with Model Serving\n",
    "\n",
    "In this demo, we will focus on real-time deployment of machine learning models. Databricks' Model Serving is an easy-to-use serverless infrastructure for serving the models in real-time that supports both online and offline feature tables as well as automatic feature lookups for online tables with no additional endpoint configuration.\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "**By the end of this demo, you will be able to;**\n",
    "\n",
    "- Understand the differences between **offline** and **online** feature tables for Databricks Model Serving.\n",
    "- Understand how to serve multiple versions of a model simultaneously and set up **A/B testing** for real-time inferencing.\n",
    "- Utilize **feature lookups** and a **feature function** for online tables for real-time inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6a2d3d8-0d8d-4e16-92e0-f0cc4407f8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "- To run this notebook, you need to use one of the following Databricks runtime(s): `16.2.x-cpu-ml-scala2.12`\n",
    "- Online Tables must be enabled for the workspace.\n",
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9108d855-46d1-4b03-8140-67b267a1ca9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9942a41d-20b6-4a5c-b6a3-8d345d27ca91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Other Conventions:\n",
    "\n",
    "Throughout this demo, weâ€™ll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "733d8f72-9a52-4174-a769-e9ff480ecb15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:       {DA.username}\")\n",
    "print(f\"Catalog Name:   {DA.catalog_name}\")\n",
    "print(f\"Schema Name:    {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0601c8f4-2341-4853-a80b-1e93691acb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Offline vs. Online Feature Tables For Real-Time Inferencing\n",
    "\n",
    "Let's take a moment to discuss the importance of feature tables with real-time model serving.\n",
    "\n",
    "We make the distinction to demonstrate real-time model serving *with* and *without* utilizing feature lookups, since the setup for utilizing offline and online tables are handled differently with Model Serving on Databricks. When using real-time model serving with Databricks, you can use **offline** tables *without* utilizing feature lookups or **online** tables *with* feature lookups (in which case Databricks provides automatic feature lookup). To utilize **offline** tables with feature lookups, there is batch inferencing via the `score_batch` method from the Databricks SDK.\n",
    "\n",
    "> **Fundamentally**, a **feature table** is a **materialized Delta table with a primary key** that we want to use for model training. Feature lookups must be configured prior to training your model. For further reading and references of offline vs online feature tables, see the Appendix at the end of this demo.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 1: Real-time Deployment With Offline Feature Tables\n",
    "\n",
    "Here we consider a scenario where you have already gone through the development process (data preparation, and model development) and you're ready to deploy a model with offline features. We will first look at deploying two models that were created as a part of the classroom setup â€“ a champion model and a challenger model with aliases `champion` and `challenger`, respectively.\n",
    "\n",
    "We will serve our two models using a 50/50 traffic split for A/B Testing. First, let's read in our data and explore its lineage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5db5d55-80a3-489f-b601-0c7d118194d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Inspect Offline The Feature Table and Model Versions\n",
    "\n",
    "For this demonstration, we will use a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including internet subscription details such as subscription plans, monthly charges and payment methods.\n",
    "\n",
    "As a part of the classroom setup for this course, a feature table was created called **features** that **did not** include feature lookups. This is the table we are reading in during the next step.\n",
    "\n",
    "---\n",
    "\n",
    "### Lineage Inspection\n",
    "\n",
    "- Navigate to the catalog and schema used with this Vocareum environment (see the output from the previous cell).\n",
    "- Find the table called `features` and model called `ml_model`.\n",
    "  - Click on **Lineage**.\n",
    "  - Click on **See lineage graph** and inspect it. This will show the footprint of how the catalog assets were made.\n",
    "\n",
    "### Step 2: Read in Features and Response Variable from Feature Store\n",
    "\n",
    "Here we will read in our dataset and split between features and response variables. We will show how this can be performed with the Databricks SDK using the Feature Engineering Client.\n",
    "\n",
    "---\n",
    "\n",
    "#### What's the difference between `fe.read_table()` and `read.spark.table()`?\n",
    "\n",
    "Essentially, we use `fe.read_table()` whenever we are specifically working with feature tables stored within Feature Store and `spark.read.table()` for general-purpose reading.  \n",
    "Note that `fe.read_table()` is part of the Databricks Feature Engineering API and integrates well with other Feature Store APIs like logging models  \n",
    "(see *Part 2: Real-Time Deployment with Online Feature Tables*).\n",
    "\n",
    "On the other hand, `spark.read.table()` is a broader Spark SQL method for reading data from any table within the Spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1937c5ac-5b70-40aa-b86d-401b6f10f4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# Initialize Feature Engineering Client\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Define primary key\n",
    "primary_key = \"customerID\"\n",
    "\n",
    "# Read in feature table\n",
    "feature_table_name = f\"{DA.catalog_name}.{DA.schema_name}.features\"\n",
    "X_train_df = fe.read_table(name=feature_table_name)\n",
    "X_train_pdf = X_train_df.drop(primary_key).toPandas()\n",
    "\n",
    "# Read in response table\n",
    "response_table_name = f\"{DA.catalog_name}.{DA.schema_name}.response\"\n",
    "Y_train_df = spark.read.table(response_table_name)\n",
    "Y_train_pdf = Y_train_df.drop(primary_key).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90114a50-b71d-4724-9dc6-20d4498750bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Real-time A/B Testing with Model Serving\n",
    "\n",
    "Let's serve the two models we logged in the previous step using Model Serving. Model Serving supports endpoint management via the UI and the API.\n",
    "\n",
    "Below you will find instructions for using the UI and it is simpler method compared to the API. **In this demo, we will use the API to configure and create the endpoint.**\n",
    "\n",
    "**Both the UI and the API support querying created endpoints in real-time.** We will use the API to query the endpoint using a test-set.\n",
    "\n",
    "---\n",
    "\n",
    "> **What is A/B Testing?**  \n",
    "> A/B testing is a method to compare two versions of a model or system by splitting user traffic and measuring performance metrics to determine which version delivers better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12109d95-8e53-4f31-a62a-741accb820da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 1: Serve model(s) using UI\n",
    "\n",
    "After registering the (new version(s) of the) model to the model registry. To provision a serving endpoint via UI, follow the steps below.\n",
    "\n",
    "1. In the left sidebar, click **Serving**.\n",
    "2. To create a new serving endpoint, click **Create serving endpoint**.\n",
    "   a. In the **Name** field, type a name for the endpoint.  \n",
    "   b. Click in the **Entity** field. A dialog appears. Go to **My models**, and then select the catalog, schema, and model from the drop-down menus.  \n",
    "   c. In the **Version** drop-down menu, select the version of the model to use.  \n",
    "   d. Click **Confirm**.  \n",
    "   e. In the **Compute Scale-out** drop-down, select Small, Medium, or Large. If you want to use GPU serving, select a GPU type from the **Compute type** drop-down menu.  \n",
    "   f. _[OPTIONAL]_ To deploy another model (e.g. for A/B testing), click on **+Add Served Entity** and fill the above mentioned details.  \n",
    "   g. Click **Create**. The endpoint page opens and the endpoint creation process starts.\n",
    "\n",
    "See the Databricks documentation for details [AWS](https://docs.databricks.com) | [Azure](https://docs.databricks.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "276bdc68-a76c-44a8-aea9-ea622cf0397b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 2: Serve Model(s) Using the Databricks Python SDK\n",
    "\n",
    "#### Get Models to Serve\n",
    "\n",
    "In order to serve the model, we will initialize the MLflow client with `MLflowClient` and the workspace client with `WorkspaceClient`. We will configure the MLflow client to point to Unity Catalog instead of the Workspace with `set_registry_uri(\"databricks-uc\")`. The workspace client will be used to create the model serving endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2049b37a-d250-456f-9b99-2166f9d13c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointTag\n",
    "\n",
    "# Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Initialize MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Initialize workspace client\n",
    "w = WorkspaceClient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0631310-5650-4c36-a322-5c61a08c0513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define variables that will be used for configuring the endpoint like `model_name`. The output from running the next cell will show version 1 of our model registered as the champion model and version 2 as being the challenger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "790948a1-1969-49bc-a708-76887f1abeda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = f\"dbacademy.{DA.schema_name}.ml_model\"\n",
    "\n",
    "# Parse model name from UC namespace\n",
    "served_model_name = model_name.split(\".\")[-1]\n",
    "\n",
    "# Define the endpoint name\n",
    "endpoint_name = f\"ML_AS_03_Demo4_{DA.unique_name('_')}\"\n",
    "\n",
    "# Get version of our model registered to UC as a part of the classroom setup\n",
    "model_version_champion = client.get_model_version_by_alias(name=model_name, alias=\"Champion\").version  # Get champion version\n",
    "model_version_challenger = client.get_model_version_by_alias(name=model_name, alias=\"Challenger\").version  # Get challenger version\n",
    "\n",
    "print(f\"Model version Champion: {model_version_champion}\")\n",
    "print(f\"Model version Challenger: {model_version_challenger}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6ada219-4d91-4825-8219-cb0e67c575fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configure\n",
    "\n",
    "Define our model serving endpoint with `endpoint_config`. The configuration below shows two versions of the same being deployed (`model_version_champion` and `model_version_challenger`) along with how to configure traffic during inferencing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "629cde71-6774-4342-9951-0c1899326468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import EndpointCoreConfigInput\n",
    "\n",
    "endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version_champion,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version_challenger,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        }\n",
    "    ],\n",
    "    \"traffic_config\": {\n",
    "        \"routes\": [\n",
    "            {\n",
    "                \"served_model_name\": f\"{served_model_name}-{model_version_champion}\",\n",
    "                \"traffic_percentage\": 50\n",
    "            },\n",
    "            {\n",
    "                \"served_model_name\": f\"{served_model_name}-{model_version_challenger}\",\n",
    "                \"traffic_percentage\": 50\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"auto_capture_config\": {\n",
    "        \"catalog_name\": DA.catalog_name,\n",
    "        \"schema_name\": DA.schema_name,\n",
    "        \"table_name_prefix\": \"db_academy\"\n",
    "    }\n",
    "}\n",
    "endpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9b934a1-1daa-421b-b8e5-ba16d37bb335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    w.serving_endpoints.create_and_wait(\n",
    "        name=endpoint_name,\n",
    "        config=endpoint_config,\n",
    "        tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"serve_fs_model_example\"})]\n",
    "    )\n",
    "    print(f\"Creating endpoint {endpoint_name} with models {model_name} versions {model_version_champion} & {model_version_challenger}\")\n",
    "\n",
    "except Exception as e:\n",
    "    if \"already exists\" in e.args[0]:\n",
    "        print(f\"Endpoint with name {endpoint_name} already exists\")\n",
    "    else:\n",
    "        raise(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f78421a4-367a-4f8e-b1ca-ad1634ec4662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Serve the endpoint\n",
    "\n",
    "Use the configuration just created to serve the model.\n",
    "\n",
    "> The time to create a model serving endpoint is roughly 10 minutes on average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e3dbe2a-31b0-4b5b-992e-68b5b8fb4d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    w.serving_endpoints.create_and_wait(\n",
    "        name=endpoint_name,\n",
    "        config=endpoint_config,\n",
    "        tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"serve_fs_model_example\"})]\n",
    "    )\n",
    "    print(f\"Creating endpoint {endpoint_name} with models {model_name} versions {model_version_champion} & {model_version_challenger}\")\n",
    "\n",
    "except Exception as e:\n",
    "    if \"already exists\" in e.args[0]:\n",
    "        print(f\"Endpoint with name {endpoint_name} already exists\")\n",
    "    else:\n",
    "        raise(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c450c01f-55b0-4391-ba1d-ab317acae204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verify Endpoint Creation\n",
    "\n",
    "Let's verify that the endpoint is created and ready to be used for inference using the `assert` command, which is used to check whether a given condition is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f0bb97b-51e1-4bbc-a60c-304e915b3e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint = w.serving_endpoints.wait_get_serving_endpoint_not_updating(endpoint_name)\n",
    "\n",
    "assert endpoint.state.config_update.value == \"NOT_UPDATING\" and endpoint.state.ready.value == \"READY\", \"Endpoint not ready or failed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab961803-f9cb-4fa0-b20c-90af7994bc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query the Endpoint and Visualize\n",
    "\n",
    "Here we will use the training dataset to query our endpoint.\n",
    "\n",
    "1. Define the dataset to sample from.  \n",
    "2. Query by batch to highlight model-split traffic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c49f79b-3f03-49e9-a454-6ac082c20637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe_records = X_train_pdf.iloc[:1000].to_dict(orient='records')  # 1k sample records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e885c14c-79d0-4cb6-ad62-60e987027274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we will query in batches so we can see the traffic split per 100 rows (there are around 2000 rows in this dataset)\n",
    "\n",
    "To help visualize the A/B testing output, create a visual using the UI (you only need to do this once; rerunning the cell will update the visualization).\n",
    "\n",
    "1. After running the next cell, select the + sign on the second table and select **Visualization**.  \n",
    "2. The default visual should represent the Yes/No split per model.\n",
    "\n",
    "> Since the dataset we're working with is not very large, you might have to run the cell a few times to get a fairly close 50/50 split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00b671e5-de50-46b2-8429-0c81b21a323d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Inference results:\")\n",
    "\n",
    "batch_size = 100  # Number of records per batch\n",
    "num_batches = (len(dataframe_records) + batch_size - 1) // batch_size  # Total number of batches\n",
    "\n",
    "all_predictions = []\n",
    "all_models = []\n",
    "\n",
    "# Process data in batches\n",
    "for i in range(num_batches):\n",
    "    batch_records = dataframe_records[i * batch_size : (i + 1) * batch_size]  # Slice batch\n",
    "\n",
    "    # Query the model serving endpoint\n",
    "    query_response = w.serving_endpoints.query(name=endpoint_name, dataframe_records=batch_records)\n",
    "\n",
    "    # Collect predictions and model served details\n",
    "    all_predictions.extend(query_response.predictions)\n",
    "    all_models.extend([query_response.served_model_name] * len(query_response.predictions))  # Duplicate model name per prediction\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"prediction\": all_predictions,\n",
    "    \"model_served\": all_models\n",
    "})\n",
    "\n",
    "# Count occurrences of predictions\n",
    "count_results = results_df['prediction'].value_counts().reset_index()\n",
    "count_results.columns = ['prediction', 'count']\n",
    "\n",
    "# Aggregate count of predictions per model\n",
    "model_count_results = results_df.groupby([\"model_served\", \"prediction\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Display results grouped by model and prediction type\n",
    "display(model_count_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "172ce7db-0261-4a64-b6ab-50735c4cae07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Real-time Deployment with Online Feature Tables\n",
    "\n",
    "In the previous section we deployed a model that utilized an offline feature table without utilizing feature lookups. In this section we will build a model that utilizes feature lookups with an online table and serve this model. Here are the steps we will take:\n",
    "\n",
    "1. Create a feature function that computes the average monthly usage charges per customer.\n",
    "2. Bundle the feature lookups and feature function into one feature-defining object called `features`.\n",
    "3. Use the Databricks SDK to create the online feature table using the same feature table from part 1.\n",
    "4. Train an ML model using `features`. By creating a model using feature lookups, we will enable automatic feature lookups when deploying the model to a model serving endpoint. This requires no additional configuration with the online feature table.\n",
    "5. Create a model serving endpoint.\n",
    "6. Query the model serving endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Create a Feature Function\n",
    "\n",
    "Here we will create a feature function that uses a Python UDF to create on-demand features.\n",
    "\n",
    "### On-Demand Features\n",
    "\n",
    "\"On-demand\" refers to features whose values are not known ahead of time, but are *calculated at the time of inference*. In this demo, we will calculate the **average monthly charges** on the fly. This is done by defining a `UDF` with SQL and registering it to Unity Catalog. **The function will be registered with the name** `monthly_charges_avrg` using the syntax `CREATE OR REPLACE FUNCTION`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4398b454-0639-4883-ad3c-2758a2d115f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION monthly_charges_avrg (TotalCharges DOUBLE, tenure DOUBLE)\n",
    "RETURNS DOUBLE\n",
    "LANGUAGE PYTHON AS\n",
    "$$\n",
    "avrg = TotalCharges / tenure\n",
    "return avrg\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cac8e8f-5476-4e9c-b814-ae6d0aa8ba43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Define Combined Features\n",
    "\n",
    "Now that we have both an **online features table** and **on-demand features** created, we can combine these together to be passed to the model training. We combine these two Unity Catalog assets and store them as a single object called `features`. Then use the Databricks SDK to create a `FeatureSpec` to bundle features into single feature-defining object.\n",
    "\n",
    "> Feature lookups and feature functions must be created prior to training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52830c9a-d5af-4dff-86a9-fa971d8d1f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_for_online_name = f\"{DA.catalog_name}.{DA.schema_name}.features\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec19369c-ecdd-490c-a6e3-57b230a66387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureLookup, FeatureFunction\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "features = [\n",
    "    FeatureLookup(\n",
    "        table_name=features_for_online_name,\n",
    "        lookup_key=\"primary_key\"\n",
    "    ),\n",
    "    FeatureFunction(\n",
    "        udf_name=\"monthly_charges_avrg\",\n",
    "        output_name=\"m_charges_avrg\",\n",
    "        input_bindings={\n",
    "            \"TotalCharges\": \"TotalCharges\",\n",
    "            \"tenure\": \"tenure\"\n",
    "        }\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881a3952-b2be-49f3-8248-e249703deebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Create an Online Table\n",
    "\n",
    "In this section, we will create an online table to serve feature table for real-time inference. When using Model Serving to serve a model that was built using features from Databricks, the model automatically looks up and transforms features for inference requests.\n",
    "\n",
    "> ðŸ›ˆ Databricks Online Tables can be created and managed via the UI and the SDK. While we provided instructions for both of these methods, you can pick one option for creating the table.\n",
    "\n",
    "---\n",
    "\n",
    "#### OPTION 1: Create Online Table via the UI\n",
    "\n",
    "You create an online table from the Catalog Explorer. The steps are described below. For more details, see the Databricks documentation ([AWS](https://docs.databricks.com) | [Azure](https://docs.databricks.com)).\n",
    "\n",
    "In **Catalog Explorer**, navigate to the source table that you want to sync to an online table.\n",
    "\n",
    "From the kebab menu, select **Create online table**.\n",
    "\n",
    "- Use the selectors in the dialog to configure the online table.\n",
    "\n",
    "  - **Name**: Name to use for the online table in Unity Catalog.\n",
    "  - **Primary Key**: Column(s) in the source table to use as primary key(s) in the online table.\n",
    "  - **Timeseries Key** *(Optional)*: Column in the source table to use as timeseries key. When specified, the online table includes only the row with the latest timeseries key value for each primary key.\n",
    "  - **Sync mode**: Select `Snapshot` for Sync mode. Please refer to the documentation for more details about available options.\n",
    "  - When you are done, click **Confirm**. The online table page appears.\n",
    "\n",
    "- The new online table is created under the catalog, schema, and name specified in the creation dialog. In Catalog Explorer, the online table is indicated by online table icon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a11cb44-668c-4898-8a81-5fbe9c96b82f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OPTION 2: Use the Databricks SDK\n",
    "\n",
    "The first option for creating an online table is using the UI. The alternative is using Databricks' [python-sdk](https://docs.databricks.com).\n",
    "\n",
    "**ðŸš¨ Note:** The workspace must be enabled for using the SDK for creating and managing online tables. You can run following code blocks if your workspace is enabled for this feature.\n",
    "\n",
    "> The following code alters your existing feature table using change data feed (CDF). Essentially, this allows tracking of row-level changes between versions of our feature table (any Delta table in general).\n",
    "\n",
    "---\n",
    "\n",
    "Define the name for our online table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b15d19-c0bf-40a8-ae21-aff0ec8194fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.catalog import (\n",
    "    OnlineTableSpec,\n",
    "    OnlineTable,\n",
    "    OnlineTableSpecTriggeredSchedulingPolicy\n",
    ")\n",
    "\n",
    "online_table_name = f\"{DA.catalog_name}.{DA.schema_name}.online_features\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "579b5bf0-4c0f-429b-895c-351abc412c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Drop the table if it already exists and enable change data feed for `features_for_online`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebe95c15-9aa9-4b16-83fd-75c97e531e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Drop the online table if it already exists\n",
    "    w.online_tables.delete(online_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Enable CDF for the table\n",
    "spark.sql(f\"\"\"ALTER TABLE {features_for_online_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eebc7ed-d69e-40fd-9560-884ce2ac69d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configure online table initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "842fe1f5-4a00-488b-ad0f-085d3901da0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an online table\n",
    "spec = OnlineTableSpec(\n",
    "    primary_key_columns=[primary_key],\n",
    "    source_table_full_name=features_for_online_name,\n",
    "    run_triggered=OnlineTableSpecTriggeredSchedulingPolicy.from_dict({'triggered': 'true'}),\n",
    "    perform_full_copy=True\n",
    ")\n",
    "\n",
    "online_table = OnlineTable(\n",
    "    name=online_table_name,\n",
    "    spec=spec\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8185f2-7b5d-4ae9-a96a-1c5002c8f798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create an online table based off `features`\n",
    "\n",
    "> **Does this mean my original model versions are now using feature lookups?**\n",
    "> No. Because feature lookups were *not* configured during model training, the model serving endpoint will not \"know\" to perform automatic feature lookup. This step simply syncs the feature table to an online table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13fc3f68-905c-4164-a56b-1b906be7adc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    online_table_pipeline = w.online_tables.create_and_wait(table=online_table)\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        pass\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(w.online_tables.get(online_table_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8adcf443-55ab-452a-95fb-0bd989d17c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Fit and Log the Model with Online Feature Table\n",
    "\n",
    "Next, we will use the feature engineering client from the Databricks SDK to create our training set that includes the `feature_lookups` parameter â€“ which is our bundled `features` object from the previous cell.\n",
    "\n",
    "The function `fit_and_register_model` is used in the cell below. This function is created as a part of the classroom setup, but we provide the code here for completeness.\n",
    "\n",
    "```python\n",
    "def fit_and_register_model(\n",
    "    feature_df,\n",
    "    response_df,\n",
    "    model_name_,\n",
    "    random_state_,\n",
    "    model_alias=None,\n",
    "    training_set_spec_=None,\n",
    "    is_online=False\n",
    "):\n",
    "    \"\"\"Train and register a Decision Tree model.\"\"\"\n",
    "    clf = DecisionTreeClassifier(random_state=random_state_)\n",
    "    if is_online:\n",
    "        X = feature_df # pyspark dataframe\n",
    "        y = response_df # pyspark dataframe\n",
    "    else:\n",
    "        feature_pdf = feature_df.df.toPandas() # instance of an mlflow.data.Dataset\n",
    "        response_pdf = response_df.df.toPandas() # instance of an mlflow.data.Dataset\n",
    "        dataset = feature_pdf.merge(response_pdf, on=\"customerID\", how=\"inner\")\n",
    "        # Prepare X and y\n",
    "        X = dataset.drop(columns=[\"customerID\", \"Churn\"])  # Drop unnecessary columns\n",
    "        y = dataset[\"Churn\"]\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Train_DecisionTree_{random_state_}\"):\n",
    "        mlflow.sklearn.autolog(\n",
    "            log_input_examples=True,\n",
    "            log_models=False,\n",
    "            log_post_training_metrics=True,\n",
    "            silent=True\n",
    "        )\n",
    "\n",
    "        clf.fit(X, y) # Fit the model\n",
    "\n",
    "        # Log model\n",
    "        if is_online:\n",
    "            try:\n",
    "                output_schema = _infer_schema(y)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "                output_schema = None\n",
    "\n",
    "            fe = FeatureEngineeringClient()\n",
    "            # Log the original dataset that supports mlflow logging\n",
    "            fe.log_model(\n",
    "                model=clf,\n",
    "                artifact_path=\"decision_tree\",\n",
    "                flavor=mlflow.sklearn,\n",
    "                training_set=training_set_spec_,\n",
    "                output_schema=output_schema,\n",
    "                registered_model_name=model_name_\n",
    "            )\n",
    "        else:\n",
    "            # Log the original dataset that supports mlflow logging\n",
    "            mlflow.log_input(feature_df, \"training_features\")\n",
    "            mlflow.log_input(response_df, \"training_responses\")\n",
    "            input_example = X.iloc[[0]]\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=clf,\n",
    "                artifact_path=\"ml_model\",\n",
    "                input_example=input_example,\n",
    "                registered_model_name=model_name_,\n",
    "            )\n",
    "\n",
    "        # Assign alias if provided\n",
    "        if model_alias:\n",
    "            time.sleep(8)  # Shorter wait time before updating alias\n",
    "            latest_version = get_latest_model_version(model_name_)\n",
    "            client.set_registered_model_alias(model_name_, model_alias, latest_version)\n",
    "\n",
    "    return clf\n",
    "``` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25409b20-8d3a-47ae-b6c0-fca653df52b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_set_spec = fe.create_training_set(\n",
    "    df=Y_train_df,  # response_df\n",
    "    label=\"Churn\",  # response\n",
    "    feature_lookups=features,\n",
    "    exclude_columns=[primary_key]\n",
    ")\n",
    "\n",
    "# Load training dataframe based on defined feature-lookup specification\n",
    "training_df = training_set_spec.load_df()\n",
    "\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf2 = training_df.drop(\"Churn\").toPandas()\n",
    "Y_train_pdf2 = training_df.select(\"Churn\").toPandas()\n",
    "\n",
    "fit_and_register_model(\n",
    "    X_train_pdf2,\n",
    "    Y_train_pdf2,\n",
    "    model_name,\n",
    "    20,\n",
    "    model_alias=\"Online\",\n",
    "    training_set_spec_=training_set_spec,\n",
    "    is_online=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dc9e3c3-0b79-46a9-8d27-bf47b32d5e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inspect the Lineage\n",
    "\n",
    "At this point, you can navigate to the registered model within Unity Catalog and inspect the alias and lineage.\n",
    "\n",
    "> The alias was configured using the MLflow client while the lineage was generated using the Databricks SDK (`FeatureEngineeringClient()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a3ee313-bf47-4771-9015-efb68cb7f1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Deploy the Model with Online Features\n",
    "\n",
    "Now that we have a model registered to Unity Catalog, we can deploy the model with Mosaic AI Model Serving and use the online table at the time of inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf6b7fb5-d9d6-4b3a-800a-80dcbad345ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  configure the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "683eafd7-9dd3-4597-b643-e8ef77bbb242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure the endpoint\n",
    "fs_endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": fs_model_version,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "fs_endpoint_config = EndpointCoreConfigInput.from_dict(fs_endpoint_config_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9901fe7-1256-4659-b6e6-9a183fa86b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### serve the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e64ae2f-19d2-47e0-958a-cd2e8a94e6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Serve the endpoint\n",
    "try:\n",
    "    w.serving_endpoints.create_and_wait(\n",
    "        name=fs_endpoint_name_online,\n",
    "        config=fs_endpoint_config,\n",
    "        tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"serve_fs_model_example\"})]\n",
    "    )\n",
    "    print(f\"Creating endpoint {fs_endpoint_name_online} with models {model_name} versions {fs_model_version}\")\n",
    "\n",
    "except Exception as e:\n",
    "    if \"already exists\" in e.args[0]:\n",
    "        print(f\"Endpoint with name {fs_endpoint_name_online} already exists\")\n",
    "    else:\n",
    "        raise(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e98ea94-8d83-4d8b-adbc-de493aed076b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6: Query the Model Serving Endpoint\n",
    "\n",
    "We'll now query the served model using the Databricks SDK like we showed in Part 1 with the offline features table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe78ea93-85d2-412a-9612-e0a1da0dc545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe_records_lookups_only = X_train_df.select('customerID') \\\n",
    "    .limit(1000) \\\n",
    "    .toPandas() \\\n",
    "    .to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbc6afc7-028a-4362-ab12-e4489acbb4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "print(\"FS Inference results:\")\n",
    "query_response = w.serving_endpoints.query(\n",
    "    name=fs_endpoint_name_online,\n",
    "    dataframe_records=dataframe_records_lookups_only\n",
    ")\n",
    "\n",
    "# Count occurrences of \"Yes\" and \"No\" in predictions from list query_response.predictions\n",
    "prediction_counts = Counter(query_response.predictions)\n",
    "\n",
    "# Convert counts to a Pandas DataFrame\n",
    "df_counts = pd.DataFrame.from_dict(prediction_counts, orient='index', columns=['Count']).reset_index()\n",
    "df_counts.rename(columns={'index': 'Prediction'}, inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bc4dc76-810e-423b-9d15-b1cade2a158c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This demonstration discussed how to deploy and serve machine learning models in real-time using Databricks Model Serving. It covered the differences between offline and online feature tables, configuring a model serving endpoint, and leveraging feature lookups for real-time inference. Additionally, it explores techniques for on-demand feature computation, and A/B testing with real-time model serving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0763b808-b3e8-4b48-8bfc-8ca28da297dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Appendix\n",
    "\n",
    "Below is some additional information regarding offline and online feature tables.\n",
    "\n",
    "### More on Offline and Online Tables with Real-Time Model Serving\n",
    "\n",
    "#### Offline\n",
    "\n",
    "- You can use an existing Delta table in Unity Catalog that includes a primary key constraint as a feature table. If the table does not have a primary key defined, you must update the table using ALTER TABLE DDL statements to add the constraint. See *Use an existing Delta table in Unity Catalog as a feature table*.\n",
    "- Any streaming table or materialized view in Unity Catalog with a primary key can be a feature table in Unity Catalog, and you can use the Features UI and API with the table.\n",
    "- You can update a feature table in Unity Catalog by adding new features or by modifying specific rows based on the primary key.\n",
    "\n",
    "**Additional Reading**: [Working with feature tables in UC](https://www.databricks.com/)\n",
    "\n",
    "#### Online\n",
    "\n",
    "- When a scoring request comes in to the model, Model Serving automatically retrieves the published feature values needed by the model. In this way, the most recent feature values are always used for predictions.\n",
    "- You can create a Python UDF in a notebook or in Databricks SQL.\n",
    "- When a Python UDF depends on the result of a FeatureLookup, the value returned if the requested lookup key is not found depends on the environment. When using score_batch, the value returned is None. When using online serving, the value returned is float(\"nan\").\n",
    "- Models packaged with feature metadata can be registered to Unity Catalog. The feature tables used to create the model must be stored in Unity Catalog.\n",
    "\n",
    "**Additional Reading**: [Use features in online workflows](https://www.databricks.com/), [Compute features on demand](https://www.databricks.com/)\n",
    "\n",
    "### Feature Serving â€“ [Feature Serving Endpoints](https://www.databricks.com/)\n",
    "\n",
    "When you use Mosaic AI Model Serving to serve a model that was built using features from Databricks, the model automatically looks up and transforms features for inference requests. With Databricks Feature Serving, you can serve structured data for retrieval augmented generation (RAG) applications, as well as features that are required for other applications, such as models served outside of Databricks or any other application that requires features based on data in Unity Catalog.\n",
    "\n",
    "Databricks Feature Serving provides a single interface that serves pre-materialized and on-demand features. It also includes the following benefits:\n",
    "\n",
    "- **Simplicity**: Databricks handles the infrastructure. With a single API call, Databricks creates a production-ready serving environment.\n",
    "- **High availability and scalability**: Feature Serving endpoints automatically scale up and down to adjust to the volume of serving requests.\n",
    "- **Security**: Endpoints are deployed in a secure network boundary and use dedicated compute that terminates when the endpoint is deleted or scaled to zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a50d7572-cdc1-42b7-bbd8-6f6e9af21fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12-M3-Demo3: Real-time Deployment with Model Serving2025-07-10 17_31_21",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
