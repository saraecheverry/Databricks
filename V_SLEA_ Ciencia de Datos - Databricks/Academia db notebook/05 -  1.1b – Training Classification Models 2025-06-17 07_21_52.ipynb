{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "189bd554-eb11-4818-bcbd-85855559db65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training Regression and Classification Models\n",
    "\n",
    "In this demo, we will explore the process of training a classification model using the sklearn API. In addition to fitting the model, we will inspect the model details and show how the decision tree is constructed.\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "**By the end of this demo, you will be able to;**\n",
    "\n",
    "- Fit a classification model on modeling data using the sklearn API.\n",
    "- Interpret a fit sklearn linear model’s coefficients and intercept.\n",
    "- Fit a decision tree model using sklearn API and training data.\n",
    "- Visualize a sklearn tree’s split points.\n",
    "- Identify which metrics are tracked by MLflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15892337-58d7-4f0c-8a94-24e27339985a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "- To run this notebook, you need to use one of the following Databricks runtime(s): **13.3.x-cpu-ml-scala2.12**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "160bf77d-7890-4e8c-834e-c42bee4f42cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56223539-d4db-4517-bf56-d596ac25a4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-01.1b\n",
    "# Note: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34118a9f-a77f-4767-97e8-6bc13f8775f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Other Conventions:\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bc56c40-7dd9-4a1a-8a17-f1dc90a7d0e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username: {DA.username}\")\n",
    "print(f\"Catalog Name: {DA.catalog_name}\")\n",
    "print(f\"Schema Name: {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location: {DA.paths.datasets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e6dd95b-ace0-4e98-a6b0-ff79101f8858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Before training any machine learning models, it's crucial to prepare the dataset. In the previous section, we have covered the steps to load, clean, and preprocess the data, ensuring it's in a suitable format for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65bc5b4a-2765-4f9b-a27c-ea1a6bdd9395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Dataset\n",
    "\n",
    "In this section, we aim to optimize the process of loading the dataset by leveraging Delta Lake's feature table functionality. Instead of directly reading from the CSV file, **we created a feature table during the setup phase**. A feature table is a structured repository that organizes data for efficient retrieval and analysis. By creating a feature table, we enhance data management and simplify subsequent operations. We can then seamlessly read the data from this feature table during our analysis, promoting a more organized and scalable approach to handling datasets. This setup enhances traceability and facilitates collaboration across different stages of the data science workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98b47056-0690-4e2e-9267-ab62bfca2061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# Read dataset from the feature store table\n",
    "fe = FeatureEngineeringClient()\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.diabetes_binary\"\n",
    "feature_data_pd = fe.read_table(name=table_name).toPandas()\n",
    "feature_data_pd = feature_data_pd.drop(columns=['unique_id'])\n",
    "\n",
    "# Convert all columns in the DataFrame to the 'double' data type\n",
    "for column in feature_data_pd.columns:\n",
    "    feature_data_pd[column] = feature_data_pd[column].astype(\"double\")\n",
    "\n",
    "# Display the Pandas DataFrame with updated data types\n",
    "display(feature_data_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb3f621-e58c-485c-a886-5b6b68cc7a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train / Test Split\n",
    "\n",
    "The `train_test_split` function from the scikit-learn library is commonly used to split a dataset into training and testing sets. This is a crucial step in machine learning to **evaluate how well a trained model generalizes to new, unseen data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd976006-a162-4f5a-b23d-92088225b14b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"We have {feature_data_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into its own dataset\n",
    "target_col = \"Diabetes_binary\"\n",
    "X_all = feature_data_pd.drop(labels=target_col, axis=1)\n",
    "y_all = feature_data_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.95, random_state=42)\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9510f165-980d-4507-9aa5-c0f48fe491b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fit a Classification Model\n",
    "\n",
    "Let's go ahead and fit a Decision Tree model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f27a18b7-9198-41ac-b5b9-25509833b6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import mlflow.sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# turn on autologging\n",
    "mlflow.sklearn.autolog(log_input_examples=True)\n",
    "\n",
    "# fit our model\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc_mdl = dtc.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the test set\n",
    "y_predicted = dtc_mdl.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_predicted)\n",
    "test_prec = precision_score(y_test, y_predicted)\n",
    "test_rec = recall_score(y_test, y_predicted)\n",
    "test_f1 = f1_score(y_test, y_predicted)\n",
    "\n",
    "print(\"Test evaluation summary:\")\n",
    "print(f\"Accuracy: {test_acc}\")\n",
    "print(f\"Precision: {test_prec}\")\n",
    "print(f\"Recall: {test_rec}\")\n",
    "print(f\"F1: {test_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d91b679-d7e3-4f50-9dcb-9b9d42ec139a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Examine Model Result\n",
    "\n",
    "Examine the **confusion matrix** to visualize the model's classification performance.\n",
    "\n",
    "The confusion matrix provides insights into the model's performance, showing how many instances were correctly or incorrectly classified for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91c98c1f-0a2d-4a45-b370-398e0d3a890d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Computing the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predicted, labels=[1, 0])\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 0])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb20eda9-1005-4628-959d-0cddd76d275c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Now we examine the resulting model\n",
    "\n",
    "We can extract and plot the `feature_importances_` inferred from this model to examine which data features are **the most critical for successful prediction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5deaffbb-0470-4c17-a80a-410de3e6789c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Retrieving feature importances\n",
    "feature_importances = dtc_mdl.feature_importances_\n",
    "feature_names = X_train.columns.to_list()\n",
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "y_pos = np.arange(len(feature_names))\n",
    "plt.bar(y_pos, feature_importances, align='center', alpha=0.7)\n",
    "plt.xticks(y_pos, feature_names, rotation=45)\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances in Decision Tree Classifier')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72fefb26-a650-4c1e-847e-1b14862fcc5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### We can also examine the resulting tree structure\n",
    "\n",
    "**Decision trees make splitting decisions on different features at different critical values**, so we can visualize the resulting decision logic by plotting that branching tree structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96088449-ba41-49f1-91a2-bcf074b2e2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The fitted DecisionTreeClassifier model has {dtc_mdl.tree_.node_count} nodes and is up to {dtc_mdl.tree_.max_depth} levels deep.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c6b2db-8541-4726-94d6-527962b239a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is a very large decision tree, printing out the full tree logic, we can see it is vast and sprawling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c3e9b16-e156-488f-bbc3-9db1ce48282e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "text_representation = export_text(dtc_mdl, feature_names=feature_names)\n",
    "print(text_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a0ff4c3-bce1-467f-b1ac-91e12daff051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This plot will give you a visual representation of the decision tree structure, helping us to understand how the model makes predictions based on different features and their critical values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f881241c-4dfe-477d-868c-da99eb2e5839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since it is so big, we can only reasonably visualize a small portion of the tree at any given time. Here is the root and first 2 levels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfe1dbc9-a550-4414-a1a5-ee78c3d0eb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plot_tree(dtc_mdl,\n",
    "          feature_names=feature_names,\n",
    "          max_depth=2,\n",
    "          class_names=['0', '1'],\n",
    "          filled=True)\n",
    "plt.title('Decision Tree Structure')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05 -  1.1b – Training Classification Models 2025-06-17 07_21_52",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
