{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d5966df-a7cd-4b4d-be4b-f14663de06f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training Unsupervised Models\n",
    "\n",
    "In this demo, we will explore **unsupervised learning**, a method where the model finds patterns in **unlabeled data** without predefined categories. We will use **text embeddings** to convert text into numerical representations and apply **K-Means clustering** to group similar text documents.\n",
    "\n",
    "To improve clustering efficiency, we will **reduce the dimensionality** of embeddings using **Principal Component Analysis (PCA)**. We will also use evaluation techniques like the **Elbow Method** and **Silhouette Score** to determine the best number of clusters and assess clustering quality.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "**By the end of this demo, you will be able to:**\n",
    "\n",
    "- **Generate text embeddings** using the embeddings model [General Text Embeddings (GTE)](https://www.example.com) to represent text numerically.\n",
    "- **Apply dimensionality reduction** (PCA) to optimize clustering performance.\n",
    "- **Train an unsupervised K-Means model** to discover patterns in text data.\n",
    "- **Determine the optimal number of clusters** using the Elbow Method.\n",
    "- **Evaluate clustering quality** using Silhouette Score.\n",
    "- **Visualize clustering results** for better interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a901ecd1-86b4-4f0e-84a1-898b6e1980f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade threadpoolctl scikit-learn\n",
    "%pip install kneed\n",
    "%restart_python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b44158-bb44-496d-b147-774b54483a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "- To run this notebook, you need to use one of the following Databricks runtime(s): **16.0.x-cpu-ml-scala2.12**\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7840c26-fcd6-4080-bd95-e86a9122ec41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.  \n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down.  \n",
    "     Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "4. Once the cluster is running, complete the steps above to select your cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae2b5064-f358-45d0-b66a-c127e011763b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b9092e-e9b0-4e99-b19b-e2dd2df4252e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-1.1bUS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0ea608-8452-4ae6-ade8-4527ba43f636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Other Conventions:\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd0a9928-d8e3-4ebc-9d0f-2aa63ef83427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:           {DA.username}\")\n",
    "print(f\"Catalog Name:       {DA.catalog_name}\")\n",
    "print(f\"Schema Name:        {DA.schema_name}\")\n",
    "print(f\"Working Directory:  {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:   {DA.paths.datasets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c253d40-1d1b-451b-98b6-e9e0043a7600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data & Generate Embeddings\n",
    "\n",
    "Before we can apply **unsupervised learning**, we need to load and process our dataset. In this step, we will:\n",
    "\n",
    "- Load the **AG News dataset** from a **Databricks feature table**.\n",
    "- Extract the **text column** for processing.\n",
    "- Prepare the data for **embedding generation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d0fc96b-1e15-4360-b203-e50b74f3b57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load the Dataset\n",
    "\n",
    "We use the **AG News dataset**, which contains news articles, to perform text clustering. The dataset is stored in a **Databricks feature table**, and we will load it as a **Spark DataFrame**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49957ea7-9954-4fd8-8eed-bc75bd50cb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load AG News dataset as a Spark DataFrame (Feature Table)\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.ag_news_features\"\n",
    "news_df = spark.read.table(table_name)\n",
    "\n",
    "# Select only the 'text' column (avoiding unnecessary columns)\n",
    "news_texts_df = news_df.select(col(\"text\"))\n",
    "\n",
    "# Display the Spark DataFrame\n",
    "display(news_texts_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f313ebb6-6b80-4487-906e-c878db5d702f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Text Embeddings Using gte-large\n",
    "\n",
    "Now that we have loaded our text dataset, the next step is to convert the text data into **numerical representations** using **text embeddings**.  \n",
    "Here we will demonstrate how easy it is to take our text and embed it using a foundational model from Mosaic AI Model Serving.  \n",
    "In particular, we will be using the `get_open_ai_client()` method, which is part of the Databricks SDK that provides a convenient way to create an OpenAI-compatible client for interacting with the foundation model.  \n",
    "For other methods of querying, please see [this documentation](https://docs.databricks.com/en/machine-learning/foundation-models/index.html).\n",
    "\n",
    "### Steps:\n",
    "- **Step 1:** Initialize OpenAI Client  \n",
    "- **Step 2:** Define Embedding Function  \n",
    "- **Step 3:** Convert Text to Embeddings  \n",
    "- **Step 4:** Convert embeddings list to a Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19c0ed5c-5863-4d9b-8f05-bb1e9367f88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize Databricks OpenAI Client\n",
    "workspace_client = WorkspaceClient()\n",
    "openai_client = workspace_client.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "# Function to get embeddings for a batch of text\n",
    "def get_embeddings_batch(text):\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=\"databricks-gte-large-en\",\n",
    "        input=text\n",
    "    )\n",
    "    return [res.embedding for res in response.data]\n",
    "\n",
    "# Convert DataFrame to list on the driver\n",
    "news_texts_list = news_texts_df.select(\"text\").rdd.map(lambda row: row[\"text\"]).collect()\n",
    "\n",
    "# Process in batches to reduce API calls\n",
    "batch_size = 100  # Adjust as needed based on API rate limits\n",
    "embeddings_list = []\n",
    "\n",
    "for i in range(0, len(news_texts_list), batch_size):\n",
    "    batch = news_texts_list[i:i + batch_size]\n",
    "    embeddings_list.extend(get_embeddings_batch(batch))\n",
    "\n",
    "# Create DataFrame with embeddings\n",
    "embeddings_df = spark.createDataFrame(zip(news_texts_list, embeddings_list), [\"text\", \"embedding\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01bf0e1a-a26b-4323-b795-9e55683f677a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Standardization and Dimensionality Reduction\n",
    "\n",
    "Now that we have generated **text embeddings**, we need to prepare them for clustering by applying **standardization** and **dimensionality reduction**.\n",
    "\n",
    "### Why Do We Need This Step?\n",
    "\n",
    "- **Standardization** ensures that all features have a similar scale, preventing some features from dominating others.\n",
    "- **Dimensionality Reduction** using [Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) helps reduce the number of features while retaining important information.  \n",
    "  This makes clustering more efficient and easier to visualize. In particular, we will be converting our embedding from 1024 dimensions down to 2 dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b2fb959-5159-4b5c-969f-8137b6ab1b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Convert Spark DataFrame to NumPy array (Extract embeddings)\n",
    "embeddings_np = np.array([row[\"embedding\"] for row in embeddings_df.select(\"embedding\").collect()])\n",
    "\n",
    "# Step 1: Standardization\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings_np)\n",
    "\n",
    "# Step 2: Dimensionality Reduction using PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2D for visualization\n",
    "embeddings_pca = pca.fit_transform(embeddings_scaled)\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "pca_df = spark.createDataFrame(\n",
    "    [(int(i), float(pc1), float(pc2)) for i, (pc1, pc2) in enumerate(embeddings_pca)],\n",
    "    [\"unique_id\", \"PC1\", \"PC2\"]\n",
    ")\n",
    "\n",
    "# Display the transformed embeddings\n",
    "display(pca_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117e2315-ecf3-45a3-83c0-3ee1f80ca41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Determine the Optimal Number of Clusters (Elbow Method)\n",
    "\n",
    "Before applying [K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering, we need to determine the **best number of clusters (K)**.  \n",
    "We use the [Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)), which helps identify the point where adding more clusters **no longer significantly reduces inertia (sum of squared distances to cluster centers)**.\n",
    "\n",
    "### How Does the Elbow Method Work?\n",
    "\n",
    "- We run **K-Means** clustering for different values of K (from 1 to 10).\n",
    "- We measure **inertia** (how well points fit within their assigned cluster).\n",
    "- We plot inertia against K and look for the **elbow point** where the decrease in inertia slows down.\n",
    "- The **optimal K** is found using **KneeLocator**, which detects the elbow point automatically.\n",
    "\n",
    "### Why not just minimize inertia?\n",
    "\n",
    "- Minimizing inertia can lead to *overfitting* (continuously decreasing while increasing the number of clusters will fit noise rather than meaningful patterns).\n",
    "- The elbow method provides interpretability and voids arbitrary decision-making by providing a point of diminishing returns.\n",
    "\n",
    "> We manually set the environment variable `OMP_NUM_THREADS` to 1 to avoid multithreading and parallelism  \n",
    "> to ensure that each run uses the same computational resources.  \n",
    "> This prevents the creation of too many threads across processes, preventing inefficient CPU utilization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae94d982-295c-4c5b-967c-175658be6aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import threadpoolctl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# Apply fixes for parallel processing\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "threadpoolctl.threadpool_limits(limits=1, user_api=\"blas\")\n",
    "\n",
    "# Perform K-Means clustering and compute inertia\n",
    "inertia = []\n",
    "k_values = range(1, 10)  # Try values from 1 to 10\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(embeddings_scaled)  # Ensure embeddings_scaled is preprocessed\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Use KneeLocator to find the elbow point\n",
    "knee_locator = KneeLocator(k_values, inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_k = knee_locator.elbow\n",
    "\n",
    "# Plot Elbow Method with detected optimal k\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(k_values, inertia, marker='o', linestyle='--', label='Inertia')\n",
    "plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal K={optimal_k}')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be090443-0d37-40c7-b3d3-e515ef51e8e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply Clustering Algorithm\n",
    "\n",
    "We will now apply **K-Means Clustering** to group similar news articles together based on their embeddings.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Train the K-Means model** using the `optimal_k`.\n",
    "2. **Assign cluster labels** to each news article.\n",
    "3. **Store clustering results** in a Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d3fd258-3e17-49a2-9887-b3af8eb1c3ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Apply K-Means clustering on the reduced embeddings\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans.fit(embeddings_scaled)  # Fit the model on the standardized embeddings\n",
    "\n",
    "# Get cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Convert labels to a Spark DataFrame\n",
    "labels_df = pd.DataFrame({\"unique_id\": range(len(labels)), \"Cluster\": labels})\n",
    "labels_spark_df = spark.createDataFrame(labels_df)\n",
    "\n",
    "# Join PCA-transformed Spark DataFrame with cluster labels\n",
    "clusters_spark_df = pca_df.join(labels_spark_df, \"unique_id\")\n",
    "\n",
    "# Display the resulting clustered DataFrame\n",
    "display(clusters_spark_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a65303a8-770e-42af-b2a1-a9655c1bff0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate Clustering Performance\n",
    "\n",
    "Once the **K-Means clustering** is applied, we need to assess how well the clusters are formed. A common metric for this evaluation is the **Silhouette Score**.\n",
    "\n",
    "### Silhouette Score\n",
    "\n",
    "*The silhouette value measures how well an object fits its assigned cluster compared to other clusters*, ranging from -1 to +1, with higher values indicating better clustering. It provides a metric for evaluating clustering quality, with average scores above 0.5 considered reasonable, though high-dimensional data may yield lower scores due to the curse of dimensionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b9c9c96-96fa-433f-a358-e4849c7babd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_avg = silhouette_score(embeddings_scaled, labels)\n",
    "print(f\"Silhouette Score for K-Means with {optimal_k} clusters: {silhouette_avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14ced363-7305-4ae6-89a8-c368dd90e9ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualize Clustering Results\n",
    "\n",
    "We will visualize the clusters to gain insights into how the news articles are grouped based on their embeddings. Here we will be using the method **ConvexHull** to help visualize. This computes the convex hull in N dimensions (here N = 2). This helps us identify the boundary of a set of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83f8866f-f26a-4804-82f2-16d652b9e9b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "clusters_pd = clusters_spark_df.toPandas()\n",
    "\n",
    "# Define color palette\n",
    "num_clusters = clusters_pd[\"Cluster\"].nunique()\n",
    "colors = sns.color_palette(\"husl\", num_clusters)  # Distinct colors\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "# Scatter plot with better visibility\n",
    "for cluster, color in zip(range(num_clusters), colors):\n",
    "    subset = clusters_pd[clusters_pd[\"Cluster\"] == cluster]\n",
    "\n",
    "    plt.scatter(\n",
    "        subset[\"PC1\"], subset[\"PC2\"],\n",
    "        label=f\"Cluster {cluster}\",\n",
    "        color=color, s=80, alpha=0.6, edgecolors='k'  # Larger points, transparency, black edges\n",
    "    )\n",
    "\n",
    "    # Convex Hull for cluster boundary (only if there are enough points)\n",
    "    if len(subset) > 2:\n",
    "        hull = ConvexHull(subset[[\"PC1\", \"PC2\"]])\n",
    "        for simplex in hull.simplices:\n",
    "            plt.plot(\n",
    "                subset.iloc[simplex][\"PC1\"],\n",
    "                subset.iloc[simplex][\"PC2\"],\n",
    "                color=color, alpha=0.5\n",
    "            )\n",
    "\n",
    "# Labels and layout\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"Clustering Visualization of News Articles\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ec0ffa5-dc75-48a1-984c-ced83e8fd3e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we explored the process of **training an unsupervised model** using **K-Means clustering** on **text embeddings**. We generated embeddings with **Databricks OpenAI models**, standardized the data, and applied **dimensionality reduction (PCA)** to optimize clustering. By using the **Elbow Method** and **Silhouette Score**, we determined the optimal number of clusters and evaluated the quality of our model. This approach helps in discovering hidden patterns in text data, making it a powerful technique for **automated categorization and pattern recognition** in **real-world applications**.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06 - M2-Demo2: Unsupervised Learning2025-06-18 22_21_53",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
