{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a7394c9-094d-465a-8388-e496e4acd400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.1 Demo – Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a36ea3-6d7a-43f9-bab8-0bc2ad8bc8ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hyperparameter Tuning with Optuna\n",
    "\n",
    "In this hands-on demo, you will learn how to leverage **Optuna**, a powerful optimization library, for efficient model tuning. We’ll guide you through the process of performing **hyperparameter optimization**, demonstrating how to define the search space, objective function, and algorithm selection. Throughout the demo, you will utilize *MLflow* to seamlessly track the model tuning process, capturing essential information such as hyperparameters, metrics, and intermediate results. By the end of the session, you will not only grasp the principles of hyperparameter optimization but also be proficient in finding the best-tuned model using various methods such as the **MLflow API** and **MLflow UI**.\n",
    "\n",
    "By integrating Optuna and MLflow, you can efficiently optimize hyperparameters and maintain comprehensive records of your machine learning experiments, facilitating reproducibility and collaborative research.\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "*By the end of this demo, you will be able to:*\n",
    "\n",
    "- Perform hyperparameter optimization using Optuna.  \n",
    "- Track the model tuning process with MLflow.  \n",
    "- Query previous runs from an experiment using the `MLflowClient`.  \n",
    "- Review an MLflow Experiment for visualizing results and selecting the best run.  \n",
    "- Read in the best model, make a prediction, and register the model to Unity Catalog.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "- To run this notebook, you need to use one of the following Databricks runtime(s): **16.0.x-cpu-ml-scala2.12**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63b3745b-343b-4bc8-9e94-bfa3d5874db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.  \n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.  \n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.  \n",
    "3. Wait a few minutes for the cluster to start.  \n",
    "4. Once the cluster is running, complete the steps above to select your cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a68203f-aad8-4407-a0f8-bfa38fbd4003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qq optuna\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb6f5dca-98e2-41c7-810c-37d45d1f4bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e56eff87-21bb-4f03-b1f6-5511f5fafcee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Other Conventions:\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c09ffd-0abf-4bf1-a642-f3f166245f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:       {DA.username}\")\n",
    "print(f\"Catalog Name:   {DA.catalog_name}\")\n",
    "print(f\"Schema Name:    {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6699cb1-5e36-4c33-a9be-b33b78a2307f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Before we start fitting a model, we need to prepare dataset. First, we will load dataset, then we will split it to train and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f818a3fe-d18f-4249-b8cf-82667e1340db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Dataset\n",
    "\n",
    "In this demo we will be using the CDC Diabetes dataset from the Databricks Marketplace. This dataset has been read in and written to a feature table called `diabetes` in our working schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04e75d88-80f8-441e-a3bf-85f47bac51e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load data from the feature table\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.diabetes\"\n",
    "diabetes_dataset = spark.read.table(table_name)\n",
    "diabetes_pd = diabetes_dataset.drop('unique_id').toPandas()\n",
    "\n",
    "# review dataset and schema\n",
    "display(diabetes_pd)\n",
    "print(diabetes_pd.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96bc11fa-a714-416f-9f37-f6453568258c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train/Test Split\n",
    "Next, we will divide the dataset to training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b6f1ce-3436-450c-bc4e-1603866c2f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"We have {diabetes_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into its own dataset\n",
    "target_col = \"Diabetes_binary\"\n",
    "X_all = diabetes_pd.drop(labels=target_col, axis=1)\n",
    "y_all = diabetes_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.95, random_state=42)\n",
    "\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34460a85-cc3c-4e77-8e97-509bd2c423b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "### Define the Objective Function\n",
    "\n",
    "An objective function in Optuna is a Python function that defines the optimization target. It takes a single argument, typically named `trial`, which is an instance of the `optuna.Trial` class. This function is responsible for:\n",
    "\n",
    "1. Defining the hyperparameter search space  \n",
    "2. Training the model with the suggested hyperparameters  \n",
    "3. Evaluating the model's performance  \n",
    "4. Returning a scalar value that Optuna will try to optimize (minimize or maximize)\n",
    "\n",
    "In our case, we are working with scikit-learn's `DecisionTreeClassifier`. Start by defining the search space for the model. Our hyperparameters are:\n",
    "\n",
    "- `criterion`: chooses between `gini` and `entropy`. Defining the criterion parameter allows the algorithm to try both options during tuning and can assist in identifying which criterion works best. `TPE` is the default, though there are [other sampling methods](https://optuna.readthedocs.io/en/stable/reference/samplers.html) like `GPSampler` and `BruteForceSampler`.\n",
    "- `max_depth`: an integer between 5 and 50  \n",
    "- `min_samples_split`: an integer between 2 and 40  \n",
    "- `min_samples_leaf`: an integer between 1 and 20  \n",
    "\n",
    "The objective function will also have nested MLflow runs for logging. Each trial starts a new MLflow run using `with mlflow.start_run()`.  \n",
    "We will also manually log metrics and the scikit-learn model within the objective function.  \n",
    "Note that the training process is using cross-validation (5-fold CV in fact) and returns the negative mean of the fold results.\n",
    "\n",
    "- Impureza de Gini mide con qué frecuencia una muestra seleccionada al azar sería clasificada incorrectamente si se etiquetara aleatoriamente según la distribución actual de clases. Cuantifica la probabilidad de una clasificación errónea.\n",
    "\n",
    "- Entropía mide la cantidad de incertidumbre o desorden en el conjunto de datos. Cuantifica cuán \"impuro\" es un nodo en términos de distribución de clases, siendo mayor la entropía cuanto más desorden o incertidumbre hay en la clasificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947092aa-b2e4-427c-bf92-c7edea78fab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Define the objective function\n",
    "def optuna_objective_function(trial):\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 40),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    }\n",
    "\n",
    "    # Start an MLflow run for logging\n",
    "    with mlflow.start_run(nested=True, run_name=f\"Model Tuning with Optuna – Trial {trial.number}\"):\n",
    "        \n",
    "        # Log parameters with MLflow\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        dtc = DecisionTreeClassifier(**params)\n",
    "        scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        cv_results = cross_validate(dtc, X_train, y_train, cv=5,\n",
    "                                    scoring=scoring_metrics, return_estimator=True)\n",
    "\n",
    "        # Log cross-validation metrics to MLflow\n",
    "        for metric in scoring_metrics:\n",
    "            mlflow.log_metric(f\"cv_{metric}\", cv_results[f'test_{metric}'].mean())\n",
    "\n",
    "        # Train the model on the full training set\n",
    "        final_model = DecisionTreeClassifier(**params)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "        # Create input signature using the first row of X_train\n",
    "        input_example = X_train.iloc[0:1]\n",
    "        signature = infer_signature(input_example, final_model.predict(input_example))\n",
    "\n",
    "        # Registrar el modelo con la firma de entrada\n",
    "        mlflow.sklearn.log_model(final_model, \"decision_tree_model\", signature=signature, input_example=input_example)\n",
    "\n",
    "        # Calcular el promedio de la validación cruzada\n",
    "        f1_score_mean = -cv_results['test_f1'].mean()\n",
    "\n",
    "        # Métrica a minimizar\n",
    "        return -f1_score_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dd0d6ca-8a0b-4956-aebf-75fd0a3e0fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimize the Scikit-Learn Model on Single-Machine Optuna and Log Results with MLflow\n",
    "\n",
    "Before running the optimization, we need to perform two key steps:\n",
    "\n",
    "1. **Initialize an Optuna Study** using `optuna.create_study()`.\n",
    "   - A *study* represents an optimization process consisting of multiple trials.\n",
    "   - A *trial* is a single execution of the *objective function* with a specific set of hyperparameters.\n",
    "\n",
    "2. **Run the Optimization** using `study.optimize()`.\n",
    "   - This tells Optuna how many trials to perform and allows it to explore the search space.\n",
    "\n",
    "Each trial will be logged to MLflow, including the hyperparameters tested and their corresponding cross-validation results. Optuna will handle the optimization while training continues.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps:\n",
    "\n",
    "- **Set up an Optuna study** with `optuna.create_study()`.\n",
    "- **Start an MLflow run** with `mlflow.start_run()` to log experiments.\n",
    "- **Optimize hyperparameters** using `study.optimize()` within the MLflow context.\n",
    "\n",
    "---\n",
    "\n",
    "## Note on `n_jobs` in `study.optimize()`:\n",
    "\n",
    "The `n_jobs` argument controls the **number of trials running in parallel** using multi-threading **on a single machine**.\n",
    "\n",
    "- If `n_jobs=-1`, Optuna will use **all available CPU cores** (e.g., on a 4-core machine, it will likely use all 4 cores).\n",
    "- If `n_jobs` is **undefined (default)**, trials run **sequentially (single-threaded)**.\n",
    "- **Important**: `n_jobs` **does not** distribute trials across multiple nodes in a Spark cluster.  \n",
    "  To parallelize across nodes, use `SparkTrials()` instead.\n",
    "\n",
    "---\n",
    "\n",
    "## Why We Don't Use `MLflowCallback`:\n",
    "\n",
    "Optuna provides an `MLflowCallback` for automatic logging. However, in this demo, we are demonstrating how to integrate the MLflow API with Optuna separate from `MLflowCallback`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa813e57-ff2c-4348-94bf-64c85a659c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, we will delete all previous runs to keep our workspace and experiment tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92c4169e-481b-4f5c-a576-b596e99cbee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the MLflow experiment name and get the id\n",
    "experiment_name = f\"/Users/{DA.username}/Demo_Optuna_Experiment_{DA.schema_name}\"\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "print(\"Clearing out old runs (If you want to add more runs, change the n_trial parameter in the next cell) ...\")\n",
    "\n",
    "# Get all runs\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id], output_format=\"pandas\")\n",
    "\n",
    "if runs.empty:\n",
    "    print(\"No runs found in the experiment.\")\n",
    "else:\n",
    "    # Iterate and delete each run\n",
    "    for run_id in runs[\"run_id\"]:\n",
    "        mlflow.delete_run(run_id)\n",
    "        print(f\"Deleted run: {run_id}\")\n",
    "\n",
    "print(\"All runs have been deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bd5e3b3-06b4-4c66-94b9-97ae8b628e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"optuna_hpo\",\n",
    "    direction=\"minimize\"\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name='demo_optuna_hpo') as parent_run:\n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        optuna_objective_function,\n",
    "        n_trials=10\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97c74298-b12c-4e47-8d53-ab6785816c63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Review Tuning Results\n",
    "\n",
    "We can use the MLflow API to review the trial results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a97d889b-688b-43f4-9bef-dbe18e028df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Define your experiment name or ID\n",
    "experiment_id = parent_run.info.experiment_id  # Replace with your actual experiment ID\n",
    "\n",
    "# Fetch all runs from the experiment\n",
    "df_runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id]\n",
    ")\n",
    "\n",
    "# Filter out the parent run\n",
    "df_runs = df_runs[df_runs['tags.mlflow.runName'] != 'demo_optuna_hpo']\n",
    "\n",
    "# Display the results\n",
    "display(df_runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "811cfee7-20f3-49f2-9bdc-3dbd7cced08a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can use the Optuna study to get the best parameters and F1-score. Validate this agrees with the table results from the previous cell’s output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "214e9d57-46e5-4593-97b2-c14fd0eb6318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the best hyperparameters and metric\n",
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "print(f\"Best negative–F1 score: {study.best_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc885253-8536-4a96-9b66-9c887d4789bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Find the Best Run Based on F1-Score\n",
    "\n",
    "In this section, we will search for registered models. There are a couple ways for achieving this. We will show how to search runs using MLflow API and the UI.\n",
    "\n",
    "**The output links for using Optuna gave the best runs. Why can’t we just use that?**\n",
    "\n",
    "You totally can! But this is the same as using the UI to navigate to the trial that was the best (which is shown below).\n",
    "\n",
    "### Option 1: Find the Best Run – MLflow API\n",
    "\n",
    "Using the MLflow API, you can search runs in an experiment, which returns results into a Pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "214a9a46-1aaf-4d2c-a6ba-da852f51d9bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_id = parent_run.info.experiment_id\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6020dd82-1b2d-4953-8bde-167dd73e2199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "search_runs_pd = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    order_by=[\"metrics.cv_f1 DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "display(search_runs_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10003bde-c2b6-4f80-ac10-fa9cde98028d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Option 2 - Find the Best Run - MLflow UI\n",
    "\n",
    "The simplest way of seeing the tuning result is to use MLflow UI.\n",
    "\n",
    "1. Click on **Experiments** from left menu.  \n",
    "2. Select experiment which has the same name as this notebook's title (_2.1 - Hyperparameter Tuning with Optuna_).  \n",
    "3. Click on the graph icon at the top left under **Run**.  \n",
    "4. Click on the parent run or manually select all 10 runs to compare. The graphs on the right of the screen will appear for inspection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0c88d21-235d-4d8f-aeac-1817e8c8ab40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualize the Hyperparameters\n",
    "\n",
    "By now, we have determined which trial had the best run according to the f1-score.  \n",
    "Now, let's visually inspect our other search space elements with respect to this metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "febcf1bf-d261-44bc-96e8-701ab14375fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the necessary parameters exist in the DataFrame before plotting\n",
    "required_params = [\n",
    "    \"params.min_samples_leaf\",\n",
    "    \"params.max_depth\",\n",
    "    \"params.min_samples_split\",\n",
    "    \"metrics.cv_f1\",\n",
    "    \"tags.mlflow.runName\"\n",
    "]\n",
    "df_filtered = df_runs.dropna(subset=required_params, how=\"any\")\n",
    "\n",
    "# Convert parameters to appropriate types\n",
    "df_filtered[\"params.min_samples_split\"] = df_filtered[\"params.min_samples_split\"].astype(float)\n",
    "df_filtered[\"params.max_depth\"] = df_filtered[\"params.max_depth\"].astype(float)\n",
    "df_filtered[\"metrics.cv_f1\"] = df_filtered[\"metrics.cv_f1\"].astype(float)\n",
    "\n",
    "# Identify the best run index (assuming higher f1 is better)\n",
    "best_run_index = df_filtered[\"metrics.cv_f1\"].idxmax()\n",
    "best_run_name = df_filtered.loc[best_run_index, \"tags.mlflow.runName\"]\n",
    "\n",
    "# Extract run names for x-axis labels\n",
    "run_names = df_filtered[\"tags.mlflow.runName\"]\n",
    "\n",
    "# Create a figure and axis for bar chart\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Bar chart for min_samples_split and max_depth\n",
    "df_filtered[[\"params.min_samples_split\", \"params.max_depth\"]].plot(\n",
    "    kind=\"bar\", ax=ax1, edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "ax1.set_xlabel(\"Run Name\")\n",
    "ax1.set_ylabel(\"Parameter Values\")\n",
    "ax1.set_title(\"Hyperparameters & cv_f1 Score per Run\")\n",
    "ax1.legend([\"Max Features\", \"Max Depth\"])\n",
    "ax1.set_xticks(range(len(df_filtered)))\n",
    "ax1.set_xticklabels(run_names, rotation=45, ha=\"right\")  # Rotate for readability\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create a second y-axis for the cv_f1 score line chart\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    range(len(df_filtered)),  # X-axis indices\n",
    "    df_filtered[\"metrics.cv_f1\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"blue\",\n",
    "    label=\"cv_f1 Score\"\n",
    ")\n",
    "\n",
    "# Highlight the best run with a bold marker\n",
    "ax2.plot(\n",
    "    df_filtered.index.get_loc(best_run_index),  # Get positional index\n",
    "    df_filtered.loc[best_run_index, \"metrics.cv_f1\"],\n",
    "    marker=\"o\",\n",
    "    markersize=10,\n",
    "    color=\"red\",\n",
    "    label=\"Best Run\"\n",
    ")\n",
    "# Add a vertical dashed line to indicate the best run\n",
    "ax2.axvline(df_filtered.index.get_loc(best_run_index), color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax2.set_ylabel(\"cv_f1 Score\")\n",
    "\n",
    "# Add legend\n",
    "fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.9))\n",
    "plt.show()\n",
    "\n",
    "# Pie chart for criterion\n",
    "plt.figure(figsize=(8, 8))\n",
    "df_filtered[\"params.criterion\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\", startangle=90)\n",
    "plt.title(\"Criterion Distribution\")\n",
    "plt.ylabel(\"\")  # Hide y-label for better visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15697585-e7fa-454d-b0ad-7f16c6f07fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the Best Model and Parameters\n",
    "\n",
    "To load the model and make a prediction, let's use the information from Option 2 shown above. Run the next cell to get the value.\n",
    "\n",
    "### Copy and Paste Option\n",
    "\n",
    "Alternatively, you can set the variables shown below manually. Using either the output from Option 1 or the UI from Option 2, locate the `run_id` and the `experiment_id`. With Option 1 or 2, this is simply the value in the first two columns. In the UI, this is presented to you in the Details table when clicking on the specific run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f9f6ab6-b8a4-4e52-aeab-763ce5aa66d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert search_runs_pd to pyspark dataframe\n",
    "search_runs_sd = spark.createDataFrame(search_runs_pd)\n",
    "\n",
    "# Get the string value from run_id and experiment_id from PySpark DataFrame hpo_runs_df\n",
    "run_id = search_runs_sd.select(\"run_id\").collect()[0][0]\n",
    "experiment_id = search_runs_sd.select(\"experiment_id\").collect()[0][0]\n",
    "\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Experiment ID: {experiment_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61922be3-86fa-4724-bbf5-bbf40376407a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "from mlflow.models import Model\n",
    "\n",
    "# Grab an input example from the test set\n",
    "input_example = X_test.iloc[0]\n",
    "\n",
    "model_path = f\"dbfs:/databricks/mlflow-tracking/{experiment_id}/{run_id}/artifacts/decision_tree_model\"\n",
    "\n",
    "# Load the model using the run ID\n",
    "loaded_model = mlflow.pyfunc.load_model(model_path)\n",
    "\n",
    "# Retrieve model parameters\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "params = client.get_run(run_id).data.params\n",
    "\n",
    "# Display model parameters\n",
    "print(\"Best Model Parameters:\")\n",
    "print(json.dumps(params, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80754497-f5fd-428a-863f-95c478adc894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb71d7bf-9d4a-43b3-b34f-2d1835d91ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "test_prediction = loaded_model.predict(input_example)\n",
    "\n",
    "# X_test is a pandas dataframe – let's add the test_prediction output as a new column\n",
    "input_example['prediction'] = test_prediction\n",
    "\n",
    "display(input_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1837037c-b3aa-4fe9-a393-5425bf51ca31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the Model to Unity Catalog\n",
    "\n",
    "After running the following cell, navigate to our working catalog and schema (see course setup above) and validate the model has been registered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f67450d-3b05-49e4-8ada-ad5f8141b706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "model_uri = f\"runs:/{run_id}/decision_tree_model\"\n",
    "\n",
    "mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=f\"{DA.catalog_name}.{DA.schema_name}.demo_optuna_model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011d00f1-07ac-415e-87f9-bd5852330ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusión\n",
    "En esta demostración, exploramos cómo mejorar el rendimiento de tu modelo utilizando Optuna para la optimización de hiperparámetros y MLflow para hacer seguimiento del proceso de ajuste.\n",
    "Mediante el uso de los eficientes algoritmos de búsqueda de Optuna, aprendiste a ajustar eficazmente los parámetros del modelo.\n",
    "Simultáneamente, MLflow facilitó un monitoreo fluido y el registro de cada prueba, capturando información esencial como hiperparámetros, métricas y resultados intermedios.\n",
    "Además, aprendiste cómo registrar el mejor modelo en Unity Catalog.\n",
    "De cara al futuro, integrar estas herramientas en tu flujo de trabajo será clave para mejorar el rendimiento del modelo y simplificar el proceso de ajuste fino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f46de6c-e8e9-436e-b724-a5fa891cf40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08-M2-Demo4: Hyperparameter Tuning with Optuna 2025-06-23 12_06_58",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
