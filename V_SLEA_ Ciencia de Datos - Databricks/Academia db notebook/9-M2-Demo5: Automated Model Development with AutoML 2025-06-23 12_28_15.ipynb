{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "366bfed1-8a98-4401-a6e9-edd67a6066d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Automated Model Development with AutoML\n",
    "\n",
    "In this demo, we will demonstrate how to initiate AutoML experiments both through the user-friendly AutoML UI and programmatically using the AutoML API.  \n",
    "When using the API, we will demonstrate some custom functionalities such as feature table integration and custom split ratios for train, validation and test.\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "*By the end of this demo, you will be able to:*\n",
    "\n",
    "- Start an AutoML experiment via the AutoML UI.  \n",
    "- Start an AutoML experiment via the AutoML API.  \n",
    "- Open and edit a notebook generated by AutoML.  \n",
    "- Identify the best model generated by AutoML based on a given metric.  \n",
    "- Modify the best model generated by AutoML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82263676-adce-46f7-a7c9-b81443e5edf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "- To run this notebook, you need to use one of the following Databricks runtime(s): **16.0.x-cpu-ml-scala2.12**\n",
    "\n",
    "---\n",
    "\n",
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default. Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select **Open in new tab**.  \n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.  \n",
    "3. Wait a few minutes for the cluster to start.  \n",
    "4. Once the cluster is running, complete the steps above to select your cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d45ad713-f342-4634-b5da-29d5dfa17c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "027244c0-0bec-486c-9426-767f7c187422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-3.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429bb691-4168-4013-97f9-7ee10840cc56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Other Conventions:\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6305f273-44c7-4163-8f6b-eda678445e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:         {DA.username}\")\n",
    "print(f\"Catalog Name:     {DA.catalog_name}\")\n",
    "print(f\"Schema Name:      {DA.schema_name}\")\n",
    "print(f\"Working Directory:{DA.paths.working_dir}\")\n",
    "print(f\"User DB Location: {DA.paths.datasets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c971c45-97ca-4615-844f-93bf5b20f50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Data\n",
    "\n",
    "For this demonstration, we will utilize a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including gender, as well as internet subscription details such as subscription plans and payment methods.\n",
    "\n",
    "A table with all features is already created for you.\n",
    "\n",
    "**Table name:** `customer_churn`\n",
    "\n",
    "To get started, execute the code block below and review the dataset schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c2a77ae-51cc-4e95-992b-6430b95d98bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "churn_data = spark.sql(\"SELECT * FROM customer_churn\")\n",
    "display(churn_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "684566a3-d944-4ad4-a716-1171a4a7e5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualize Dataset\n",
    "\n",
    "Letâ€™s preview the `customer_churn` dataset using a SQL query with Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c05784de-dbc8-48ad-aae6-d714318529eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## AutoML Experiment with UI\n",
    "\n",
    "Databricks AutoML supports experimentation via the UI and the API.  \n",
    "Thus, **in the first section of this demo we will demonstrate how to create an experiment using the UI.**  \n",
    "Then, show how to create the same experiment via the API.\n",
    "\n",
    "### Create AutoML Experiment\n",
    "\n",
    "Let's initiate an AutoML experiment to construct a baseline model for predicting customer churn.  \n",
    "The target field for this prediction will be the `Churn` field.\n",
    "\n",
    "Follow these step-by-step instructions to create an AutoML experiment:\n",
    "\n",
    "1. Navigate to the **Experiments** section in Databricks.  \n",
    "2. Click on **Start training** under **Classification**.\n",
    "- imagen \n",
    "3. Choose a cluster to execute the experiment.  \n",
    "4. Select the **catalog > schema > `customers_churn`** table,  \n",
    "   which was created in the previous step, as the input training dataset.  \n",
    "5. Specify `Churn` as the prediction target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01440350-b5f1-4714-be6f-b3976655cd82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## AutoML Experiment with API\n",
    "\n",
    "In the previous section, we created an AutoML experiment using the user interface (UI) with basic functionalities. AutoML also supports advanced functionalities, such as **feature table integration** and **custom data split ratios**, which can enhance model performance and flexibility.\n",
    "\n",
    "In this section, we will utilize the AutoML API to create an experiment incorporating these advanced features. By leveraging the API, we gain greater control over the experiment's configuration, enabling the customization of feature inputs and the specification of data splitting strategies.\n",
    "\n",
    "### Set Features Table\n",
    "\n",
    "AutoML supports the use of feature tables as input. During setup, a feature table (`customer_churn_features`) is created. In this section, we will utilize this feature table during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2a51aa9-1076-4779-ae96-c6faa905f7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_table_path = f\"{DA.catalog_name}.{DA.schema_name}.customer_churn_features\"\n",
    "\n",
    "# View features tables\n",
    "display(spark.sql(f\"SELECT * FROM {features_table_path}\"))\n",
    "\n",
    "# Define the feature store lookups\n",
    "feature_store_lookups = [\n",
    "    {\n",
    "        \"table_name\": features_table_path,\n",
    "        \"lookup_key\": [\"CustomerID\"]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5905788d-d476-42d6-abac-bf632fd87f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set Custom Split - Random Split\n",
    "\n",
    "If you prefer AutoML to split the dataset with a different ratio than **the default 60:20:20**, you can create a new column in your dataset with the desired split assignments. This column **should contain the values** `\"train\"`, `\"validate\"`, or `\"test\"` to designate each row's role. When invoking the AutoML API, pass this column to the `split_col` parameter.\n",
    "\n",
    "This approach allows you to define custom data splits tailored to your specific requirements. Ensure that the `custom_split` column accurately reflects the intended distribution of your data into training, validation, and test sets.\n",
    "\n",
    "> **Example for understanding the code below**:  \n",
    "> Consider the three values 0.5, 0.8, and 0.91 that are each mapped to three different rows.  \n",
    "> We will consider the row containing 0.5 as a train data point, while 0.8 is considered a validation data point and 0.91 as a test data point.  \n",
    "> Basically, values in the interval [0, 0.79] belong to the training dataset,  \n",
    "> values between [0.8, 0.89] belong to the validation set,  \n",
    "> and values between [0.9, 1.0] belong to the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a0fdd1f-0e5c-4f71-aeb6-15b95f48e107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, rand\n",
    "\n",
    "dataset = spark.read.table(\"customer_churn\")\n",
    "\n",
    "seed = 42  # define your seed here for reproduction\n",
    "train_ratio, validate_ratio, test_ratio = 0.8, 0.1, 0.1  # define your preferred ratios here\n",
    "\n",
    "dataset = dataset.withColumn(\"random\", rand(seed=seed))\n",
    "dataset = dataset.withColumn(\n",
    "    \"custom_split\",\n",
    "    when(dataset.random < train_ratio, \"train\")\n",
    "    .when(dataset.random < 1 - test_ratio, \"validate\")\n",
    "    .otherwise(\"test\")\n",
    ")\n",
    "\n",
    "dataset = dataset.drop(\"random\")\n",
    "display(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c026fca-a088-433e-8c39-aa25eba149eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Further Reading: Stratified Sampling with AutoML\n",
    "\n",
    "Stratified sampling ensures that the distribution of a categorical variable (e.g., target labels) is preserved across the training, validation, and test sets. This is particularly useful when dealing with imbalanced datasets.\n",
    "\n",
    "1. **Identify the Stratification Column** â€“ Choose a categorical variable to maintain proportions across dataset splits.\n",
    "2. **Compute Class Proportions** â€“ Determine the distribution of each category in the dataset.\n",
    "3. **Calculate Sample Sizes** â€“ Apply the desired split ratios to compute the exact number of records per class for each split.\n",
    "4. **Perform Stratified Sampling** â€“ Split each category proportionally into training, validation, and test sets.\n",
    "5. **Assign Labels and Combine Splits** â€“ Label the subsets accordingly and merge them into the final dataset.\n",
    "6. **Validate Class Distribution** â€“ Ensure each split maintains the original class proportions.\n",
    "\n",
    "---\n",
    "\n",
    "### Sample Code\n",
    "(see below)\n",
    "```python\n",
    "# Load dataset\n",
    "dataset = spark.read.table(\"customer_churn\")\n",
    "\n",
    "# Define stratification column\n",
    "stratify_col = \"Gender\"\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio, validate_ratio, test_ratio = 0.8, 0.1, 0.1\n",
    "seed = 42\n",
    "\n",
    "# Step 1: Compute class counts and original distribution\n",
    "class_counts = dataset.groupBy(stratify_col).agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "original_distribution = (\n",
    "    class_counts.withColumn(\"percentage\", round((col(\"count\") / dataset.count()) * 100, 2))\n",
    "                 .withColumn(\"dataset\", lit(\"original\"))\n",
    ")\n",
    "\n",
    "# Step 2: Perform stratified sampling\n",
    "train_df = dataset.sampleBy(stratify_col, {row[stratify_col]: train_ratio for row in class_counts.collect()}, seed)\n",
    "validate_df = dataset.subtract(train_df).sampleBy(\n",
    "    stratify_col,\n",
    "    {row[stratify_col]: validate_ratio / (validate_ratio + test_ratio) for row in class_counts.collect()},\n",
    "    seed\n",
    ")\n",
    "test_df = dataset.subtract(train_df).subtract(validate_df)\n",
    "\n",
    "# Assign split labels\n",
    "train_df = train_df.withColumn(\"custom_split\", lit(\"train\"))\n",
    "validate_df = validate_df.withColumn(\"custom_split\", lit(\"validate\"))\n",
    "test_df = test_df.withColumn(\"custom_split\", lit(\"test\"))\n",
    "\n",
    "# Combine datasets efficiently\n",
    "final_dataset = train_df.unionByName(validate_df).unionByName(test_df)\n",
    "```python\n",
    "# Step 4: Validate stratification with correct percentage calculation\n",
    "def validate_distribution(df, split_name):\n",
    "    total_split_count = df.count()\n",
    "    return (\n",
    "        df.groupBy(stratify_col)\n",
    "          .agg(count(\"*\").alias(\"count\"))\n",
    "          .withColumn(\"dataset\", lit(split_name))\n",
    "          .withColumn(\"percentage\", round((col(\"count\") / total_split_count) * 100, 2))\n",
    "    )\n",
    "\n",
    "# Compute distributions\n",
    "train_dist = validate_distribution(train_df, \"train\")\n",
    "validate_dist = validate_distribution(validate_df, \"validate\")\n",
    "test_dist = validate_distribution(test_df, \"test\")\n",
    "\n",
    "# Ensure Schema Consistency Before Union\n",
    "columns_order = [\"Gender\", \"count\", \"percentage\", \"dataset\"]\n",
    "\n",
    "original_distribution = original_distribution.select(*columns_order)\n",
    "train_dist = train_dist.select(*columns_order)\n",
    "validate_dist = validate_dist.select(*columns_order)\n",
    "test_dist = test_dist.select(*columns_order)\n",
    "\n",
    "# Combine all distributions (original + splits)\n",
    "distribution_comparison = original_distribution.unionByName(train_dist) \\\n",
    "                                               .unionByName(validate_dist) \\\n",
    "                                               .unionByName(test_dist)\n",
    "\n",
    "# Display the final distribution comparison\n",
    "display(distribution_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efaaf593-b922-4102-8085-4d2387fd5e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start an Experiment\n",
    "\n",
    "Now that we have **feature lookups** and **custom splits column** ready, we can continue to setup an AutoML experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc60ece1-6e8b-4308-bbcc-6e4c1ae8b556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import automl\n",
    "from datetime import datetime\n",
    "\n",
    "automl_run = automl.classify(\n",
    "    dataset = dataset,\n",
    "    target_col = \"Churn\",\n",
    "    split_col = \"custom_split\",\n",
    "    exclude_cols = [\"CustomerID\"],  # Exclude columns as needed\n",
    "    timeout_minutes = 5,\n",
    "    feature_store_lookups = feature_store_lookups\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed4cb56c-76bd-4331-a770-7083d53fb828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Search for the Best Run\n",
    "\n",
    "The search for the best run in this experiment, we need to first **get the experiment ID** and then **search for the runs** by experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c859b24e-1d9b-481f-8c3b-9502d4c63246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Get the experiment path by experiment ID\n",
    "exp_path = mlflow.get_experiment(automl_run.experiment.experiment_id).name\n",
    "\n",
    "# Find the most recent experiment in the AutoML folder\n",
    "filter_string = f\"name LIKE '{exp_path}'\"\n",
    "automl_experiment_id = mlflow.search_experiments(\n",
    "    filter_string=filter_string,\n",
    "    max_results=1,\n",
    "    order_by=[\"last_update_time DESC\"]\n",
    ")[0].experiment_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a10d8324-3943-4590-be7e-44adc5ed17b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "# Find the best run ...\n",
    "automl_runs_pd = mlflow.search_runs(\n",
    "    experiment_ids=[automl_experiment_id],\n",
    "    filter_string=\"attributes.status = 'FINISHED'\",\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    order_by=[\"metrics.val_f1_score DESC\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c7c9da3-e906-451c-a2da-df21e1c667d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Print information about the best trial from the AutoML experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1f43808-258f-4958-bb67-25e4ef2831b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(automl_run.best_trial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7965fe2d-eff4-4ead-b86b-e7e3c803cdc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import notebooks for other runs in AutoML\n",
    "\n",
    "For classification and regression experiments, AutoML generated notebooks for data exploration and the best trial in your experiment are automatically imported to your workspace. Generated notebooks for other experiment trials are saved as MLflow artifacts on DBFS instead of auto-imported into your workspace.\n",
    "\n",
    "For all trials besides the best trial, the `notebook_path` and `notebook_url` in the TrialInfo Python API are not set. If you need to use these notebooks, you can manually import them into your workspace with the AutoML experiment UI or the `automl.import_notebook` Python API.\n",
    "\n",
    "ðŸ“› **Notice**: `destination_path` takes Workspace as root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa4f467-980f-4953-8306-571acaa44c8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the Destination path for storing the best run notebook\n",
    "destination_path = f\"/Users/{DA.username}/imported_notebooks/demo-3.1-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "# Get the path and URL for the generated notebook\n",
    "result = automl.import_notebook(automl_run.trials[1].artifact_uri, destination_path)\n",
    "print(f\"The notebook is imported to: {result.path}\")\n",
    "print(f\"The notebook URL           : {result.url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "998bd147-c29b-4e27-b71d-9f545123f468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we show how to use AutoML UI and AutoML API for creating classification model and how we can retrieve the best run and access the generated notebook, and how we can modify the parameters of the best model.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "9-M2-Demo5: Automated Model Development with AutoML 2025-06-23 12_28_15",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
