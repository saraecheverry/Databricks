{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aee2aca-8b0e-43e8-bb8e-42a2a6b84cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Preparation and Feature Engineering\n",
    "\n",
    "In this demo, we'll delve into techniques such as preparing modeling data, including splitting data, handling missing values, encoding categorical features, and standardizing features. We will also discuss outlier removal and coercing columns to the correct data type.\n",
    "\n",
    "By the end, you will have a comprehensive understanding of data preparation for modeling and feature preparation.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "By the end of this demo, you will be able to:\n",
    "\n",
    "- Coerce columns to be the correct data type based on feature or target variable type.\n",
    "- Identify and remove outliers from the modeling data.\n",
    "- Drop rows/columns that contain missing values.\n",
    "- Impute categorical missing values with the mode value.\n",
    "- Replace missing values with a specified replacement value.\n",
    "- One-hot encode categorical features.\n",
    "- Apply pre-existing embeddings to categorical features.\n",
    "- Standardize features in a training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4de9af94-5475-4107-9cd2-3cee8b695dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a14a3b39-455d-4b39-875e-4df2a1e79219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Classroom Setup\n",
    "%run ../Includes/Classroom-Setup-01\n",
    "\n",
    "# Note: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\n",
    "# Note: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d17450c8-1931-4e79-a8fa-a1f6748a2b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Other Conventions:\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a988ae4-670b-4ecc-98ac-5fe30b8cf2f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username: {DA.username}\")\n",
    "print(f\"Catalog Name: {DA.catalog_name}\")\n",
    "print(f\"Schema Name: {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location: {DA.paths.datasets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7078c62-b1a2-4c0c-9c5d-0dc4e62f3249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Cleaning and Imputation\n",
    "\n",
    "- Load the dataset from the specified path using Spark and read it as a DataFrame.\n",
    "- Drop any rows with missing values from the DataFrame using the `dropna()` method.\n",
    "- Fill any remaining missing values in the DataFrame with the 0 using the `fillna()` method.\n",
    "- Create a temporary view named as `telco_customer_churn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ff2a98-9e02-431a-9a43-67c4667f6e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = f\"{DA.paths.datasets}/telco/telco-customer-churn-noisy.csv\"\n",
    "telco_df = spark.read.csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "# telco_df.printSchema()\n",
    "display(telco_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "553831e6-d3fc-4619-be3b-eb9fa0c8004d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Coerce/Fix Data Types\n",
    "\n",
    "Even though most of the data types are correct, let's do the following to have a better memory footprint of the dataframe in memory:\n",
    "\n",
    "- Convert `SeniorCitizen` and `Churn` binary columns to boolean type.\n",
    "- Converting the `tenure` column to a long integer using `.selectExpr` and reordering the columns.\n",
    "- Using `spark.sql` to convert `Partner`, `Dependents`, `PhoneService` and `PaperlessBilling` columns to boolean, and reordering the columns. Then, saving the dataframe as a DELTA table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff04a71d-ed9a-4a0e-a71c-27ee48123cdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, ShortType, IntegerType\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "binary_columns = [\"SeniorCitizen\", \"Churn\"]\n",
    "telco_customer_churn_df = telco_df\n",
    "\n",
    "for column in binary_columns:\n",
    "    telco_customer_churn_df = telco_customer_churn_df.withColumn(column, col(column).cast(BooleanType()))\n",
    "\n",
    "telco_customer_churn_df.select(*binary_columns).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e65b412-dae9-416f-82ae-e84c1c576b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Casting didn't work on `SeniorCitizen` most probably because there were some null values or values which couldn't be encoded correctly. We can force coercion using a simple filter method (assuming missing values in this column can be encoded as `False`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b0f978d-d59e-497d-b100-72015175e024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_churn_df = telco_customer_churn_df.withColumn(\n",
    "    \"SeniorCitizen\", when(col(\"SeniorCitizen\") == 1, True).otherwise(False)\n",
    ")\n",
    "\n",
    "telco_customer_churn_df.select(\"SeniorCitizen\").printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a9a331-22c4-4b80-ad20-cdd75c247596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbbf4a4c-d3fd-41cd-b3f6-74ccca0f58f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PhoneService & PaperlessBilling to new boolean using spark.sql and re-order columns\n",
    "telco_customer_churn_df.createOrReplaceTempView(\"telco_customer_churn_temp_view\")\n",
    "\n",
    "telco_customer_casted_df = spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    customerID,\n",
    "    BOOLEAN(Dependents),\n",
    "    BOOLEAN(Partner),\n",
    "    BOOLEAN(PhoneService),\n",
    "    BOOLEAN(PaperlessBilling),\n",
    "    *\n",
    "  EXCEPT (customerID, Dependents, Partner, PhoneService, PaperlessBilling, Churn),\n",
    "    Churn\n",
    "  FROM telco_customer_churn_temp_view\n",
    "\"\"\")\n",
    "\n",
    "telco_customer_casted_df.select(\"Dependents\", \"Partner\", \"PaperlessBilling\", \"PhoneService\").printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee48d78-cc2c-4113-9508-e765d3cf8e09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tenure months to Long/Integer using .selectExpr\n",
    "telco_customer_casted_df = telco_customer_churn_df.selectExpr(\"* except(tenure)\", \"cast(tenure as long) tenure\")\n",
    "telco_customer_casted_df.select(\"tenure\").printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db44bf41-fe0c-4979-ae5e-30b99771c205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_name_full = \"telco_customer_full\"\n",
    "\n",
    "# [OPTIONAL] Save as DELTA table (silver)\n",
    "telco_customer_full_silver = f\"{telco_customer_name_full}_silver\"\n",
    "telco_customer_casted_df.write.mode(\"overwrite\").option(\"mergeSchema\", True).saveAsTable(telco_customer_full_silver)\n",
    "\n",
    "# print(telco_customer_casted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af6ce67-195f-4268-b40e-17392607df53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling Outliers\n",
    "\n",
    "We will see how to handle outliers in column by identifying and addressing data points that fall far outside the typical range of values in a dataset. Common methods for handling outliers include removing them, filtering, transforming the data, or replacing outliers with more representative values.\n",
    "\n",
    "Follow these steps for handling outliers:\n",
    "\n",
    "- Create a new silver table named as `telco_customer_full_silver` by appending `silver` to the original table name and then accessing it using Spark SQL.\n",
    "- Filtering out outliers from the `TotalCharges` column by removing rows where the column value exceeds the specified cutoff value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76a88df7-b3b0-4eae-863d-231ced822416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Filtering out outliers from the `TotalCharges` column by removing rows where the column value exceeds the specified cutoff value (e.g. negative values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f33b491f-a2ea-4e7e-a61f-8928372c9bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_casted_df.select(\"TotalCharges\", \"tenure\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53cdfdc-7386-47f3-b9ae-e2f8651bde9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Remove customers with negative TotalCharges\n",
    "TotalCharges_cutoff = 0\n",
    "\n",
    "# Use .filter method and SQL col() function\n",
    "telco_no_outliers_df = telco_customer_casted_df.filter(\n",
    "    (col(\"TotalCharges\") > TotalCharges_cutoff) | (col(\"TotalCharges\").isNull())  # Keep Nulls\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a98ae43f-82fd-491e-acf2-8765b5058be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Removing outliers from PaymentMethod\n",
    "\n",
    "- Identify the two lowest occurrence groups in the `PaymentMethod` column and calculating the total count and average `MonthlyCharges` for each group.\n",
    "- Removing customers from the identified low occurrence groups in the `PaymentMethod` column to filter out outliers.\n",
    "- Create a new dataframe `telco_filtered_df` containing the filtered data.\n",
    "- Comparing the count of records before and after by dividing the count of `telco_casted_full_df` and `telco_no_outliers_df` dataframe removing outliers and then materializing the resulting dataframe as a new table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b65d1aa-4514-46b1-b813-6af11bbf62f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, avg\n",
    "\n",
    "# Identify 2 lowest group occurrences\n",
    "group_var = \"PaymentMethod\"\n",
    "stats_df = telco_no_outliers_df.groupBy(group_var) \\\n",
    "    .agg(count(\"*\").alias(\"Total\"), \\\n",
    "         avg(\"MonthlyCharges\").alias(\"MonthlyCharges\")) \\\n",
    "    .orderBy(col(\"Total\").desc())\n",
    "\n",
    "# Display\n",
    "display(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee250bde-a8e7-466b-ba58-71e21e379651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gather 2 lowest groups name assuming count threshold is below 20% of full dataset and monthly charges <20$\n",
    "N = telco_no_outliers_df.count()  # total count\n",
    "lower_groups = [elem[group_var] for elem in stats_df.head(2) if elem[\"Total\"]/N < 0.2 and elem[\"MonthlyCharges\"] < 20]\n",
    "\n",
    "print(f\"Removing groups: {', '.join(lower_groups)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74da9d9a-35cb-4af8-a434-191eac19751b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter/Remove listings from these low occurrence groups while keeping null occurrences\n",
    "telco_no_outliers_df = telco_no_outliers_df.filter(\n",
    "    ~col(group_var).isin(lower_groups) | col(group_var).isNull()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a64baf-081a-408b-bacf-624a3aff9a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count/Compare datasets before/after removing outliers\n",
    "print(f\"Count â€“ Before: {telco_customer_casted_df.count()} / After: {telco_no_outliers_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f93c66e6-6caf-42c5-8bf5-d2a173b09690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Materialize/Snap table [OPTIONAL/for instructor only]\n",
    "telco_no_outliers_df.write.mode(\"overwrite\").saveAsTable(telco_customer_full_silver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7795d59e-0cbc-4273-89ca-528c162fc991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "To handle missing values in dataset we need to identify columns with high percentages of missing data and drop those columns. Then, it removes rows with missing values. Numeric columns are imputed with 0, and string columns are imputed with 'N/A'. Overall, the code demonstrates a comprehensive approach to handling missing values in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Delete Columns\n",
    "\n",
    "- Create a DataFrame called `missing_df` to count the missing values per column in the `telco_no_outliers_df` dataset.\n",
    "\n",
    "- The `missing_df` DataFrame is then transposed for better readability using the TransposeDF function, which allows for easier analysis of missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b024ba-a5e2-43bb-8192-e8ae2dfdc156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count, concat_ws, collect_list\n",
    "\n",
    "def calculate_missing(input_df, show=True):\n",
    "    \"\"\"\n",
    "    Helper function to calculate and display missing data\n",
    "    \"\"\"\n",
    "\n",
    "    # First get count of missing values per column to get a singleton row DF\n",
    "    missing_df_ = input_df.select([\n",
    "        count(\n",
    "            when(\n",
    "                col(c).contains('None') |\n",
    "                col(c).contains('NULL') |\n",
    "                (col(c) == '') |\n",
    "                col(c).isNull(), c\n",
    "            )\n",
    "        ).alias(c) for c in input_df.columns\n",
    "    ])\n",
    "\n",
    "    # Transpose for better readability\n",
    "    def TransposeDF(df, columns, pivotCol):\n",
    "        \"\"\"Helper function to transpose spark dataframe\"\"\"\n",
    "        columnsValue = list(map(lambda x: str(\"\") + str(x) + str(\",\"), columns))\n",
    "        stackCols = ','.join([x for x in columnsValue])\n",
    "        df_1 = df.selectExpr(pivotCol, \"stack(\" + str(len(columns)) + \",\" + stackCols + \")\") \\\n",
    "            .select(pivotCol, \"col0\", \"col1\") \\\n",
    "            .withColumnRenamed(\"col0\", pivotCol)\n",
    "        final_df = df_1.groupBy(pivotCol).pivot(pivotCol).agg(concat_ws(\"\", collect_list(col(\"col1\"))))\n",
    "        return final_df\n",
    "\n",
    "    missing_df_out_T = TransposeDF(\n",
    "        spark.createDataFrame([{\"Column\": \"Number of Missing Values\"}]).join(missing_df_),\n",
    "        missing_df_.columns,\n",
    "        \"Column\"\n",
    "    ).withColumn(\"Number of Missing Values\", col(\"Number of Missing Values\").cast(\"long\"))\n",
    "\n",
    "    if show:\n",
    "        display(missing_df_out_T.orderBy(\"Number of Missing Values\", ascending=False))\n",
    "\n",
    "    return missing_df_out_T\n",
    "\n",
    "# Ejecutar la funciÃ³n para calcular y mostrar missing_df\n",
    "missing_df = calculate_missing(telco_no_outliers_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a011fa8-8d8b-42f0-a5e2-93053391f2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop columns with more than x% of missing rows\n",
    "\n",
    "Columns with more than 60% missing data are identified and stored in the `to_drop_missing` list, and these columns are subsequently dropped from the `telco_no_outliers_df` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c73cd6-bd5f-4e29-95db-08ec95deb4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop if column has more than 60% missing data\n",
    "per_thresh = 0.6\n",
    "N = telco_no_outliers_df.count()  # total count\n",
    "\n",
    "to_drop_missing = [\n",
    "    x.asDict()['Column'] for x in missing_df.select(\"Column\")\n",
    "    .where(col(\"Number of Missing Values\") / N >= per_thresh)\n",
    "    .collect()\n",
    "]\n",
    "\n",
    "print(f\"Dropping columns {to_drop_missing} for more than {per_thresh * 100}% missing data\")\n",
    "telco_no_missing_df = telco_no_outliers_df.drop(*to_drop_missing)\n",
    "\n",
    "# display(telco_no_missing_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72b167a-54ca-4b4c-bb0e-780447670f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop rows containing specific numbers of missing columns/fields\n",
    "\n",
    "Rows with more than 1/4 of the columns missing values are dropped using the `na.drop()` and the remaining missing values in numeric columns are imputed with 0, while missing values in string columns are imputed with 'N/A'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fd2226a-af0b-4de3-921b-dc58b7ffbc9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_cols = len(telco_no_missing_df.columns)\n",
    "telco_no_missing_df = telco_no_missing_df.na.drop(how='any', thresh=round(n_cols / 4))  # Drop rows where at least half values are missing, how='all' can also be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0672ac7a-6898-4558-b4b3-156aad2d02c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count/Compare datasets before/after removing missing\n",
    "print(f\"Count â€“ Before: {telco_no_outliers_df.count()} / After: {telco_no_missing_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5f22388-ff0c-47ff-b3f4-62185b81a572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Impute Missing Data\n",
    "\n",
    "Replace missing values with a specified replacement value.\n",
    "\n",
    "- The `num_cols` and `string_cols` lists are created to identify numeric and string columns in the dataset, respectively.\n",
    "\n",
    "- Finally, missing values in the numeric and string columns are imputed with appropriate values using the `na.fill()`, resulting in the `telco_imputed_df` dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0180b4a8-b123-40f0-897e-7b99142181f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replace numeric missing with constant/0\n",
    "\n",
    "NOTE: not applicable in this dataset's case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7483166-d683-4aa6-a370-314c9619f1b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Get a list of numeric columns\n",
    "num_cols = [c.name for c in telco_no_missing_df.schema.fields if (c.dataType == DoubleType() or c.dataType == IntegerType())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "702879db-74d1-4cd0-9478-b6dd45d79a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Impute\n",
    "# telco_imputed_df = telco_no_missing_df.na.fill(value=0, subset=num_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dc724e5-ce25-42bd-b78f-7bb20ed1d624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replace boolean missing with `False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99f57807-83ec-4a68-918b-38f0d4173d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Get a list of boolean columns\n",
    "bool_cols = [c.name for c in telco_no_missing_df.schema.fields if c.dataType == BooleanType()]\n",
    "\n",
    "# Impute\n",
    "telco_imputed_df = telco_no_missing_df.na.fill(value=False, subset=bool_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ce87663-4429-4c4a-9f83-abee85848b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Replace string missing with `No`\n",
    "\n",
    "All string cols except `gender`, `Contract` and `PaymentMethod`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8bd6ee1-7620-4aed-ada6-dd5128493a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Get list of string cols\n",
    "to_exclude = [\"customerID\", \"gender\", \"Contract\", \"PaymentMethod\"]\n",
    "string_cols = [c.name for c in telco_no_missing_df.drop(*to_exclude).schema.fields if c.dataType == StringType()]\n",
    "\n",
    "# Impute\n",
    "telco_imputed_df = telco_imputed_df.na.fill(value='No', subset=string_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48db32fb-7a6d-4b5e-8108-b4a7faa8e167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare missing stats again\n",
    "calculate_missing(telco_imputed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "007ec03c-634a-41c3-b4e2-6dc51b80d304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Encoding Categorical Features\n",
    "\n",
    "In this section, we will one-hot encode categorical/string features using Spark MLlib's `OneHotEncoder` estimator.\n",
    "\n",
    "If you are unfamiliar with one-hot encoding, there's a description below. If you're already familiar, you can skip ahead to the **One-hot encoding in Spark MLlib** section toward the bottom of the cell.\n",
    "\n",
    "---\n",
    "\n",
    "#### Categorical features in machine learning\n",
    "\n",
    "Many machine learning algorithms are not able to accept categorical features as inputs. As a result, data scientists and machine learning engineers need to determine how to handle them.\n",
    "\n",
    "An easy solution would be to remove the categorical features from the feature set. While this is quick, **you are removing potentially predictive information** â€” so this usually isnâ€™t the best strategy.\n",
    "\n",
    "Other options include ways to represent categorical features as numeric features. A few common options are:\n",
    "1. **One-hot encoding**: create dummy/binary variables for each category.\n",
    "2. **Target/label encoding**: replace each category value with a value that represents the target variable (e.g. replace a specific category value with the mean of the target variable for rows with that category value).\n",
    "3. **Embeddings**: use/create a vector-representation of meaningful words in each category's value.\n",
    "\n",
    "Each of these options can be really useful in different scenarios. We're going to focus on one-hot encoding here.\n",
    "\n",
    "#### One-hot encoding basics\n",
    "\n",
    "One-hot encoding creates a binary/dummy feature for each category in each categorical feature.\n",
    "\n",
    "In the example below, the feature `Animal` is split into three binary features â€” one for each value in `Animal`. Each binary feature's value is equal to 1 if its respective category value is present in `Animal` for each row. If its category value is not present in the row, the binary feature's value will be 0.\n",
    "\n",
    "\n",
    "#### One-hot encoding in Spark MLlib\n",
    "\n",
    "Even if you understand one-hot encoding, it's important to learn how to perform it using Spark MLlib.\n",
    "\n",
    "To one-hot encode categorical features in Spark MLlib, we are going to use two classes: the `StringIndexer` class and the `OneHotEncoder` class.\n",
    "\n",
    "- The `StringIndexer` class indexes string-type columns to a numerical index. Each unique value in the string-type column is mapped to a unique integer.\n",
    "- The `OneHotEncoder` class accepts indexed columns and converts them to a one-hot encoded vector-type feature.\n",
    "\n",
    "---\n",
    "\n",
    "#### Applying the `StringIndexer -> OneHotEncoder -> VectorAssembler` workflow\n",
    "\n",
    "First, we'll need to index the categorical features of the DataFrame. `StringIndexer` takes a few arguments:\n",
    "1. A list of categorical columns to index.\n",
    "2. A list names for the indexed columns being created.\n",
    "3. Directions for how to handle new categories when transforming data.\n",
    "\n",
    "Because `StringIndexer` has to learn which categories are present before indexing, it's an **estimator** â€” remember that means we need to call its `fit` method. Its result can then be used to transform our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fecf69-f78e-454d-b4eb-c9cded695970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Crear sesiÃ³n de Spark (si no estÃ¡ activa)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Datos simulados\n",
    "data = [\n",
    "    (\"Month-to-month\",),\n",
    "    (\"One year\",),\n",
    "    (\"Two year\",),\n",
    "    (\"Month-to-month\",),\n",
    "    (\"Two year\",),\n",
    "]\n",
    "\n",
    "# Definir esquema\n",
    "schema = StructType([StructField(\"Contract\", StringType(), True)])\n",
    "\n",
    "# Crear DataFrame\n",
    "telco_imputed_df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53833c80-e637-4386-9e59-a1d438dfe5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_df = telco_imputed_df.select(\"Contract\").distinct()\n",
    "sample_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e897f453-e55b-4dbc-a358-20a2b77cf9d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# StringIndexer\n",
    "string_cols = [\"Contract\"]\n",
    "index_cols = [column + \"_index\" for column in string_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=string_cols, outputCols=index_cols, handleInvalid=\"skip\")\n",
    "string_indexer_model = string_indexer.fit(sample_df)\n",
    "indexed_df = string_indexer_model.transform(sample_df)\n",
    "\n",
    "indexed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d63d548-fd93-4724-a398-dff664d55f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # StringIndexer\n",
    "# string_cols = [\"Contract\"]\n",
    "# index_cols = [column + \"_index\" for column in string_cols]\n",
    "\n",
    "# string_indexer = StringIndexer(inputCols=string_cols, outputCols=index_cols, handleInvalid=\"skip\")\n",
    "# string_indexer_model = string_indexer.fit(sample_df)\n",
    "# indexed_df = string_indexer_model.transform(sample_df)\n",
    "\n",
    "# indexed_df.show()\n",
    "\n",
    "# arriba falla este no\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "# Crear Ã­ndice categÃ³rico manual\n",
    "window_spec = Window.orderBy(\"Contract\")\n",
    "indexed_df = sample_df.withColumn(\"Contract_index\", dense_rank().over(window_spec) - 1)\n",
    "\n",
    "indexed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b874ac9d-e72b-40a2-b7aa-f2e8b41f9264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once our data has been indexed, we are ready to use the `OneHotEncoder` estimator.\n",
    "\n",
    "ðŸ’¡ **Hint:** Look at the `OneHotEncoder` [documentation](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder) and our previous Spark MLlib workflows that use estimators for guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c729d517-61db-46c8-bf86-6cb640ed8e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Create a list of one-hot encoded feature names\n",
    "ohe_cols = [column + \"_ohe\" for column in string_cols]\n",
    "\n",
    "# Instantiate the OneHotEncoder with the column lists\n",
    "ohe = OneHotEncoder(inputCols=index_cols, outputCols=ohe_cols, handleInvalid=\"keep\")\n",
    "\n",
    "# Fit the OneHotEncoder on the indexed data\n",
    "ohe_model = ohe.fit(indexed_df)\n",
    "\n",
    "# Transform indexed_df using the ohe_model\n",
    "ohe_df = ohe_model.transform(indexed_df)\n",
    "ohe_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a25a96-bbf7-4621-8cb9-7ae1b3b8339e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "selected_ohe_cols = [\"Contract_ohe\"]\n",
    "\n",
    "# Use VectorAssembler to assemble the selected one-hot encoded columns into a dense vector\n",
    "assembler = VectorAssembler(inputCols=selected_ohe_cols, outputCol=\"features\")\n",
    "result_df_dense = assembler.transform(ohe_df)\n",
    "\n",
    "# Select relevant columns for display\n",
    "result_df_display = result_df_dense.select(\"Contract\", \"features\")\n",
    "result_df_display.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c134f1d1-d9d2-4703-9cb4-7b8e5850c950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apply pre-existing embeddings to categorical/discrete features\n",
    "\n",
    "Let's bin `tenure` to convert the discrete data into bins/categories format for further analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b0096f85-d740-46f4-bf17-450e9f59f2b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Crear sesiÃ³n de Spark (si no estÃ¡ activa)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Datos simulados\n",
    "data = [\n",
    "    (1, \"Month-to-month\"),\n",
    "    (5, \"Month-to-month\"),\n",
    "    (12, \"One year\"),\n",
    "    (24, \"Two year\"),\n",
    "    (36, \"Two year\"),\n",
    "    (60, \"Two year\")\n",
    "]\n",
    "\n",
    "# Definir esquema\n",
    "schema = StructType([\n",
    "    StructField(\"tenure\", IntegerType(), True),\n",
    "    StructField(\"Contract\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Crear DataFrame\n",
    "telco_imputed_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Mostrar\n",
    "telco_imputed_df.select(\"tenure\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36561dae-4cde-4f2f-80f7-3cd8e8b7f30e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_to_bin = \"tenure\"\n",
    "display(telco_imputed_df.select(column_to_bin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62828748-1136-4bd9-b9f6-cbd958d325e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Specify bin ranges and column to bin\n",
    "bucketizer = Bucketizer(\n",
    "    splits=[0, 24, 48, float('Inf')],\n",
    "    inputCol=column_to_bin,\n",
    "    outputCol=f\"{column_to_bin}_bins\"\n",
    ")\n",
    "\n",
    "# Apply the bucketizer to the DataFrame\n",
    "bins_df = bucketizer.transform(telco_imputed_df.select(column_to_bin))\n",
    "\n",
    "# Recast bin numbers to integer\n",
    "bins_df = bins_df.withColumn(f\"{column_to_bin}_bins\", col(f\"{column_to_bin}_bins\").cast(\"integer\"))\n",
    "\n",
    "# Display the result\n",
    "display(bins_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a540ff-8a13-4de5-8ae8-d473014cf143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Map back to human-readable embedding scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "751ec948-8599-4dbc-8498-358742ff8bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bins_embedded_df = (\n",
    "    bins_df.withColumn(f\"{column_to_bin}_embedded\", col(f\"{column_to_bin}_bins\").cast(StringType()))\n",
    "    .replace(to_replace={\n",
    "        \"0\": \"<2y\",\n",
    "        \"1\": \"2-4y\",\n",
    "        \"2\": \">4y\"\n",
    "    }, subset=[f\"{column_to_bin}_embedded\"])\n",
    ")\n",
    "\n",
    "display(bins_embedded_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e326db05-ec91-4762-b809-a72accd358e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ordered Indexing\n",
    "\n",
    "Perform ordered indexing as an alternative categorical feature preparation for random forest modeling.\n",
    "\n",
    "Some categoricals are in fact `ordinal` and thus may require additional/manual encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e784270-f472-4c9e-be0f-fe54066fdf25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ordinal_cat = \"Contract\"\n",
    "telco_imputed_df.select(ordinal_cat).distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c03607f1-8f2b-4239-900b-23bf9044eaf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Ordinal (category:index) map/dict\n",
    "ordered_list = [\n",
    "    \"Month-to-month\",\n",
    "    \"One year\",\n",
    "    \"Two year\"\n",
    "]\n",
    "\n",
    "ordinal_dict = {category: f\"{index+1}\" for index, category in enumerate(ordered_list)}\n",
    "display(ordinal_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0302199-abba-4ea7-9305-4a0542ed52e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column with ordered indexing\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "ordinal_df = (\n",
    "    telco_imputed_df\n",
    "    .withColumn(f\"{ordinal_cat}_ord\", col(ordinal_cat))  # Duplicate column\n",
    "    .replace(to_replace=ordinal_dict, subset=[f\"{ordinal_cat}_ord\"])  # Map to ordinal values\n",
    "    .withColumn(f\"{ordinal_cat}_ord\", col(f\"{ordinal_cat}_ord\").cast('int'))  # Cast to integer\n",
    ")\n",
    "\n",
    "display(ordinal_df.select(ordinal_cat, f\"{ordinal_cat}_ord\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdf00618-e850-4c05-ba16-273ba2f6e6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Splitting Data (for Cross-Validation)\n",
    "\n",
    "Split modeling data into a train-test-holdout split as part of a modeling process.\n",
    "\n",
    "In this section, we will perform the best-practice workflow for a train-test split using the Spark DataFrame API.\n",
    "\n",
    "Recall that due to things like changing cluster configurations and data partitioning, it can be difficult to ensure a reproducible train-test split. As a result, we recommend:\n",
    "\n",
    "1. Split the data using the same random seed\n",
    "2. Write out the train and test DataFrames\n",
    "\n",
    "ðŸ’¡ **Hint:** Check out the `randomSplit` [documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.randomSplit.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e0fed9-3d44-41f8-a4db-4df98c5048bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split with 80 percent of the data in train_df and 20 percent of the data in test_df\n",
    "train_df, test_df = telco_imputed_df.randomSplit([.8, .2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0dea10f-6404-41b8-b319-460d905d48b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Materialize (OPTIONAL)\n",
    "train_df.write.mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.telco_customers_train\")\n",
    "test_df.write.mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.telco_customers_baseline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c127a49-a521-40fc-a83f-1340cb170247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Standardize Features in a Training Set\n",
    "\n",
    "For sake of example, we'll pick a column without missing data (e.g. `MonthlyCharges`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b3caf2-d99d-46bc-ade8-a3b16e37c677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, RobustScaler, VectorAssembler\n",
    "\n",
    "# Columnas a escalar\n",
    "num_cols_to_scale = [\"MonthlyCharges\"]  # num_cols\n",
    "\n",
    "# Crear ensamblador\n",
    "assembler = VectorAssembler().setInputCols(num_cols_to_scale).setOutputCol(\"numerical_assembled\")\n",
    "\n",
    "# Transformar train y test\n",
    "train_assembled_df = assembler.transform(train_df.select(*num_cols_to_scale))\n",
    "test_assembled_df = assembler.transform(test_df.select(*num_cols_to_scale))\n",
    "\n",
    "# Definir scaler y ajustar en conjunto de entrenamiento\n",
    "scaler = RobustScaler(inputCol=\"numerical_assembled\", outputCol=\"numerical_scaled\")\n",
    "scaler_fitted = scaler.fit(train_assembled_df)\n",
    "\n",
    "# Aplicar a ambos conjuntos\n",
    "train_scaled_df = scaler_fitted.transform(train_assembled_df)\n",
    "test_scaled_df = scaler_fitted.transform(test_assembled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b973b33-ee29-4ac8-ab30-56f24221c484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Peek at Training set\")\n",
    "train_scaled_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f49bde3-025d-446e-870e-325e0d7a11db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Impute categorical missing values with the mode value using sparkml\n",
    "\n",
    "How to handle missing data only at training time and bake as part of inference pipeline to avoid data leakage and ensure that observation with missing data is used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b3c6cb-c6eb-4003-b278-689f937e5109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols_to_impute = [\"PaymentMethod\"]  # string_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60abfd6b-a736-4344-a64a-10bd6c631e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Index categoricals first as Imputer doesn't handle categoricals directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad69f94c-da80-42b0-a449-f694d440b3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import  col\n",
    "\n",
    "# Index categorical columns using StringIndexer\n",
    "cat_index_cols = [column + \"_index\" for column in categorical_cols_to_impute]\n",
    "cat_indexer = StringIndexer(\n",
    "    inputCols=categorical_cols_to_impute,\n",
    "    outputCols=cat_index_cols,\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Fit on training set\n",
    "cat_indexer_model = cat_indexer.fit(train_df.select(categorical_cols_to_impute))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3439e23-4493-4ca4-be41-773aa0c08e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform both train & test set using the fitted StringIndexer model\n",
    "cat_indexed_train_df = cat_indexer_model.transform(train_df.select(*categorical_cols_to_impute))\n",
    "cat_indexed_test_df = cat_indexer_model.transform(test_df.select(*categorical_cols_to_impute))\n",
    "\n",
    "# display(cat_indexed_train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75a39415-0d54-4c8a-bff2-5364ddf27466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_indexed_train_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e12630dd-df11-42bf-bf32-ccb71396955c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The `StringIndexer` will create a new label (e.g. `4`) for missing when setting the `handleInvalid` flag to `keep`, so it's important to keep track/revert indexes values back to `null` if we want to impute them, otherwise `null` will be treated as their own/separate category automatically.\n",
    "\n",
    "Alternatively for imputing categorical/strings, we can use `.fillna()` method by providing the `mode` value manually (as described above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7eeb883-4ecd-41f4-8d6b-2372e3726991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revert indexes to `null` for missing categories\n",
    "for c in categorical_cols_to_impute:\n",
    "    cat_indexed_train_df = cat_indexed_train_df.withColumn(\n",
    "        f\"{c}_index\",\n",
    "        when(col(c).isNull(), None).otherwise(col(f\"{c}_index\"))\n",
    "    )\n",
    "    cat_indexed_test_df = cat_indexed_test_df.withColumn(\n",
    "        f\"{c}_index\",\n",
    "        when(col(c).isNull(), None).otherwise(col(f\"{c}_index\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79539f5f-4d60-402a-a559-011d8f02934e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_indexed_train_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d0f26d9-2979-489f-83d7-505e4d72e152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fit the imputer on indexed categoricals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9041a871-62ac-4951-89bd-d77bc04e07d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Define 'mode' imputer\n",
    "output_cat_index_cols_imputed = [col + '_imputed' for col in cat_index_cols]\n",
    "mode_imputer = Imputer(\n",
    "    inputCols=cat_index_cols,\n",
    "    outputCols=output_cat_index_cols_imputed,\n",
    "    strategy=\"mode\"\n",
    ")\n",
    "\n",
    "# Fit on training_df\n",
    "mode_imputer_fitted = mode_imputer.fit(cat_indexed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce24fea4-138d-490f-ac2c-f35a949c9263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_indexed_train_imputed_df = mode_imputer_fitted.transform(cat_indexed_train_df)\n",
    "cat_indexed_test_imputed_df = mode_imputer_fitted.transform(cat_indexed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f042ee3e-2690-423c-8cf4-f149a19104da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "776fcfbb-21d5-4fca-840f-6e46298a55d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e8b82dc-8759-4ed8-8d72-66348d2c2025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3eec647-e261-41ec-895e-e45e8adac511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cede832-a5aa-447e-89d7-a0f1c5dbb400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "872fa9d5-bfbf-48f6-82f2-6f28d48f55f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db9926af-5f0a-4017-9f88-ba18c51429dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0134c1b6-0bfe-4be1-8a3d-e3840f4c6f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb84c161-b82c-4840-8f0e-b01989e7c2d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2574f7cb-dd28-4601-aa36-15f8ed5e2adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22e43e3-99bf-43b0-a2c6-782e7c4719ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2036e65b-c97a-4e3a-8e72-4588b6e4a6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6a6248-8b11-44e1-bad0-3ffead159861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e48c5be0-0cdb-42b9-9ae1-3effa58ffb1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e03bb90f-14cc-446d-86c0-8af0561d3649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1- Demo: Data Imputation and Transformation Pipeline 2025-06-10 19_48_03",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
