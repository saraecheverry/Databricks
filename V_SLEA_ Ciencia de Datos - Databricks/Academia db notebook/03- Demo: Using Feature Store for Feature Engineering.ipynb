{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "027e3401-958c-40e0-9a42-db7ff4365483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using Feature Store for Feature Engineering\n",
    "\n",
    "In this demo, we will guide you to explore the use of Feature Stores to enhance feature engineering workflow and understand their crucial role in development of machine learning models. First we will create feature store tables for effective implementation in feature engineering processes and then discuss how to update features. Also, we will cover how to convert existing table to feature tables in Unity Catalog.\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "**By the end of this demo, you will be able to:**\n",
    "\n",
    "1. Create a Feature Store table from a PySpark DataFrame for training/modeling data and holdout data.\n",
    "2. Identify the requirements for a Delta table in Unity Catalog to be automatically configured as a feature table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3294536-6553-4434-8b6a-64b8b76693ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8187cd8-aa97-495c-b10e-e2824839578c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-03.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e37c450f-4343-4a8d-852a-043b87865826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Other Conventions:\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef7a7770-2499-4ebe-aa3f-ca3ef0188b95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:           {DA.username}\")\n",
    "print(f\"Catalog Name:       {DA.catalog_name}\")\n",
    "print(f\"Schema Name:        {DA.schema_name}\")\n",
    "print(f\"Working Directory:  {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:   {DA.paths.datasets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ee4744a-f925-4f92-a416-9cb93aa06b0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Before we save features to a feature table we need to create features that we are interested in. Feature selection criteria depend on your project goals and business problem. Thus, in this section, we will pick some features, however, it doesn't necessarily mean that these features are significant for our purpose.\n",
    "\n",
    "> **One important point is that you need to exclude the target field from the feature table and you need to define a primary key for the table.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bfd0b5d-c1b1-4e4b-be13-aaac5bf2efdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "Typically, first, you will need to conduct data pre-processing and select features. As we covered data pre-processing and feature preparation, we will load a clean dataset which you would typically load from a `silver` table.\n",
    "\n",
    "Let's load in our dataset from a CSV file containing Telco customer churn data from the specified path using Apache Spark.  \n",
    "**In this dataset the target column will be `Churn` and primary key will be `customerID`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b71076-5870-4104-833d-c5be242fd3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_path = f\"{DA.paths.datasets}/telco/telco-customer-churn.csv\"\n",
    "telco_df = spark.read.csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "# Drop the target column\n",
    "telco_df = telco_df.drop(\"Churn\")\n",
    "\n",
    "# View dataset\n",
    "display(telco_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b535619f-1c73-4505-bae8-880499b0e7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Crear la sesión de Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TelcoCustomerChurn\").getOrCreate()\n",
    "#Paso 2: Crear los datos (con base en las imágenes)\n",
    "data = [\n",
    "    (\"7590-VHVEG\", \"Female\", 0, \"Yes\", \"No\", 1, \"No\", \"No phone service\", \"DSL\", \"No\", \"Yes\"),\n",
    "    (\"5575-GNVDE\", \"Male\", 0, \"No\", \"No\", 34, \"Yes\", \"No\", \"DSL\", \"Yes\", \"No\"),\n",
    "    (\"3668-QPYBK\", \"Male\", 0, \"No\", \"No\", 2, \"Yes\", \"No\", \"DSL\", \"Yes\", \"Yes\"),\n",
    "    (\"7795-CFOCW\", \"Male\", 0, \"No\", \"No\", 45, \"No\", \"No phone service\", \"DSL\", \"Yes\", \"No\"),\n",
    "    (\"9237-HQITU\", \"Female\", 0, \"No\", \"No\", 2, \"Yes\", \"No\", \"Fiber optic\", \"No\", \"No\"),\n",
    "    (\"9305-CDSKC\", \"Female\", 0, \"No\", \"No\", 8, \"Yes\", \"Yes\", \"Fiber optic\", \"No\", \"No\")\n",
    "]\n",
    "# Paso 3: Definir el esquema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customerID\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"SeniorCitizen\", IntegerType(), True),\n",
    "    StructField(\"Partner\", StringType(), True),\n",
    "    StructField(\"Dependents\", StringType(), True),\n",
    "    StructField(\"tenure\", IntegerType(), True),\n",
    "    StructField(\"PhoneService\", StringType(), True),\n",
    "    StructField(\"MultipleLines\", StringType(), True),\n",
    "    StructField(\"InternetService\", StringType(), True),\n",
    "    StructField(\"OnlineSecurity\", StringType(), True),\n",
    "    StructField(\"OnlineBackup\", StringType(), True)\n",
    "])\n",
    "# Paso 4: Crear el DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6f04178-e4d2-4dc0-b611-18340ed65f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save Features to Feature Table\n",
    "\n",
    "Let's start creating a [Feature Engineering Client](https://docs.databricks.com/en/machine-learning/feature-store.html) so we can populate our feature store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e6d77b-800f-4aa2-ab2a-d237e3f11c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "help(fe.create_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f885617-5691-4898-a67a-31a4272f5927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Feature Table\n",
    "\n",
    "Next, we can create the Feature Table using the `create_table` method.\n",
    "\n",
    "This method takes a few parameters as inputs:\n",
    "\n",
    "- **name** - A feature table name of the form `<catalog>.<schema_name>.<table_name>`\n",
    "- **primary_keys** - The primary key(s). If multiple columns are required, specify a list of column names.\n",
    "- **timestamp_col** - [OPTIONAL] Any timestamp column which can be used for `point-in-time` lookup.\n",
    "- **df** - Data to insert into this feature table. The schema of `features_df` will be used as the feature table schema.\n",
    "- **schema** - Feature table schema. Note that either `schema` or `features_df` must be provided.\n",
    "- **description** - Description of the feature table.\n",
    "- **partition_columns** - Column(s) used to partition the feature table.\n",
    "- **tags** - Tag(s) to tag feature table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "136f768a-8ed8-4b8d-bec4-701c7ea7975d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a feature table from the dataset\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.telco_customer_features\"\n",
    "\n",
    "fe.create_table(\n",
    "    name=table_name,\n",
    "    primary_keys=[\"customerID\"],\n",
    "    df=telco_df,\n",
    "    # partition_columns=[\"InternetService\"]  # for small datasets partitioning is not recommended\n",
    "    description=\"Telco customer features\",\n",
    "    tags={\"source\": \"bronze\", \"format\": \"delta\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85f9736c-8083-4c83-82c8-a508c4b26bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Alternatively, you can `create_table` with schema only (without `df`), and populate data to the feature table with `fe.write_table`.  \n",
    "`fe.write_table` has `merge` mode ONLY (to overwrite, we should drop and then re-create the table).\n",
    "\n",
    "Example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25b64bfe-9b9e-4607-a103-35574e06d208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Alternatively, you can `create_table` with schema only (without `df`), and populate data to the feature table with `fe.write_table`.  \n",
    "`fe.write_table` has `merge` mode ONLY (to overwrite, we should drop and then re-create the table).\n",
    "\n",
    "Example:\n",
    " One time creation\n",
    "fs.create_table(\n",
    "    name=table_name,\n",
    "    primary_keys=[\"index\"],\n",
    "    schema=telco_df.schema,\n",
    "    description=\"Original Telco data (Silver)\"\n",
    ")\n",
    "\n",
    "Repeated/Scheduled writes\n",
    "fs.write_table(\n",
    "    name=table_name,\n",
    "    df=telco_df,\n",
    "    mode=\"merge\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb8e8d33-2c8a-4198-a076-0e17b8555800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore Feature Table with the UI\n",
    "\n",
    "Now let's explore the UI and see how it tracks the tables that we created.\n",
    "\n",
    "- Click on **Features** from the left panel.\n",
    "- Select the **catalog** that you used for creating the feature table.\n",
    "- Click on the feature table and you should see the table details as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e905108e-9833-4fc6-a706-80d1eba41872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fb62bb8-a2ab-4a61-8ddf-cd352760c371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Feature Table\n",
    "\n",
    "We can also look at the metadata of the feature store via the FeatureStore client by using `get_table()`.  \n",
    "As feature table is a Delta table we can load it with Spark as normally we do for other tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c63cbba2-14a1-4333-8e7a-925c85793622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ft = fe.get_table(name=table_name)\n",
    "print(f\"Feature Table description: {ft.description}\")\n",
    "print(ft.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378ca468-b794-4126-9aa0-c2256197c51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fe.read_table(name=table_name))\n",
    "# display(spark.table(table_name))  # we could just read as delta table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3a23e7-704d-4ba6-b4b7-0f171c502459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Update Feature Table\n",
    "\n",
    "In some cases we might need to update an existing feature table by adding new features or deleting existing features.  \n",
    "In this section, we will show how to make these types of changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b6fadd-7d0a-4391-bab5-4dde4ae96093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Add a New Feature\n",
    "\n",
    "To illustrate adding a new feature, let's redefine an existing one. In this case, we'll transform the `tenure` column by categorizing it into three groups: `short`, `mid`, and `long`, representing different tenure durations.\n",
    "\n",
    "Then we will write the dataset back to the feature table.  \n",
    "The important parameter is the `mode` parameter, which we should set to `\"merge\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "935bad77-ef1e-42d0-85df-9c973310ba99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "telco_df_updated = telco_df.withColumn(\"tenure_group\",\n",
    "    when((telco_df.tenure >= 0) & (telco_df.tenure <= 25), \"short\")\n",
    "    .when((telco_df.tenure > 25) & (telco_df.tenure <= 50), \"mid\")\n",
    "    .when((telco_df.tenure > 50) & (telco_df.tenure <= 75), \"long\")\n",
    "    .otherwise(\"invalid\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb3faf7b-c901-4488-b765-fcb2651fe0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Selecting relevant columns. Use an appropriate mode (e.g., `\"merge\"`) and display the written table for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce5f738d-adb8-44ff-9b25-0fec9962ff0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe.write_table(\n",
    "    name=table_name,\n",
    "    df=telco_df_updated.select(\"customerID\", \"tenure_group\"),  # primary_key and column to add\n",
    "    mode=\"merge\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a96e30-7b9b-472a-9a58-fb85f36b4611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delete Existing Feature\n",
    "\n",
    "To remove a feature column from the table you can just drop the column. Let's drop the original `tenure` column.\n",
    "\n",
    "💡 **Note**: We need to set Delta read and write protocol version manually to support column mapping.  \n",
    "If you want to learn more about this you can check related [documentation page](https://docs.databricks.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb53a23d-8f78-40c4-9b2c-a9365a449ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE telco_customer_features SET TBLPROPERTIES (\n",
    "  'delta.columnMapping.mode' = 'name',\n",
    "  'delta.minReaderVersion' = '2',\n",
    "  'delta.minWriterVersion' = '5'\n",
    ");\n",
    "\n",
    "ALTER TABLE telco_customer_features DROP COLUMNS (tenure);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fba75e1-ad95-48ad-a78c-0c40a13f85d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Feature Table by Version\n",
    "\n",
    "As feature tables are based on Delta tables, we get all nice features of Delta including versioning.  \n",
    "To demonstrate this, let's read from a snapshot of the feature table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a0a91fb-4cc1-4215-9dea-687f09c28fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get timestamp for initial feature table\n",
    "timestamp_v3 = spark.sql(f\"DESCRIBE HISTORY {table_name}\").orderBy(\"version\").collect()[2].timestamp\n",
    "print(timestamp_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99bc6ffa-7cc0-4849-b73d-a5d2749e060c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read previous version using native spark API\n",
    "telco_df_v3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"timestampAsOf\", timestamp_v3)\n",
    "    .table(table_name)\n",
    ")\n",
    "\n",
    "display(telco_df_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e9e9df-9681-450f-adfa-6e6eb300cc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display old version of feature table\n",
    "feature_df = fe.read_table(\n",
    "    name=table_name,\n",
    "    as_of_delta_timestamp=timestamp_v3\n",
    ")\n",
    "\n",
    "feature_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b095773-270c-4709-9ec8-d385a337b15e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a Feature Table from Existing UC Table\n",
    "\n",
    "Alter/Change existing UC table to become a feature table.  \n",
    "Add a primary key (PK) with non-null constraint *(with timestamp if applicable)* on any UC table to turn it into a feature table ([more info here](https://docs.databricks.com)).\n",
    "\n",
    "In this example, we have a table created in the beginning of the demo which contains security features. Let's convert this delta table to a feature table.\n",
    "\n",
    "For this, we need to do these two changes:\n",
    "\n",
    "1. Set primary key columns to `NOT NULL`.\n",
    "2. Alter the table to add the `Primary Key` constraint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c984363a-19f6-4f22-be00-9471954980a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM security_features\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9ef8b4-358e-46ae-ab24-791bcdf18933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE security_features ALTER COLUMN customerID SET NOT NULL;\n",
    "\n",
    "ALTER TABLE security_features ADD CONSTRAINT security_features_pk_constraint \n",
    "PRIMARY KEY (customerID);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f966c15-6446-498f-9d3f-ed48bfdfbca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### *[OPTIONAL]* Migrate Workspace Feature Table to Unity Catalog\n",
    "\n",
    "If you have a classic/workspace feature table, you can migrate it to Unity Catalog feature store. To do that, first, you will need to upgrade the table to UC supported table and then use `UpgradeClient` to complete the upgrade. For instructions please visit [this documentation page](https://docs.databricks.com/).\n",
    "\n",
    "A sample code snippet for upgrading classic workspace table:\n",
    "\n",
    "```python\n",
    "from databricks.feature_engineering import UpgradeClient\n",
    "\n",
    "upgrade_client = UpgradeClient()\n",
    "\n",
    "upgrade_client.upgrade_workspace_table(\n",
    "    source_workspace_table=\"database.test_features_table\",\n",
    "    target_uc_table=f\"{CATALOG}.{SCHEMA}.test_features_table\"\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03- Demo: Using Feature Store for Feature Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
