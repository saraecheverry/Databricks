{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d429fdb-b438-417e-affd-6ea5a3840b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Demo - Build a Feature Engineering Pipeline with Embeddings\n",
    "\n",
    "In this demo, we will build a feature engineering pipeline that performs data loading, imputation, transformation, and embedding generation for categorical features. The pipeline will be applied to training and testing datasets, ensuring consistency in data preprocessing. Finally, we will save the pipeline for future reuse, allowing efficient and reproducible data preparation for machine learning.\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "**By the end of this demo, you will be able to:**\n",
    "\n",
    "- Build a structured feature engineering pipeline that includes multiple preprocessing steps.\n",
    "- Create a pipeline with tasks for data imputation and numerical feature scaling.\n",
    "- Generate embeddings for categorical features to represent categorical data effectively.\n",
    "- Assemble transformed numerical and embedded categorical features into a single feature vector.\n",
    "- Apply the feature engineering pipeline to both training and test datasets.\n",
    "- Display the results of the transformation.\n",
    "- Save a data preparation and feature engineering pipeline to Unity Catalog for potential future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5362bb8a-5837-4dff-bc14-cf10938e591c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "- To run this notebook, you need to use one of the following Databricks runtime(s): **16.0.x-cpu-ml-scala2.12**\n",
    "\n",
    "---\n",
    "\n",
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default. Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - In the drop-down, select **More**.\n",
    "   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select _Open in new tab_.\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "4. Once the cluster is running, complete the steps above to select your cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ee2736-7381-4105-9ce5-12f0617b41a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d59053e7-f46f-43f6-9b34-5f5e344a8047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Other Conventions:\n",
    "\n",
    "Throughout this demo, we’ll refer to the object `DA`. This object, provided by Databricks Academy, contains **variables such as your username, catalog name, schema name, working directory, and dataset locations**. Run the code block below to view these details:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b0410b-8b67-4c6d-b319-be0901ba2e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username: {DA.username}\")\n",
    "print(f\"Catalog Name: {DA.catalog_name}\")\n",
    "print(f\"Schema Name: {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location: {DA.paths.datasets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b771930-3f7a-4baa-836b-6b2c3d7ce3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before constructing the feature engineering pipeline, we need to ensure the dataset is consistent and properly formatted. This includes handling data types, addressing missing values, and preparing the dataset for further transformations. The _Telco Customer Churn_ dataset will be used for this process.\n",
    "\n",
    "### Steps in Data Preparation:\n",
    "\n",
    "1. Load the dataset into a Spark DataFrame.\n",
    "2. Split the dataset into training and testing sets.\n",
    "3. Convert Integer and Boolean columns to Double to ensure compatibility with Spark ML.\n",
    "4. Handle missing values by identifying and imputing them in:\n",
    "   - Numeric columns\n",
    "   - String columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee5f62f9-4110-43a5-8465-907e1dec37a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "We start by loading the dataset from the specified file path using Spark.\n",
    "\n",
    "> This step ensures that only relevant columns are included for feature engineering and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f2bf93c-9887-47d6-95c7-dd549dc93e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Load dataset with spark\n",
    "shared_volume_name = 'telco'  # From Marketplace\n",
    "csv_name = 'telco-customer-churn-missing'  # CSV file name\n",
    "\n",
    "dataset_path = f\"{DA.paths.datasets.telco}/{shared_volume_name}/{csv_name}.csv\"  # Full path\n",
    "\n",
    "telco_df = spark.read.csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "# Select columns of interest\n",
    "telco_df = telco_df.select(\n",
    "    \"gender\", \"SeniorCitizen\", \"Partner\", \"tenure\", \"InternetService\",\n",
    "    \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"TotalCharges\", \"Churn\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c101035-0f9e-4db9-a553-e8f0520b73f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# se toma captura de pantalla de los datos y se crea un df\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType\n",
    "\n",
    "# Crear sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"ChurnData\").getOrCreate()\n",
    "\n",
    "# Crear los datos manualmente (según la imagen)\n",
    "data = [\n",
    "    (None, False, \"Yes\", 1, \"DSL\", \"Month-to-month\", \"Yes\", \"Electronic check\", 29.85, \"No\"),\n",
    "    (\"Male\", False, \"No\", 34, \"DSL\", \"One year\", \"No\", \"Mailed check\", 1889.5, \"Yes\"),\n",
    "    (\"Male\", False, \"Yes\", 2, \"DSL\", \"Month-to-month\", \"Yes\", \"Mailed check\", 108.15, \"No\"),\n",
    "    (\"Male\", False, \"No\", 45, \"DSL\", \"One year\", \"No\", \"Bank transfer (automatic)\", 1840.75, \"No\"),\n",
    "    (\"Female\", False, \"No\", None, \"Fiber optic\", \"Month-to-month\", \"Yes\", \"Electronic check\", 151.65, \"Yes\"),\n",
    "    (\"Female\", False, \"No\", 8, \"Fiber optic\", \"Month-to-month\", \"Yes\", \"Electronic check\", 820.5, \"Yes\"),\n",
    "    (\"Male\", False, \"No\", None, \"Fiber optic\", \"Month-to-month\", \"Yes\", \"Credit card (automatic)\", 1949.4, \"Yes\"),\n",
    "    (\"Female\", False, \"No\", 10, \"DSL\", \"Month-to-month\", \"No\", \"Mailed check\", 301.9, \"No\"),\n",
    "    (None, False, \"Yes\", 28, \"Fiber optic\", \"Month-to-month\", \"Yes\", \"Electronic check\", 3046.05, \"Yes\"),\n",
    "    (\"Male\", False, \"No\", 62, \"DSL\", \"One year\", \"No\", \"Bank transfer (automatic)\", 3487.95, \"No\"),\n",
    "    (\"Male\", False, \"Yes\", 13, None, \"Month-to-month\", \"Yes\", \"Mailed check\", 587.45, \"No\"),\n",
    "    (\"Male\", False, \"No\", 16, \"No\", \"Two year\", \"No\", \"Credit card (automatic)\", 326.8, \"No\"),\n",
    "    (None, False, \"Yes\", 58, \"Fiber optic\", \"One year\", \"No\", \"Credit card (automatic)\", 5681.1, \"No\"),\n",
    "    (\"Male\", False, None, 49, \"Fiber optic\", \"Month-to-month\", \"Yes\", None, None, \"Yes\"),\n",
    "]\n",
    "\n",
    "# Definir el esquema\n",
    "schema = StructType([\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"SeniorCitizen\", BooleanType(), True),\n",
    "    StructField(\"Partner\", StringType(), True),\n",
    "    StructField(\"tenure\", IntegerType(), True),\n",
    "    StructField(\"InternetService\", StringType(), True),\n",
    "    StructField(\"Contract\", StringType(), True),\n",
    "    StructField(\"PaperlessBilling\", StringType(), True),\n",
    "    StructField(\"PaymentMethod\", StringType(), True),\n",
    "    StructField(\"TotalCharges\", FloatType(), True),\n",
    "    StructField(\"Churn\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Crear DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "329d6ce1-88a5-4325-8c2d-2907815cc914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "Some columns require cleaning and type conversion to maintain consistency and compatibility with machine learning models.\n",
    "\n",
    "### Handling Null Values\n",
    "\n",
    "- In some datasets, missing values might be stored as the string `\"null\"`, which needs to be properly converted to `NULL` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1d13f52-dd38-4030-a5a8-77d042ce6911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace string \"null\" with actual NULL values\n",
    "for column in telco_df.columns:\n",
    "    telco_df = telco_df.withColumn(column, when(col(column) == \"null\", None).otherwise(col(column)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7d305a5-6fdd-4b3e-9299-92c39dfb3c56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Converting Data Types\n",
    "\n",
    "- The `SeniorCitizen` column is a binary categorical variable (0 or 1) and should be converted to **Boolean**.\n",
    "- The `TotalCharges` column should be cast to a **double** type since it represents a numerical feature.\n",
    "\n",
    "> These conversions help in maintaining proper data representation and ensure compatibility with Spark ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb65b1b5-9968-4f6f-8456-7b6272e4ac4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clean-up columns\n",
    "telco_df = telco_df.withColumn(\"SeniorCitizen\", when(col(\"SeniorCitizen\") == 1, True).otherwise(False))\n",
    "telco_df = telco_df.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"double\"))\n",
    "\n",
    "display(telco_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e04d621f-d095-4f19-b01d-ff82f81229db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Splitting the Dataset into Training and Testing Sets\n",
    "\n",
    "Once the data has been cleaned, we split it into training and testing sets using an 80-20 split.\n",
    "\n",
    "- Since `telco_df` is a PySpark DataFrame, we will use `randomSplit()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adbbbe61-8aaf-478c-a842-07212c676f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = telco_df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c85ccf11-db11-4a20-9594-be846a5b85c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transforming the Dataset\n",
    "\n",
    "To ensure that all numerical and categorical features are compatible with machine learning algorithms, we perform several transformations.\n",
    "\n",
    "### Convert Integer and Boolean Columns to Double\n",
    "\n",
    "- Many machine learning algorithms require numeric input, so we convert all **integer** and **boolean** columns to **double**.\n",
    "\n",
    "> This ensures numerical consistency in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8385c529-bd65-4f5e-a347-d8e4ac8d3912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Get a list of integer & boolean columns\n",
    "integer_cols = [column.name for column in train_df.schema.fields \n",
    "                if column.dataType == IntegerType() or column.dataType == BooleanType()]\n",
    "\n",
    "# Loop through integer columns to cast each one to double\n",
    "for column in integer_cols:\n",
    "    train_df = train_df.withColumn(column, col(column).cast(\"double\"))\n",
    "    test_df = test_df.withColumn(column, col(column).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f9331ce-69de-4192-ba0b-589e3754a5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Identifying Missing Values\n",
    "\n",
    "Handling missing data is crucial to prevent errors and bias in machine learning models. We first check for missing values in numerical and categorical columns.\n",
    "\n",
    "- **Find Numeric Columns with Missing Values**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa24e64-5584-427a-87dd-f73f0deb82b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "# Identify numeric columns\n",
    "num_cols = [c.name for c in train_df.schema.fields if c.dataType == DoubleType()]\n",
    "\n",
    "# Count missing values in numeric columns\n",
    "num_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in num_cols]\n",
    "row_dict_num = train_df.select(num_missing_values_logic).first().asDict()\n",
    "num_missing_cols = [column for column in row_dict_num if row_dict_num[column] > 0]\n",
    "\n",
    "print(f\"Numeric columns with missing values: {num_missing_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f0a1712-b016-4b66-996d-c0a06d0bb18b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Find string columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7955047b-fcc4-4427-a1bc-ae46d64fc1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify string columns\n",
    "string_cols = [c.name for c in train_df.schema.fields if c.dataType == StringType()]\n",
    "\n",
    "# Count missing values in string columns\n",
    "string_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in string_cols]\n",
    "row_dict_string = train_df.select(string_missing_values_logic).first().asDict()\n",
    "string_missing_cols = [column for column in row_dict_string if row_dict_string[column] > 0]\n",
    "\n",
    "print(f\"String columns with missing values: {string_missing_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03568787-df0b-4582-b0e1-4d709be96f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating a Feature Engineering Pipeline\n",
    "\n",
    "To efficiently preprocess and transform data for machine learning, we construct a **Spark ML pipeline**. This pipeline automates key preprocessing steps, ensuring **consistency** and **reproducibility** in data preparation. The pipeline processes the _Telco Customer Churn_ dataset by performing:\n",
    "\n",
    "### Key Feature Engineering Steps:\n",
    "\n",
    "#### Generating Embeddings for Categorical Features\n",
    "- Instead of traditional categorical encoding techniques, we generate **dense vector representations** (non-zero embeddings) using SparkML’s **Word2Vec**.\n",
    "- These embeddings capture **semantic relationships** between categories, improving model performance.\n",
    "\n",
    "> We are interested in the differences between a **senior citizen with fiber optic internet** and a **senior citizen with DSL**, for example. Therefore, we embed those datasets separately. Otherwise, **fiber optic** and **senior citizen status** would have their own embeddings.\n",
    "\n",
    "#### Handling Missing Values\n",
    "- Missing values in **numerical columns** (e.g., `tenure`, `TotalCharges`) are imputed using the **mean strategy** to ensure completeness.\n",
    "- Missing **categorical values** are automatically encoded as a separate category.\n",
    "\n",
    "#### Standardizing Numerical Features\n",
    "- SparkML’s `VectorAssembler` **combines numerical columns** into a single feature vector.\n",
    "- SparkML’s `StandardScaler` **standardizes numerical values**, reducing sensitivity to outliers.\n",
    "\n",
    "#### Combining Features into a Final Vector\n",
    "- The **scaled numerical features** and **embedded categorical representations** are **combined** into a single feature vector.\n",
    "- This ensures a **structured and uniform format** for machine learning models.\n",
    "\n",
    "#### Encapsulating Steps into a Pipeline\n",
    "- All preprocessing steps are included in a **Spark ML pipeline**, making the transformations **modular and reusable**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "019c52bc-edea-48bd-a090-67ac825273c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generating Embeddings for Categorical Features\n",
    "\n",
    "To improve model performance, we create **embedding vectors** for categorical columns using **Word2Vec**. These embeddings capture complex relationships between different categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a006975-d96c-45f7-bd97-5de0dc6428af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, Word2Vec, VectorAssembler\n",
    "from pyspark.sql.functions import split, concat_ws, col\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def generate_categorical_embeddings(df, categorical_cols, vector_size=5):\n",
    "    \"\"\"\n",
    "    Generate embeddings for categorical columns using Word2Vec.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Input Spark DataFrame\n",
    "    - categorical_cols (list): List of categorical column names\n",
    "    - vector_size (int): Size of the embedding vectors\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with embeddings as a single vector column\n",
    "    \"\"\"\n",
    "    # Replace NULL categorical values with \"unknown\"\n",
    "    for col_name in categorical_cols:\n",
    "        df = df.withColumn(col_name, F.when(F.col(col_name).isNull(), \"unknown\").otherwise(F.col(col_name)))\n",
    "\n",
    "    # Combine all categorical columns into a single text column for Word2Vec\n",
    "    df = df.withColumn(\"categorical_sequence\", concat_ws(\" \", *categorical_cols))\n",
    "\n",
    "    # Tokenize categorical data\n",
    "    df = df.withColumn(\"categorical_tokens\", split(col(\"categorical_sequence\"), \" \"))\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    word2vec = Word2Vec(vectorSize=vector_size, minCount=0, inputCol=\"categorical_tokens\", outputCol=\"embedding_struct\")\n",
    "    model = word2vec.fit(df)\n",
    "    df = model.transform(df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bf2870c-c40c-451d-8619-7aa2903fa09e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Applying Embeddings to Categorical Features\n",
    "\n",
    "We apply the embedding function to the categorical columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "976332d3-1738-47e6-a394-e5e8c5da2788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_columns = [\"gender\", \"Partner\", \"InternetService\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"Churn\"]\n",
    "\n",
    "# Generate embeddings for categorical columns\n",
    "train_df = generate_categorical_embeddings(train_df, categorical_columns)\n",
    "test_df = generate_categorical_embeddings(test_df, categorical_columns)\n",
    "\n",
    "display(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f9ab95-9f3d-4075-ad24-575607e57698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Converting Embeddings into Dense Vectors\n",
    "\n",
    "For compatibility with Spark ML models, we convert embedding lists into **DenseVector** format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34ac89b4-bfa2-4801-89aa-7232e319ed2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the 'values' field from the Word2Vec struct\n",
    "DenseVector_udf = F.udf(lambda v: DenseVector(v.values) if v else DenseVector([0.0] * 5), VectorUDT())\n",
    "\n",
    "# Convert embeddings into DenseVectors\n",
    "for col_name in categorical_columns:\n",
    "    train_df = train_df.withColumn(col_name + \"_embedding\", DenseVector_udf(F.col(\"embedding_struct\")))\n",
    "    test_df = test_df.withColumn(col_name + \"_embedding\", DenseVector_udf(F.col(\"embedding_struct\")))\n",
    "\n",
    "# Drop unnecessary columns after embeddings are extracted\n",
    "train_df = train_df.drop(\"categorical_sequence\", \"categorical_tokens\", \"embedding_struct\")\n",
    "test_df = test_df.drop(\"categorical_sequence\", \"categorical_tokens\", \"embedding_struct\")\n",
    "\n",
    "display(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "878219ed-4e7c-4226-82cc-49915ee6b300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preview the Embedded Features\n",
    "\n",
    "We can inspect a sample of categorical features transformed into embedding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ee1dd9-6363-4579-80dc-cb7ee34b7829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show a sample of embedded categorical features\n",
    "train_df.select(\"PaymentMethod\", \"PaymentMethod_embedding\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d3715b3-f5d9-4a88-ae9f-a30b7e3d137b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering and Pipeline Initialization\n",
    "\n",
    "Now that categorical columns have been transformed into embeddings, we finalize the feature engineering steps.\n",
    "\n",
    "- **Handle Missing Values in Numerical Columns**\n",
    "  - Impute missing numerical values with the mean of each column.\n",
    "\n",
    "- **Standardize Numerical Features**\n",
    "  - Use `StandardScaler` to normalize numerical features, reducing sensitivity to outliers.\n",
    "\n",
    "- **Assemble the Final Feature Vector**\n",
    "  - Combine numerical and categorical embeddings into a single feature vector.\n",
    "\n",
    "- **Initializing the Spark ML Pipeline**\n",
    "  - Encapsulate all transformations into a Spark ML Pipeline for structured data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "400ee769-e977-4d5f-b1b2-b3b8ba9987fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define numerical columns for imputation\n",
    "numerical_cols = [\"SeniorCitizen\", \"tenure\", \"TotalCharges\"]\n",
    "\n",
    "# Impute missing numerical features\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_cols,\n",
    "    outputCols=[col + \"_imputed\" for col in numerical_cols]\n",
    ")\n",
    "\n",
    "# Assemble numerical columns into a single vector\n",
    "numerical_assembler = VectorAssembler(\n",
    "    inputCols=[col + \"_imputed\" for col in numerical_cols],\n",
    "    outputCol=\"numerical_assembled\"\n",
    ")\n",
    "\n",
    "# Scale numerical features to standardize values\n",
    "numerical_scaler = StandardScaler(\n",
    "    inputCol=\"numerical_assembled\",\n",
    "    outputCol=\"numerical_scaled\"\n",
    ")\n",
    "\n",
    "# Assemble all features (numerical + categorical embeddings) into a single feature vector\n",
    "feature_cols = [\"numerical_scaled\"] + [col + \"_embedding\" for col in categorical_columns]\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"all_features\"\n",
    ")\n",
    "\n",
    "# Define the sequence of transformations\n",
    "stages_list = [imputer, numerical_assembler, numerical_scaler, vector_assembler]\n",
    "\n",
    "# Instantiate the pipeline\n",
    "pipeline = Pipeline(stages=stages_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a304f1-561a-4f74-bd94-29609fb8d4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fit the Pipeline\n",
    "\n",
    "In the context of machine learning and MLflow, `fitting` corresponds to the process of training a machine learning model on a specified dataset.\n",
    "\n",
    "In the previous step we created a pipeline. Now, we will fit a model based on the pipeline. This pipeline will impute missing values, scale numerical columns, generate embeddings for categorical variables, and create a feature vector for modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10221ab8-8ce1-4a81-a08d-76505b308664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What Happens During Fitting?\n",
    "\n",
    "When we call `.fit(train_df)`, the pipeline applies the following transformations:\n",
    "\n",
    "- **Imputation of Missing Values**\n",
    "  - The `Imputer` calculates the **mean** for numerical columns in `train_df` and replaces missing values accordingly.\n",
    "\n",
    "- **Scaling of Numerical Features**\n",
    "  - The `StandardScaler` computes **scaling factors** based on the distribution of numerical features.\n",
    "  - These factors are applied uniformly across datasets to **normalize feature values**.\n",
    "\n",
    "- **Generating Embeddings for Categorical Variables**\n",
    "  - The `Word2Vec` model converts categorical text data into **dense vector representations**.\n",
    "  - These embeddings **capture semantic relationships** between categories.\n",
    "  - The trained embedding model is **stored** and later applied to unseen data.\n",
    "\n",
    "- **Combining Features into a Single Vector**\n",
    "  - The `VectorAssembler` consolidates:\n",
    "\n",
    "    - ▪ **Scaled numerical features** – standardized values from `StandardScaler`.\n",
    "    - ▪ **Categorical embeddings** – dense embeddings generated by `Word2Vec`.\n",
    "\n",
    "This results in a **final feature vector**, ready for input into machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabcce67-190e-42a4-8849-12dfcdb19445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit the Pipeline\n",
    "pipeline_model = pipeline.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09cecc73-d3b7-4ac1-9bee-47bb16c01e49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply the Feature Engineering Pipeline\n",
    "\n",
    "Once the pipeline is **fitted** to the training data, it can be **applied to any dataset** using `.transform()`.\n",
    "\n",
    "We apply the pipeline to both:\n",
    "\n",
    "- **Train Dataset (`train_df`)** → Generates **transformed training features**.\n",
    "- **Test Dataset (`test_df`)** → Ensures that the same transformations are applied consistently.\n",
    "\n",
    "The output is a **transformed dataset** with the **final feature vector** ready for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf495d84-e1d4-4fb5-8a95-cd984e77d45e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform both training_df and test_df\n",
    "train_transformed_df = pipeline_model.transform(train_df)\n",
    "test_transformed_df = pipeline_model.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65e20249-9930-42be-a6a8-21ff1cb86f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show transformed features\n",
    "train_transformed_df.select(\"all_features\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adaa1826-1ca1-4ff8-b71e-1414e85ce160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save and Reuse the Pipeline\n",
    "\n",
    "Preserving the Telco Customer Churn Prediction pipeline, encompassing the model, parameters, and metadata, is vital for maintaining reproducibility, enabling version control, and facilitating collaboration among team members. This ensures a detailed record of the machine learning workflow. In this section, we will follow these steps:\n",
    "\n",
    "1. **Save the Pipeline**: Save the pipeline model, including all relevant components, to the designated artifact storage. The saved pipeline is organized within the `spark_pipelines` folder for clarity.\n",
    "2. **Explore Loaded Pipeline Stages**: Upon loading the pipeline, inspect the stages to reveal key transformations and understand the sequence of operations applied during the pipeline's execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73d3f950-4455-4727-b4a4-1f3bbee7b05c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the pipeline model with overwrite mode\n",
    "pipeline_model.write().overwrite().save(f\"{DA.paths.working_dir}/spark_pipelines\")\n",
    "print(f\"Saved model to: {DA.paths.working_dir}/spark_pipelines\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d0cd343-ac79-48c5-a24d-b6747991d0d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and Use Saved Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba6ad4c1-0c54-418d-990c-afb70090dfcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and use the saved model\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_pipeline = PipelineModel.load(f\"{DA.paths.working_dir}/spark_pipelines\")\n",
    "\n",
    "# Show pipeline stages\n",
    "loaded_pipeline.stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ba72668-7e30-4b9e-a57e-45173d2fd2cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the loaded pipeline to transform the test dataset\n",
    "test_transformed_df = loaded_pipeline.transform(test_df)\n",
    "display(test_transformed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f470969b-1abc-4ded-b415-272a6d67cc9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we built a feature engineering pipeline to streamline data preparation. The pipeline handled data loading, missing value imputation, numerical feature scaling, and generated embeddings for categorical variables using `Word2Vec`.\n",
    "\n",
    "By applying the pipeline to both training and test sets, we ensured a consistent and reproducible feature transformation process. Finally, saving the pipeline allows for future reuse, enabling efficient and standardized data preprocessing for machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01472e3-d311-4abe-aae6-f2c8c47b643c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02- Demo: Build a Feature Engineering Pipeline with Embeddings 2025-06-14 09_45_47",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
