{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79f6a428-6c1c-4fce-8dc4-c5fc108673f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline Deployment\n",
    "\n",
    "In this demo, we will show how to use a model as part of a data pipeline for inference. In the first section of the demo, we will prepare data and perform some basic feature engineering. Then, we will fit and register the model to model registry. Please note that these two steps are already covered in other courses and they are not the main focus of this demo. In the last section, which is the main focus of this demo, we will create a Delta Live Tables (DLT) pipeline and use the registered model as part of the pipeline.\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "**By the end of this demo, you will be able to:**\n",
    "\n",
    "- Describe steps for deploying a model within a pipeline.\n",
    "- Develop a simple Delta Live Tables pipeline that performs batch inference in its final step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8809ef-35d9-4604-83f9-672e69a173a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "- To run this notebook, you need to use one of the following Databricks runtime(s): `{{supported_dbrs}}`\n",
    "\n",
    "ðŸ“› **Prerequisites**:\n",
    "\n",
    "- **Feature Engineering** and **Feature Store** are not focus of this lesson. This course expects that you already know these topics. If not, you can check the *Data Preparation for Machine Learning* course.\n",
    "\n",
    "- Model development with MLflow is not in the scope of this course. If you need to refresh your knowledge about model tracking and logging, you can check the *Machine Learning Model Development* course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bab85b5f-f207-47ac-a265-c664aced33d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c26633-0da5-43c8-bf3a-f74988aca3f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-01\n",
    "\n",
    "# Note: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f3f9141-c2cf-4be3-82ed-8ff453baa038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this demonstration, we will utilize a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including gender, as well as internet subscription details such as subscription plans and payment methods.\n",
    "\n",
    "After loading the dataset, we will perform simple **data cleaning and feature selection**.\n",
    "\n",
    "In the final step, we will split the dataset to **features** and **response** sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67414e9b-3fe7-46c1-84fa-17ff256f9c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Dataset path\n",
    "dataset_p_telco = f\"{DA.paths.datasets}/telco/telco-customer-churn.csv\"\n",
    "\n",
    "# Dataset specs\n",
    "primary_key = \"customerID\"\n",
    "response = \"churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]  # Keeping numerical only for simplicity and demo purposes\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "    .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"double\"))\\\n",
    "    .na.drop(\"any\")\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = telco_df.select(primary_key, *features)\n",
    "response_df = telco_df.select(primary_key, response)\n",
    "\n",
    "# Train a sklearn Decision Tree Classification model\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()\n",
    "\n",
    "for col in X_train_pdf.select_dtypes(\"int32\"):\n",
    "    X_train_pdf[col] = X_train_pdf[col].astype(\"double\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75235bf3-b6ad-41d0-8930-067d38d5be9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Preparation\n",
    "\n",
    "> **Note:** This section is not the main focus of this course. We are just repeating the model development and registration process here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b37d9eb9-1678-46b6-952a-be41cec60b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Model Registry with UC\n",
    "\n",
    "Before we start model deployment, we need to fit and register a model. In this demo, **we will log models to Unity Catalog**, which means first we need to setup the **MLflow Model Registery URI**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c08fe8c3-28bc-475e-86d5-85507b173c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get latest model version\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    return max([model_version_info.version for model_version_info in model_version_infos])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c82e457-7eca-451f-9ba4-b2e2da3c1548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fit and Register a Model with UC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2e5dc0-2028-4df9-8e7f-dbfa486ae2e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Use 3-level namespace for model name\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.ml_model\"\n",
    "\n",
    "# model to use for classification\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=10)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Model-Deployment demo\") as mlflow_run:\n",
    "\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        log_models=False,\n",
    "        log_post_training_metrics=True,\n",
    "        silent=True\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train_pdf, Y_train_pdf)\n",
    "\n",
    "    # Log model and push to registry\n",
    "    signature = infer_signature(X_train_pdf, Y_train_pdf)\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf,\n",
    "        artifact_path=\"decision_tree\",\n",
    "        signature=signature,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "\n",
    "# Set model alias\n",
    "client.set_registered_model_alias(model_name, \"DLT\", get_latest_model_version(model_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d36ed96-dd1b-407c-a1f4-6b651bb0b016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Pipeline to Run Batch Inference\n",
    "\n",
    "Now that our model is registered and ready, we can move on the most important part; using the model for inference inside a pipeline.\n",
    "\n",
    "**Note:** The DLT pipeline is already defined in `3.1.b` notebook.  \n",
    "**Note:** If you want to learn more about DLT please check out `Data Engineering with Databricks (Data Pipeline with Delta Live Tables)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb2af94a-38b0-4069-8964-562b1a6924d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Config Variables\n",
    "\n",
    "While defining the DLT pipeline, you will need to use the following variables. Run the code block below first. Then, use the output in the next section while creating the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb9a912-82e8-484f-9bf0-4438b96d4d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"mlpipeline.bronze_dataset_path: {dataset_p_telco}\")\n",
    "print(f\"mlpipeline.model_name: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d991815b-1fdf-4cd8-a57e-4f93c8ccde15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "va la seccion de delta live table y crea un pipeline manual desde ahÃ­ , \n",
    "selecciona un notebook, selecciona data, nos vamos para otro notebook que tienen lo siguiente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de08ea2-8392-4f75-bc80-d1306b120f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference Pipeline\n",
    "\n",
    "MLflow-trained models can be used in Delta Live Tables pipelines. MLflow models are treated as transformations in Databricks, meaning they act upon a Spark DataFrame input and return results as a Spark DataFrame. Because Delta Live Tables defines datasets against DataFrames, you can convert Apache Spark workloads that leverage MLflow to Delta Live Tables with just a few lines of code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58fda861-4f26-40e1-a9ac-c0831bcea721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e97ac20-cc2a-4a02-be8a-fe077787bed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_dataset_path = spark.conf.get(\"mlpipeline.bronze_dataset_path\")\n",
    "model_name = spark.conf.get(\"mlpipeline.model_name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82904591-ff51-4d96-b9a7-717f7fe33c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62874ab0-aeb7-47f2-84fe-34ecb3d6505b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_uri=f\"models:/{model_name}@DLT\"\n",
    "loaded_model_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri, result_type=\"string\")\n",
    "\n",
    "primary_key = \"customerID\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2311c58-28d3-4772-9193-2b5a2558e5d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DLT Inference code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acdd181a-c8cd-4f52-a401-d33845c84992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, struct\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"raw_inputs\",\n",
    "  comment=\"Raw inputs table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\"\n",
    "  }\n",
    ")\n",
    "def raw_inputs():\n",
    "  return spark.read.csv(bronze_dataset_path, inferSchema=True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"features_input\",\n",
    "  comment=\"Features table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"silver\"\n",
    "  }\n",
    ")\n",
    "def features_input():\n",
    "  return (\n",
    "    dlt.read(\"raw_inputs\")\n",
    "      .select(primary_key, *features)\n",
    "      .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"double\"))\n",
    "      .na.drop(how=\"any\")\n",
    "  )\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"model_predictions\",\n",
    "  comment=\"Inference table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"gold\"\n",
    "  }\n",
    ")\n",
    "def model_predictions():\n",
    "  return (\n",
    "    dlt.read(\"features_input\")\n",
    "      .withColumn(\"prediction\", loaded_model_udf(struct(features)))\n",
    "  )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "11-M3_Demo2: pipeline Deployment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
