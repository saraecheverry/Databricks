{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b26105d-ecbc-4b29-a095-150341bd6dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18dd7b18-1217-4b74-8b5e-4fb553bc1adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Compute summary statistics on a Spark DataFrame using .summary() or dbutils data summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b8cfdc-848c-43a8-aecb-a169a8117ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Crear SparkSession\n",
    "spark = SparkSession.builder.appName(\"ResumenVinos\").getOrCreate()\n",
    "\n",
    "# Definir esquema\n",
    "schema = StructType([\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"tipo\", StringType(), True),\n",
    "    StructField(\"alcohol\", DoubleType(), True),\n",
    "    StructField(\"acidez\", DoubleType(), True),\n",
    "    StructField(\"dulzor\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Datos de ejemplo\n",
    "datos = [\n",
    "    (\"Frutal Rosado\", \"joven\", 11.5, 3.2, 4.0),\n",
    "    (\"Dulce Mora\", \"joven\", 12.0, 3.5, 6.0),\n",
    "    (\"Cítrico Blanco\", \"joven\", 10.8, 3.8, 2.5),\n",
    "    (\"Suave Tropical\", \"joven\", 11.2, 3.3, 5.0),\n",
    "    (\"Vino de Uva\", \"joven\", 12.2, 3.6, 3.5)\n",
    "]\n",
    "\n",
    "# Crear DataFrame\n",
    "df = spark.createDataFrame(data=datos, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3fcc0e-d886-4985-86b5-371b23c63b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar describe\n",
    "# df.describe().show()\n",
    "\n",
    "# Aplicar summary (más completo)\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d3de3d-f7d6-4a86-8ac0-25e4e115e196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.data.summarize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4778e14-e54c-48c3-bcc5-64ec36c2ebc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ab7bc9-7d8c-4491-96cf-598ae87b590a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81fe7f01-f47e-441b-aa26-4e06a9b7c482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📌 Puntos clave\n",
    "**.summary() (PySpark)**\n",
    "✅ Se usa para obtener estadísticas resumidas de todas las columnas (numéricas y no numéricas).\n",
    "✅ Proporciona: 'count', 'mean', 'stddev', 'min', '25%', '50%', '75%', 'max'\n",
    "✅ Incluye percentiles → mejor que .describe().\n",
    "✅ Requiere .show() para visualizar\n",
    "\n",
    "**.describe() (comparación)(PySpark)**\n",
    "Solo da: 'count', 'mean', 'stddev', 'min', 'max'\n",
    "❌ No incluye percentiles\n",
    "❌ Menos completa → no es suficiente para análisis exploratorio completo\n",
    "\n",
    "**dbutils.data.summarize(df) (Databricks)**\n",
    "✅ Abre una vista visual interactiva del DataFrame.\n",
    "✅ Muestra automáticamente:\n",
    "    Distribución de cada columna (gráficos)\n",
    "    Estadísticas básicas y tipo de variable\n",
    "    Recuentos para categorías\n",
    "    Detección de outliers\n",
    "✅ No necesita show(), pero solo funciona dentro de notebooks Databricks.\n",
    "\n",
    "**display(df) (Databricks)**\n",
    "Muestra la tabla con paginación.\n",
    "Permite elegir visualizaciones:\n",
    "\n",
    "✅ Histogram\n",
    "✅ Bar chart\n",
    "✅ Line chart\n",
    "✅ Box plot\n",
    "✅ Scatter plot\n",
    "✅ Map (si hay coordenadas)\n",
    "❌ No imprime estadísticas numéricas automáticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6493e2bb-465e-4622-a992-2067c3ba7d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Remove outliers from a Spark DataFrame based on standard deviation or IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fa9e127-fea0-4945-b847-0d506fa11701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Standard deviation\n",
    "\n",
    "Un valor es outlier si está fuera del rango: **μ ± k ⋅ σ**\n",
    "\n",
    "Donde:\n",
    "- μ es la media\n",
    "- σ es la desviación estándar\n",
    "- k suele ser 2 o 3\n",
    "\n",
    "Pasos en PySpark:\n",
    "- Calcular mean y stddev con .agg()\n",
    "- Filtrar con .filter() los valores fuera del rango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "430e692f-79e6-4688-a66e-ef772be6e6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "\n",
    "mean_std = df.select(mean(\"price\").alias(\"mean\"), stddev(\"price\").alias(\"stddev\")).collect()[0]\n",
    "mean_val, stddev_val = mean_std[\"mean\"], mean_std[\"stddev\"]\n",
    "# Rango permitido\n",
    "lower_bound = mean_val - 2 * stddev_val\n",
    "upper_bound = mean_val + 2 * stddev_val\n",
    "filtered_df = df.filter((df[\"price\"] >= lower_bound) & (df[\"price\"] <= upper_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b71570f7-c68f-4526-99a3-a340ace05602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## IQR (Interquartile Range)\n",
    "\n",
    "Usa los cuartiles Q1, Q3 y el IQR:\n",
    "\n",
    "**IQR = Q3 − Q1**\n",
    "\n",
    "Outliers:\n",
    "- menores que Q1 − 1.5 ⋅ IQR\n",
    "- mayores que Q3 + 1.5 ⋅ IQR\n",
    "\n",
    "Pasos en PySpark:\n",
    "- Obtener cuartiles con .approxQuantile()\n",
    "- Calcular IQR\n",
    "- Filtrar fuera de los límites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50ef2d78-afb7-4fde-a025-2ec8c632eb46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pyspark con .approxQuantile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa8d8519-165f-4ec2-ab89-33449db9b853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q1, q3 = df.approxQuantile(\"price\", [0.25, 0.75], 0.0)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "filtered_df = df.filter((df[\"price\"] >= lower_bound) & (df[\"price\"] <= upper_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf19b400-b3a0-4560-a033-ef08a5d7a951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pandas con .quantile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803e464e-f9a3-469e-940f-14d2b6cfd7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ejemplo en pandas \n",
    "\n",
    "Q1 = df['col'].quantile(0.25)\n",
    "Q3 = df['col'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "filtered_df = df[(df['col'] >= Q1 - 1.5 * IQR) & (df['col'] <= Q3 + 1.5 * IQR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a195aa49-6c13-49e9-b495-7fb0fe1eb834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📌 Puntos clave\n",
    "¿Cuándo usar cada método?\n",
    "\n",
    "| Método                  | Cuándo usar                                                                    |\n",
    "| ----------------------- | ------------------------------------------------------------------------------ |\n",
    "| **Desviación estándar** | Cuando la variable sigue una distribución **normal o simétrica**               |\n",
    "| **IQR**                 | Cuando la variable tiene una distribución **sesgada o con outliers** evidentes |\n",
    "\n",
    "**Nota:**\n",
    "\n",
    "- **Distribución normal o simétrica:** Se representa gráficamente como una curva en forma de campana\n",
    "- **Distribución sesgada con outliers:** Una distribución está sesgada cuando una de las colas de la curva es más larga que la otra\n",
    "\n",
    "**Data Cleaning for Machine Learning** \n",
    "https://community.databricks.com/t5/technical-blog/data-cleaning-for-machine-learning/ba-p/95410#:~:text=Removing%20outliers%3A%20Using%20statistical%20methods,the%20overall%20impact%20of%20outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a091c30d-0a4a-4fd4-8c98-7dc42d34e6b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create visualizations for categorical or continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "857333d3-7760-4cf2-838c-56e976e76e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Con display(df) en notebooks\n",
    "Método nativo de Databricks.\n",
    "\n",
    "Muestra una tabla interactiva → puedes cambiar el tipo de gráfico desde el menú.\n",
    "Soporta:\n",
    "Bar chart\n",
    "Histogram\n",
    "Line chart\n",
    "Box plot\n",
    "Scatter plot\n",
    "Maps (si hay datos de coordenad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec6d417-c153-4dab-bd77-6d3d3cd57fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Con pandas API on Spark (pyspark.pandas) o conversión a Pandas\n",
    "Esto te permite usar .plot() como en pandas normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b42b65-1238-485a-aca5-9049c75d575b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "psdf = df.pandas_api()\n",
    "psdf[\"age\"].plot(kind=\"hist\")\n",
    "\n",
    "####\n",
    "pdf = df.toPandas()\n",
    "pdf[\"price\"].plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62b49a7e-57c6-42c9-ab35-55ffb7a3fad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📌 Puntos Clave\n",
    "\n",
    "| Tipo de variable | Ejemplo                    | Tipo de gráfico más adecuado                    |\n",
    "| ---------------- | -------------------------- | ----------------------------------------------- |\n",
    "| **Categórica**   | género, país, clase social | Bar chart, Pie chart, Count plot                |\n",
    "| **Continua**     | edad, precio, ingreso      | Histogram, Box plot, Density plot, Scatter plot |\n",
    "\n",
    "**Tipo de gráfico según el objetivo**\n",
    "\n",
    "** - Para variables categóricas:**\n",
    "\n",
    "| Gráfico                            | Cuándo usar                              | Función                              |\n",
    "| ---------------------------------- | ---------------------------------------- | ------------------------------------ |\n",
    "| **Bar chart**                      | Comparar frecuencias de categorías       | `display()` → selecciona “Bar chart” |\n",
    "| **Pie chart**                      | Comparar proporciones (pocas categorías) | ⚠️ Pocas veces útil                  |\n",
    "| **Count plot** (en pandas/seaborn) | Contar ocurrencias                       | `sns.countplot()`                    |\n",
    "\n",
    "** - Para variables continuas:**\n",
    "\n",
    "| Gráfico          | Cuándo usar                                       | Función                                       |\n",
    "| ---------------- | ------------------------------------------------- | --------------------------------------------- |\n",
    "| **Histogram**    | Ver distribución                                  | `display()` o `psdf[\"col\"].plot(kind=\"hist\")` |\n",
    "| **Box plot**     | Detectar outliers y asimetría                     | `display()` o `.plot(kind=\"box\")`             |\n",
    "| **Density plot** | Visualizar suavemente la forma de la distribución | `sns.kdeplot()` (pandas/seaborn)              |\n",
    "| **Scatter plot** | Relación entre dos variables numéricas            | `display(df)` → “Scatter”                     |\n",
    "\n",
    "\n",
    "https://docs.databricks.com/aws/en/visualizations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1f15843-f871-4625-b516-e8e88efa9bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b2d66e-3fd6-423a-bbd5-d69279de77f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Compare two categorical or two continuous features using the appropriate method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0f62429-ff66-42b7-92c8-ee5d1161f8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Comparar dos variables categóricas\n",
    "\n",
    "Usa una tabla de contingencia y un test de chi-cuadrado si quieres evaluar independencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39d7545e-ceee-40b9-8b44-1b2530971a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejemplo: Comparar genero y compra_realizada\n",
    "\n",
    "Supongamos este DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a855355-6cc2-41bf-a172-a8a98487beb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"F\", \"Sí\"), (\"M\", \"No\"), (\"F\", \"Sí\"), (\"M\", \"Sí\"),\n",
    "    (\"F\", \"No\"), (\"M\", \"No\"), (\"F\", \"Sí\"), (\"M\", \"Sí\")\n",
    "]\n",
    "df_spark = spark.createDataFrame(data, [\"genero\", \"compra_realizada\"])\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac55de10-2ebe-4104-8fb2-40308c01f10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "a) Tabla de contingencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d68d7e-e3d1-4d08-8b56-8b1138d3ef6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df['genero'], df['compra_realizada'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64468187-3b0a-4cc1-bfeb-17bc2647727f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "b) Prueba de chi-cuadrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17b4df5-d473-47df-b159-f73508a734f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# una forma \n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "tabla = pd.crosstab(df['genero'], df['compra_realizada'])\n",
    "chi2, p, dof, expected = chi2_contingency(tabla)\n",
    "\n",
    "print(f\"Chi2: {chi2}, p-valor: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e649b55-567c-4903-8bc3-3b1a293279b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# con spark \n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "# Convertir texto a numérico\n",
    "indexer1 = StringIndexer(inputCol=\"genero\", outputCol=\"genero_idx\")\n",
    "indexer2 = StringIndexer(inputCol=\"compra_realizada\", outputCol=\"compra_idx\")\n",
    "\n",
    "df_indexed = indexer1.fit(df_spark).transform(df_spark)\n",
    "df_indexed = indexer2.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "# Crear vector de características\n",
    "assembler = VectorAssembler(inputCols=[\"genero_idx\"], outputCol=\"features\")\n",
    "df_features = assembler.transform(df_indexed)\n",
    "\n",
    "# Aplicar prueba chi-cuadrado\n",
    "chi_result = ChiSquareTest.test(df_features, \"features\", \"compra_idx\")\n",
    "chi_result.select(\"pValues\", \"degreesOfFreedom\", \"statistics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "619a7399-ea44-41f2-bb2e-889711ea1e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Si el p-valor < 0.05, hay relación significativa entre las dos variables categóricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a64c82bb-3cba-4709-9155-c97c41345505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Comparar dos variables continuas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fea6ad0d-317b-468c-905d-05822242da6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Usa la correlación (Pearson o Spearman).\n",
    "\n",
    "Ejemplo: Comparar edad vs ingresos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d84fa2ef-2350-4f59-807e-8b6b69084fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_continua = [\n",
    "    (25, 2200), (32, 2700), (47, 3500), (51, 4000), (38, 3000)\n",
    "]\n",
    "df_continuo = spark.createDataFrame(data_continua, [\"edad\", \"ingresos\"])\n",
    "df_continuo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "384701fa-c74c-4ad1-a016-28c6c2808ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "a) Correlación de Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cc23094-2b58-473d-877d-0e5ed4f914a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pearson (lineal)\n",
    "df_continuo.stat.corr(\"edad\", \"ingresos\", method=\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e02dd50-3f49-49a0-9da5-21dbd114de31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "b) Correlación de Spearman (si no es lineal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb848e50-c4c2-4950-9f7e-8e51b4314f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spearman (monótona)\n",
    "df_continuo.stat.corr(\"edad\", \"ingresos\", method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "965262d3-4ac7-467f-8d36-4b683dfb1507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📌 Puntos clave\n",
    "\n",
    "**Comparación final de métodos**\n",
    "\n",
    "| Tipo de variables        | Método recomendado           | Visualización                       |\n",
    "| ------------------------ | ---------------------------- | ----------------------------------- |\n",
    "| Categórica vs categórica | `crosstab`, chi-cuadrado     | Heatmap de frecuencias, stacked bar |\n",
    "| Continua vs continua     | Pearson/Spearman correlation | Scatter plot                        |\n",
    "\n",
    "**Conceptos clave que debes dominar para el examen**\n",
    "| Concepto                     | Detalle                                                               |\n",
    "| ---------------------------- | --------------------------------------------------------------------- |\n",
    "| `df.stat.crosstab()`         | Único método de Spark para conteo entre categorías                    |\n",
    "| `chi2_contingency()` (scipy) | No está en Spark nativo, pero evaluado conceptualmente                |\n",
    "| `df.stat.corr()`             | Aplica Pearson o Spearman en Spark                                    |\n",
    "| Pearson vs Spearman          | Pearson: lineal + normalidad<br>Spearman: ordinal o relación monótona |\n",
    "\n",
    "| Función clave                                | Qué hace                                                       |\n",
    "| -------------------------------------------- | -------------------------------------------------------------- |\n",
    "| `df.stat.crosstab(col1, col2)`               | Frecuencia entre dos columnas categóricas                      |\n",
    "| `df.stat.corr(col1, col2, method=\"pearson\")` | Correlación de Pearson o Spearman                              |\n",
    "| `df.groupBy(...).count()`                    | También puede usarse para agrupaciones categóricas simples     |\n",
    "| `display(df)`                                | Permite hacer scatter plot para ver relación entre 2 numéricas |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bce5b3bb-547a-4b63-a5ba-bc6c53a734a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Compare and contrast imputing missing values with the mean or median or mode value\n",
    "(Impute missing values with the mode, mean, or median value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd03999-08a0-4a69-b943-3d60aac0926c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Imputar = reemplazar valores faltantes (nulls / NaNs) con algún valor que represente el resto de la distribución.\n",
    "¿Por qué imputar?\n",
    "\n",
    "- Muchos modelos no aceptan datos nulos.\n",
    "- Evita perder información si eliminas filas.\n",
    "- El método elegido afecta el sesgo y la varianza del modelo.\n",
    "\n",
    "| Método               | Se aplica a                                 | Ventajas                                             | Desventajas                                      | Cuándo usar                                                  |\n",
    "| -------------------- | ------------------------------------------- | ---------------------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| **Mean (media)**     | Variables numéricas continuas               | Fácil de calcular, mantiene media de la distribución | Afectado por *outliers* y sesgo                  | Cuando la variable tiene distribución **simétrica**          |\n",
    "| **Median (mediana)** | Variables numéricas continuas               | Robusta a *outliers* y sesgo                         | No conserva la media original                    | Cuando la variable tiene **distribución sesgada o outliers** |\n",
    "| **Mode (moda)**      | Variables categóricas o numéricas discretas | Mejor para categorías; representa el valor más común | No siempre es representativo si hay varias modas | Cuando la variable es **categórica o discreta**              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e158f7d-87b3-41f9-b225-8d6194b97ba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Con .fillna() → uso directo, manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fa7b7b0-43d6-4a0f-94f7-3ca1b30163c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.fillna({\"age\": 30, \"gender\": \"Male\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e8e69b-0608-4f3e-854b-48e6584ac20d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Con Imputer (Spark ML) → solo numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d6103ea-3f68-42f7-8493-e62f740910de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"age\", \"income\"],\n",
    "    outputCols=[\"age_imputed\", \"income_imputed\"]\n",
    ").setStrategy(\"mean\")  # o \"median\"\n",
    "\n",
    "model = imputer.fit(df)\n",
    "df_imputed = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f2c3b9-e73f-4475-ab9b-9219e6c36482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📌 Puntos clave\n",
    "\n",
    "| Concepto                                    | Detalle clave                                         |\n",
    "| ------------------------------------------- | ----------------------------------------------------- |\n",
    "| `mode` no está en `Imputer`                 | Debes calcularlo manualmente y usar `.fillna()`       |\n",
    "| `median` es más robusta que `mean`          | Se usa cuando hay outliers o datos sesgados           |\n",
    "| `mean` puede deformar la distribución       | Especialmente en variables de ingresos, precios, etc. |\n",
    "| `Imputer` solo sirve con columnas numéricas | Para categóricas, debes usar otras técnicas           |\n",
    "| Para variables categóricas                  | Solo **mode** es válido como imputación simple        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8afa4ea9-64fa-46c0-9a18-b90e98bfcfe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Use one-hot encoding for categorical features\n",
    "\n",
    "(Identify and explain the model types or data sets for which one-hot encoding is or is not\n",
    "appropriate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63bf3ea3-02f9-4b72-9612-f2156ca2a24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark\n",
    "En Spark, el one-hot encoding se hace típicamente en dos pasos con el módulo de MLlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e12eca64-c755-4efd-9e85-96dc9654f115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Indexar la columna categórica (convertir a números)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"color\", outputCol=\"color_index\")\n",
    "df_indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "# 2. Aplicar OneHotEncoder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"color_index\"], outputCols=[\"color_ohe\"])\n",
    "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92c07703-e564-44c2-8dfd-d96b6cd8bd67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📌 Puntos clave\n",
    "\n",
    "| Punto                                                              | Explicación                                                              |\n",
    "| ------------------------------------------------------------------ | ------------------------------------------------------------------------ |\n",
    "| Puede crear muchas columnas si la variable tiene muchas categorías | → Aumenta dimensionalidad y puede causar overfitting                     |\n",
    "| No es ideal para árboles de decisión o random forest en Spark      | → Spark ML maneja los índices de categoría directamente en estos modelos |\n",
    "| No funciona con valores nulos                                      | → Debes imputar o filtrar primero                                        |\n",
    "\n",
    "\n",
    "✅ Úsalo cuando:\n",
    "\n",
    "La variable es categórica sin orden (nominal)\n",
    "Usas modelos que no aceptan variables categóricas directamente, como regresión lineal, logística, redes neuronales\n",
    "\n",
    "❌ Evita si:\n",
    "\n",
    "Hay muchas categorías → usar feature hashing o embedding\n",
    "Estás usando tree-based models en Spark ML (como Random Forest o GBT) → estos aceptan directamente índices de StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6105341-f8ea-4cdb-9185-74caa327e98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Identify scenarios where log scale transformation is appropriate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a15eb58c-22fe-4fa0-8025-f30fc58c9a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "¿Qué es una transformación logarítmica?\n",
    "Es una transformación matemática que aplica la función logaritmo (por ejemplo, log base 10 o log natural) a una variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2482036c-5727-440f-bc94-600eee81dd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log\n",
    "df = df.withColumn(\"log_income\", log(df[\"income\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d70dc875-4174-4693-b239-17e8e4065f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Esta técnica reduce la escala de los valores grandes y puede ayudar a que los datos cumplan mejor los supuestos de ciertos modelos.\n",
    "\n",
    "**Objetivo de la transformación logarítmica**\n",
    "- Reducir sesgo hacia la derecha (right-skewed data)\n",
    "- Estabilizar la varianza (heterocedasticidad)\n",
    "- Mejorar relaciones lineales entre variables\n",
    "- Permitir que el modelo aprenda mejor patrones exponenciales\n",
    "\n",
    "**Casos típicos donde sí es apropiado aplicar log**\n",
    "| Variable                                                  | Motivo                                                                     | Resultado esperado                         |\n",
    "| --------------------------------------------------------- | -------------------------------------------------------------------------- | ------------------------------------------ |\n",
    "| `income`, `house_price`, `sales`, `population`            | Datos muy **sesgados a la derecha**, con valores grandes y dispersión alta | Distribución más simétrica, más linealidad |\n",
    "| `count` de eventos, como accesos por hora                 | Muchos ceros + valores extremos                                            | Compresión de valores grandes              |\n",
    "| Relación exponencial (ej: y crece exponencialmente con x) | Mejora la linealidad                                                       | Permite usar regresión lineal              |\n",
    "\n",
    "**Casos donde NO debes aplicar log**\n",
    "| Caso                               | Por qué evitarlo                                |\n",
    "| ---------------------------------- | ----------------------------------------------- |\n",
    "| Valores ≤ 0 (negativos o ceros)    | No se puede aplicar log a 0 o números negativos |\n",
    "| Variables ya normales o simétricas | No necesitas transformar                        |\n",
    "| Variables categóricas              | No tiene sentido aplicar log                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c805582a-6c92-4e14-8048-5a78089ee470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📌 Puntos clave\n",
    "\n",
    "| Situación                                                 | ¿Aplicar log? | Motivo                       |\n",
    "| --------------------------------------------------------- | ------------- | ---------------------------- |\n",
    "| Datos numéricos muy sesgados positivamente (right-skewed) | ✅ Sí          | Reduce la asimetría          |\n",
    "| Rangos muy amplios                                        | ✅ Sí          | Normaliza la escala          |\n",
    "| Exponenciales                                             | ✅ Sí          | Facilita modelado lineal     |\n",
    "| Ceros o negativos                                         | ❌ No          | log(x) indefinido para x ≤ 0 |\n",
    "| Categóricos                                               | ❌ No          | No tiene sentido             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf430f1-ff6b-4cd6-b6c6-b7ad228833d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f00971-3142-4a47-86d5-85b9d3596976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e28ca67-7b5a-477e-abfa-8e02ee0e8786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f64353d-9579-4353-a3d1-018044914e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Section 2: Data Processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
