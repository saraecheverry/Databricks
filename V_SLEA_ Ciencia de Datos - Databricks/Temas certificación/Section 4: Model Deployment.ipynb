{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a99164e-bc95-46d0-9f69-6aab9ee4e803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Identify the differences and advantages of model serving approaches: batch, realtime, and streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5739211-62a3-4578-9661-ae6557c4a028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Batch (lote):** Ejecuta predicciones sobre un conjunto grande de registros seg√∫n un horario o un disparador (por ejemplo, cada hora, cada noche o bajo demanda). Los resultados se almacenan (tabla/archivo) y se consumen despu√©s.\n",
    "\n",
    "**Tiempo real (online):** Sirve predicciones por solicitud a trav√©s de un endpoint API; devuelve la respuesta en milisegundos o pocos segundos.\n",
    "\n",
    "**Streaming:** Punt√∫a datos continuos y sin fin (por ejemplo, Kafka o Auto Loader hacia Delta) con latencia baja y constante; los resultados se env√≠an de forma continua a un destino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c64de665-5c9f-4027-b0c0-ca14d586a96c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Comparativa r√°pida**\n",
    "\n",
    "| Dimensi√≥n             | Batch                                                          | Tiempo real                                      | Streaming                                               |\n",
    "| --------------------- | -------------------------------------------------------------- | ------------------------------------------------ | ------------------------------------------------------- |\n",
    "| **Disparador**        | Programado / job bajo demanda                                  | Solicitud HTTP/gRPC                              | Llega un nuevo evento                                   |\n",
    "| **Latencia objetivo** | Minutos ‚Üí horas (a veces segundos)                             | \\~5‚Äì3000 ms                                      | Subsegundo ‚Üí segundos, continuo                         |\n",
    "| **Patr√≥n de volumen** | Muy alto, en oleadas                                           | Por solicitud; tr√°fico irregular                 | Flujo constante alto                                    |\n",
    "| **Estado y orden**    | Instant√°nea o micro-batches                                    | Generalmente sin estado                          | Puede ser con estado (ventanas, joins)                  |\n",
    "| **Costo**             | Bajo costo por predicci√≥n en grandes vol√∫menes                 | M√°s caro por estar siempre activo                | Costo estable para cargas constantes                    |\n",
    "| **Manejo de fallos**  | F√°cil reintento; reprocesos idempotentes                       | Retrys en cliente/gateway; cuidado con timeouts  | Checkpoints, garant√≠as ‚Äúexactly-once‚Äù o ‚Äúat-least-once‚Äù |\n",
    "| **Mejor para**        | Backfills, puntajes nocturnos, features para BI, listas de CRM | Personalizaci√≥n UX, fraude en checkout, chatbots | Clickstream, IoT, detecci√≥n de anomal√≠as                |\n",
    "| **Anti-patr√≥n com√∫n** | Querer ms de latencia usando batch                             | Usar online para millones de registros           | Usar streaming cuando batch cada hora es suficiente     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c76f3de-f883-4d22-983b-337e952e47f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìå Puntos clave\n",
    "**Tres reglas r√°pidas para decidir**\n",
    "\n",
    "**¬øQui√©n espera?**\n",
    "Si alguien o un sistema espera la respuesta ‚Üí Tiempo real. Si no ‚Üí considera Batch o Streaming.\n",
    "\n",
    "¬øC√≥mo llega el dato?\n",
    "En bloques ‚Üí Batch. Flujo continuo ‚Üí Streaming.\n",
    "\n",
    "¬øFrescura vs costo?\n",
    "Si ‚Äúminutos‚Äù es aceptable para grandes vol√∫menes ‚Üí Batch.\n",
    "Si necesitas segundos y actualizaci√≥n constante ‚Üí Streaming.\n",
    "Si necesitas <1 segundo para un registro ‚Üí Tiempo real.\n",
    "\n",
    "**En Databricks (enfoque del examen)**\n",
    "\n",
    "Batch: Jobs/Workflows que leen de Delta, punt√∫an con pandas o Spark UDFs, y escriben resultados a Delta. Usado para backfills y scoring peri√≥dico.\n",
    "\n",
    "Tiempo real: Model Serving endpoints (MLflow). Reciben peticiones REST, permiten canary y divisi√≥n de tr√°fico, con autoscaling y autenticaci√≥n.\n",
    "\n",
    "Streaming: Delta Live Tables (DLT) o Structured Streaming para leer de Auto Loader/Kafka, generar features (ventanas/joins), puntuar modelos (UDFs o endpoints externos) y guardar a Delta o buses de eventos. Importante: checkpoints y watermarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b7b157e-9327-4c40-ae9e-ed650a5f3b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deploy a custom model to a model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b898522f-6eb0-4c7e-9696-4a68da367e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. ¬øQu√© significa ‚Äúcustom model‚Äù?**\n",
    "\n",
    "En Databricks puedes desplegar modelos entrenados con frameworks conocidos (sklearn, XGBoost, PyTorch, TensorFlow).\n",
    "Un modelo personalizado significa que t√∫ defines la l√≥gica de inferencia en lugar de usar el flavor autom√°tico de MLflow.\n",
    "\n",
    "Ejemplo: quieres preprocesar texto antes de predecir, o devolver m√∫ltiples salidas, o integrar librer√≠as externas.\n",
    "\n",
    "En este caso, defines tu propio pyfunc model o escribes la l√≥gica de predict().\n",
    "\n",
    "**2. Flujo general en Databricks**\n",
    "\n",
    "**1.1 Entrenar y guardar el modelo**\n",
    "\n",
    "- Entrenas en Databricks Notebook / Jobs.\n",
    "- Guardas el modelo con MLflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3671e2c1-bb97-4ba0-bbcc-a08315091b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "class MyCustomModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input):\n",
    "        # l√≥gica de preprocesamiento + inferencia\n",
    "        # ej: limpiar texto antes de usar un clasificador\n",
    "        processed = model_input.str.lower()\n",
    "        return [\"OK\" if \"good\" in x else \"BAD\" for x in processed]\n",
    "\n",
    "mlflow.pyfunc.save_model(path=\"custom_model\", python_model=MyCustomModel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36a47ace-a6ab-46b8-97f0-78ebec8b6715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1.2 Registrar el modelo en Unity Catalog**\n",
    "\n",
    "Usas MLflow para registrar el modelo, As√≠ el modelo queda versionado y gobernado.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227296f7-82c8-476e-b768-3a36cfed674c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.register_model(\"runs:/<run-id>/model\", \"catalog.schema.custom_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe85690d-7be9-41f4-a79e-8c63a95ce724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1.3 Crear un endpoint de Serving**\n",
    "\n",
    "Desde la UI:\n",
    "\n",
    "- Machine Learning ‚Üí Model Serving ‚Üí Create Serving Endpoint\n",
    "- Seleccionas el modelo registrado y la versi√≥n.\n",
    "\n",
    "O desde API/CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6078966-d737-4d24-a466-d0c2be4c1dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import databricks.sdk.service.serving as serving\n",
    "\n",
    "client = DatabricksClient()\n",
    "client.serving_endpoints.create(\n",
    "    name=\"custom-model-endpoint\",\n",
    "    config=serving.EndpointCoreConfigInput(\n",
    "        served_models=[serving.ServedModelInput(\n",
    "            name=\"custom-model\",\n",
    "            model_name=\"catalog.schema.custom_model\",\n",
    "            model_version=\"1\",\n",
    "            workload_size=\"Small\"\n",
    "        )]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2547792b-bb1a-4af7-8ae2-ef249a95d754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Diferencia entre Deploy con flavor y Deploy custom**\n",
    "\n",
    "**1) Flavors (sabores MLflow)**\n",
    "\n",
    "Qu√© es: MLflow ya trae integraciones (flavors) para frameworks comunes:\n",
    "\n",
    "- mlflow.sklearn\n",
    "- mlflow.xgboost\n",
    "- mlflow.pytorch\n",
    "- mlflow.tensorflow\n",
    "- mlflow.lightgbm, etc.\n",
    "\n",
    "**C√≥mo funciona:**\n",
    "\n",
    "- Guardas el modelo con un flavor:\n",
    "\n",
    "```\n",
    "import mlflow.sklearn\n",
    "mlflow.sklearn.log_model(model, \"model\")\n",
    "```\n",
    "\n",
    "- MLflow sabe autom√°ticamente c√≥mo serializar, cargar y servir ese modelo.\n",
    "- El endpoint desplegado entiende la API est√°ndar de predicci√≥n (predict()) de ese framework.\n",
    "\n",
    "**Ventajas:**\n",
    "\n",
    "- Simplicidad ‚Üí r√°pido y sin c√≥digo extra.\n",
    "- Compatibilidad ‚Üí Databricks Serving puede levantarlo sin problemas.\n",
    "- Ideal cuando no necesitas l√≥gica extra, solo correr el modelo entrenado.\n",
    "\n",
    "**Limitaci√≥n:**\n",
    "\n",
    "- No puedes personalizar la l√≥gica de predicci√≥n m√°s all√° de lo que hace el framework.\n",
    "- No es flexible para pipelines complejos, preprocesamiento/postprocesamiento.\n",
    "\n",
    "**2) Custom model (PythonModel / pyfunc)**\n",
    "\n",
    "Qu√© es: Definido por ti mediante la interfaz mlflow.pyfunc.PythonModel.\n",
    "\n",
    "**C√≥mo funciona:**\n",
    "\n",
    "```\n",
    "import mlflow.pyfunc\n",
    "\n",
    "class MyCustomModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input):\n",
    "        # l√≥gica definida por ti\n",
    "        # puedes incluir preprocesamiento + modelo + postprocesamiento\n",
    "        return [\"OK\" if \"good\" in x else \"BAD\" for x in model_input]\n",
    "\n",
    "mlflow.pyfunc.save_model(path=\"custom_model\", python_model=MyCustomModel())\n",
    "```\n",
    "\n",
    "**Ventajas:**\n",
    "\n",
    "- Flexibilidad total ‚Üí defines c√≥mo procesar la entrada y la salida.\n",
    "- Puedes combinar varios modelos (ensembles).\n",
    "- Puedes a√±adir transformaciones personalizadas (ej. NLP, normalizaci√≥n).\n",
    "- Permite integrar librer√≠as externas que no tienen flavor oficial.\n",
    "\n",
    "**Limitaci√≥n:**\n",
    "\n",
    "- M√°s c√≥digo a mantener.\n",
    "- Mayor riesgo de errores si no manejas bien dependencias o entornos.\n",
    "- Requiere empaquetar dependencias (conda.yaml, requirements.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6c98122-8215-4f9a-9851-9d98674b2e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìå Puntos clave\n",
    "**Beneficios de usar endpoints en Databricks**\n",
    "\n",
    "- Escalado autom√°tico: ajusta recursos seg√∫n tr√°fico.\n",
    "- Seguridad: integraci√≥n con Unity Catalog y permisos RBAC.\n",
    "- Observabilidad: m√©tricas de latencia (p50/p95/p99), throughput, logs de errores.\n",
    "- Integraci√≥n con CI/CD: cambiar versiones con aliases (champion, challenger).\n",
    "- Customizaci√≥n: l√≥gica de inferencia avanzada, ensembles, post-procesamiento.\n",
    "\n",
    "---\n",
    "**Flavor =** r√°pido, est√°ndar, soportado directamente por MLflow.\n",
    "\n",
    "**Custom =** cuando necesitas m√°s control (pre/post procesamiento, ensembles, frameworks no soportados).\n",
    "\n",
    "---\n",
    "‚úÖ En el examen:\n",
    "\n",
    "- Tienes que distinguir qu√© significa ‚Äúdeploy custom model‚Äù ‚Üí registrar con MLflow, crear endpoint, y exponer l√≥gica propia.\n",
    "- Reconocer ventajas: gobernanza, escalado, observabilidad, seguridad.\n",
    "- Identificar cu√°ndo usar endpoint vs batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7069a1e7-c08c-40b9-afa7-53f56a64df4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Use pandas to perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40930cc5-aa3c-4d42-b267-b87772066930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Concepto b√°sico**\n",
    "\n",
    "**Batch inference =** aplicar un modelo a un conjunto de datos ya almacenado (ej. Delta table, CSV, Parquet) en lugar de responder peticiones en tiempo real.\n",
    "\n",
    "El flujo es:\n",
    "\n",
    "- Cargar modelo desde el registro (mlflow.pyfunc.load_model o mlflow.sklearn.load_model).\n",
    "- Cargar datos a un DataFrame de pandas.\n",
    "- Usar .predict() para obtener resultados.\n",
    "- Guardar los resultados en una tabla, archivo o endpoint downstream.\n",
    "\n",
    "**2. Ejemplo pr√°ctico**\n",
    "\n",
    "Supongamos que registraste un modelo de clasificaci√≥n en MLflow con alias \"champion\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e13f5d45-5497-4c56-b087-34bfca543e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Cargar modelo registrado\n",
    "model_uri = \"models:/my_model@champion\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# 2. Cargar datos batch (ejemplo CSV o Delta)\n",
    "df = pd.read_csv(\"/dbfs/FileStore/batch_input.csv\")\n",
    "\n",
    "# 3. Inference con pandas DataFrame\n",
    "preds = model.predict(df)\n",
    "\n",
    "# 4. Guardar resultados\n",
    "output = df.copy()\n",
    "output[\"prediction\"] = preds\n",
    "output.to_csv(\"/dbfs/FileStore/batch_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b712997e-ef78-4f8a-a9de-b762632c9876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìå Puntos clave\n",
    "Entrada esperada: el modelo sirve datos en forma de pandas.DataFrame.\n",
    "\n",
    "**Salida esperada:** depende del modelo, pero usualmente es numpy.ndarray, pandas.Series, o un DataFrame con columnas predichas.\n",
    "**Escalabilidad:** aunque usas pandas localmente, en Databricks puedes escalar con Spark ‚Üí convertir spark.DataFrame a pandas.DataFrame para lotes peque√±os, o usar applyInPandas() para procesar en paralelo lotes grandes.\n",
    "\n",
    "Casos de uso: predicciones nocturnas, scoring de millones de filas, ETL con features ya preparados.\n",
    "\n",
    "**Diferencia con Real-time**\n",
    "\n",
    "**Batch:** procesas todo un dataset a la vez (ejemplo: predecir churn para 10M de clientes esta noche).\n",
    "**Real-time:** procesas una fila o request a la vez (ejemplo: recomendar productos cuando un cliente entra al sitio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d20f97c-648d-41bd-b79b-97525a3f7890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Identify how streaming inference is performed with Delta Live Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5d2ff2b-0121-4cf1-bef2-de81bf1d094f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Concepto**\n",
    "\n",
    "- Streaming inference = aplicar un modelo de ML continuamente sobre datos que llegan en tiempo real (por ejemplo, eventos de IoT, logs, clics de usuarios).\n",
    "- En Databricks, se implementa con Delta Live Tables (DLT) porque estas permiten definir pipelines declarativos que soportan tanto batch como streaming.\n",
    "- \n",
    "**2. Flujo t√≠pico con DLT**\n",
    "\n",
    "- Fuente de datos (ejemplo: Kafka, Event Hub, Kinesis) ‚Üí ingestada como streaming DataFrame.\n",
    "- Transformaci√≥n en DLT ‚Üí procesamiento declarativo con funciones de limpieza, enriquecimiento y preparaci√≥n de features.\n",
    "- Inference con modelo MLflow ‚Üí usar el modelo registrado (mlflow.pyfunc.spark_udf) como una UDF en el flujo.\n",
    "- Salida ‚Üí escribir resultados en una tabla Delta (batch + streaming compatible), dashboards en tiempo real o endpoints downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96138104-1297-41e1-8462-9249738408f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import mlflow\n",
    "\n",
    "# 1. Cargar modelo registrado como UDF Spark\n",
    "model_uri = \"models:/fraud_detector@champion\"\n",
    "predict_udf = mlflow.pyfunc.spark_udf(spark, model_uri, result_type=\"double\")\n",
    "\n",
    "# 2. Definir tabla streaming en DLT\n",
    "@dlt.table\n",
    "def scored_transactions():\n",
    "    return (\n",
    "        spark.readStream.format(\"delta\").table(\"raw_transactions\")\n",
    "        .withColumn(\"fraud_score\", predict_udf(\"feature1\", \"feature2\", \"feature3\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc7172e6-b098-422b-834c-9cb5d86d3194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìå Puntos clave\n",
    "- **DLT + MLflow: **el modelo se convierte en una UDF con mlflow.pyfunc.spark_udf.\n",
    "- **Streaming:** readStream y writeStream son compatibles dentro de DLT.\n",
    "- **Fiabilidad: **DLT maneja checkpointing y retries autom√°ticamente ‚Üí garantiza tolerancia a fallos.\n",
    "- **Exactly-once processing:** DLT asegura que cada evento se procese una sola vez en caso de fallos, lo cual es cr√≠tico para inferencia.\n",
    "\n",
    "**Casos de uso:**\n",
    "\n",
    "- Detecci√≥n de fraude en transacciones.\n",
    "- Alertas en IoT en tiempo real.\n",
    "- Recomendaciones instant√°neas en e-commerce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62b838c7-853e-4f5f-85b6-692cc040c4b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Deploy and query a model for realtime inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb53f971-831a-443f-a93d-53539abc5792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Concepto**\n",
    "\n",
    "Realtime inference = publicar un modelo como un endpoint REST que puede responder a predicciones inmediatas para entradas individuales o en lotes peque√±os (milisegundos‚Äìsegundos).\n",
    "\n",
    "En Databricks esto se hace con Model Serving, integrado con Unity Catalog y MLflow.\n",
    "\n",
    "**2. Flujo t√≠pico**\n",
    "\n",
    "- Registrar el modelo en Unity Catalog (con MLflow ‚Üí flavors o pyfunc).\n",
    "- Crear el endpoint de serving desde la UI o v√≠a API:\n",
    "- Se define un alias (champion, challenger) para controlar la versi√≥n servida.\n",
    "- Se pueden asociar varios modelos al mismo endpoint con traffic splitting.\n",
    "- Enviar solicitudes REST al endpoint para obtener predicciones.\n",
    "- Se usa curl, requests en Python, o librer√≠as cliente.\n",
    "\n",
    "**3. Ejemplo ‚Äì Deploy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a105b28-c88b-4ba5-ba6d-5e29026a4a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a) Registro del modelo\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Entrenar o cargar un modelo sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"catalog.schema.my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa6d98d-6d44-43a3-abaf-71c33cb7d9fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# b) Crear el endpoint (UI o API)\n",
    "databricks model-serving-endpoints create \\\n",
    "  --name my-endpoint \\\n",
    "  --config '{\n",
    "    \"served_models\": [{\n",
    "      \"model_name\": \"catalog.schema.my_model\",\n",
    "      \"model_version\": \"1\",\n",
    "      \"workload_size\": \"Small\"\n",
    "    }]\n",
    "  }'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98705095-91f3-463f-9e70-fb8cc8e6746b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Ejemplo ‚Äì Query\n",
    "\n",
    "# a) Enviar solicitud REST (JSON)\n",
    "curl -X POST https://<workspace-url>/serving-endpoints/my-endpoint/invocations \\\n",
    "  -H \"Authorization: Bearer $DATABRICKS_TOKEN\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"dataframe_split\": {\"columns\": [\"feature1\",\"feature2\"], \"data\": [[1.0, 3.5]]}}'\n",
    "\n",
    "# Python con requests\n",
    "import requests, os, json\n",
    "\n",
    "url = \"https://<workspace-url>/serving-endpoints/my-endpoint/invocations\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['DATABRICKS_TOKEN']}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = {\"dataframe_split\": {\"columns\": [\"feature1\",\"feature2\"], \"data\": [[1.0, 3.5]]}}\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "print(response.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e739fc98-5bac-46cd-8a6e-0564b2743ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìå Puntos clave\n",
    "\n",
    "Caracter√≠sticas importantes para el examen\n",
    "\n",
    "- SLA: Model Serving en Databricks ofrece latency SLA para workloads pro (p95 < 100 ms).\n",
    "- Escalado autom√°tico: endpoints se escalan seg√∫n tr√°fico.\n",
    "- Versioning con aliases: @champion y @challenger facilitan A/B testing.\n",
    "- Traffic splitting: se puede dividir tr√°fico entre modelos (ej. 90% vs 10%).\n",
    "- Compatibilidad: soporta sklearn, xgboost, pytorch, transformers y custom pyfunc.\n",
    "\n",
    "| **Aspecto**      | **Batch inference (pandas/Spark)** | **Realtime inference (endpoints)** | **Streaming inference (DLT)** |\n",
    "| ---------------- | ---------------------------------- | ---------------------------------- | ----------------------------- |\n",
    "| Latencia         | Minutos‚Äìhoras                      | Milisegundos‚Äìsegundos              | Segundos‚Äìsubsegundos          |\n",
    "| Escenario t√≠pico | Scoring hist√≥rico (archivos)       | API en vivo (web/app)              | Flujo continuo (IoT, eventos) |\n",
    "| Herramienta      | Pandas/Spark                       | Model Serving (REST endpoint)      | DLT + MLflow UDF              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e378fbe-b011-459c-a3d8-c45cb9b70132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Split data between endpoints for realtime interference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eff27a7b-d913-4019-8ee9-7fc5fe64f054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Concepto**\n",
    "\n",
    "- En Databricks no necesitas crear m√∫ltiples endpoints para probar distintos modelos.\n",
    "- Un solo endpoint puede servir varios modelos.\n",
    "- Puedes dividir el tr√°fico (split) entre ellos en porcentajes (ej. 90% Champion, 10% Challenger).\n",
    "\n",
    "Esto se usa para:\n",
    "\n",
    "- A/B testing: comparar dos modelos en producci√≥n.\n",
    "- Canary deployment: liberar gradualmente un modelo nuevo antes de reemplazar al anterior.\n",
    "- Experimentaci√≥n segura: evaluar precisi√≥n y latencia en vivo con usuarios reales.\n",
    "\n",
    "**2. C√≥mo funciona**\n",
    "Cada served_model dentro del endpoint tiene:\n",
    "\n",
    "- model_name\n",
    "- model_version\n",
    "- traffic_percentage\n",
    "\n",
    "El endpoint autom√°ticamente enruta solicitudes siguiendo esos porcentajes.\n",
    "\n",
    "**3. Ejemplo ‚Äì Configuraci√≥n**\n",
    "\n",
    "Crear un endpoint con 2 modelos (Esto env√≠a el 80% de solicitudes al modelo v3 (champion) y el 20% al modelo v4 (challenger).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88e4d6c3-eccb-48aa-a552-70bba671a46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "databricks model-serving-endpoints create \\\n",
    "  --name my-endpoint \\\n",
    "  --config '{\n",
    "    \"served_models\": [\n",
    "      {\n",
    "        \"model_name\": \"catalog.schema.my_model\",\n",
    "        \"model_version\": \"3\",\n",
    "        \"workload_size\": \"Small\",\n",
    "        \"traffic_percentage\": 80\n",
    "      },\n",
    "      {\n",
    "        \"model_name\": \"catalog.schema.my_model\",\n",
    "        \"model_version\": \"4\",\n",
    "        \"workload_size\": \"Small\",\n",
    "        \"traffic_percentage\": 20\n",
    "      }\n",
    "    ]\n",
    "  }'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79bbc051-1f65-415e-a1c5-81b77b5a65f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Actualizar el split (promoci√≥n progresiva) Aqu√≠ promovemos el modelo v4 a 100% tr√°fico, descartando v3.\n",
    "databricks model-serving-endpoints update \\\n",
    "  --name my-endpoint \\\n",
    "  --config '{\n",
    "    \"served_models\": [\n",
    "      {\n",
    "        \"model_name\": \"catalog.schema.my_model\",\n",
    "        \"model_version\": \"4\",\n",
    "        \"workload_size\": \"Small\",\n",
    "        \"traffic_percentage\": 100\n",
    "      }\n",
    "    ]\n",
    "  }'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d7810a0-b9bd-4544-b227-d035597ef952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìå Puntos clave\n",
    "\n",
    "Ventajas\n",
    "\n",
    "‚úÖ Sin downtime: no necesitas detener el endpoint para cambiar modelo.\n",
    "‚úÖ Promoci√≥n gradual: reduces riesgo al introducir un modelo nuevo.\n",
    "‚úÖ Experimentos controlados: puedes evaluar m√©tricas de latencia (p95/p99) y precisi√≥n real.\n",
    "‚úÖ Rollback sencillo: si el Challenger falla, solo vuelves a aumentar el porcentaje del Champion.\n",
    "\n",
    "| **Estrategia**               | **Uso principal**                                      |\n",
    "| ---------------------------- | ------------------------------------------------------ |\n",
    "| **Champion/Challenger**      | Probar un nuevo modelo contra el actual en paralelo.   |\n",
    "| **Traffic Split (A/B test)** | Evaluar rendimiento en producci√≥n con usuarios reales. |\n",
    "| **Canary Deployment**        | Migrar progresivamente (ej. 5% ‚Üí 20% ‚Üí 100%).          |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Section 4: Model Deployment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
